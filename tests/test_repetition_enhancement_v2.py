# -*- coding: utf-8 -*-
"""
åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3 ãƒ†ã‚¹ãƒˆãƒ»è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
æˆåŠŸç‡80%+ã‚’ç›®æŒ‡ã™v3å¼·åŒ–ç‰ˆã®ãƒ†ã‚¹ãƒˆã¨ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ”¯æ´
"""

import json
import time
import sys
import os
from typing import Dict, List, Tuple
from tqdm import tqdm

# ãƒ‘ã‚¹è¨­å®š
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(current_dir, "src")
sys.path.insert(0, src_dir)

from utils.repetition_suppressor_v3 import AdvancedRepetitionSuppressorV3, SuppressionMetricsV3


class RepetitionEnhancementTesterV2:
    """
    åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3 ã®ãƒ†ã‚¹ãƒˆãƒ»è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
    """
    
    def __init__(self):
        self.test_cases = self._prepare_enhanced_test_cases()
        self.test_results = []
        
    def _prepare_enhanced_test_cases(self) -> List[Dict]:
        """å¼·åŒ–ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚»ãƒƒãƒˆï¼ˆå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æç”¨ï¼‰"""
        return [
            # === åŒèªåå¾©ã‚±ãƒ¼ã‚¹ï¼ˆv2é‡ç‚¹å¯¾è±¡ï¼‰ ===
            {
                "name": "åŸºæœ¬åŒèªåå¾©",
                "input": "ãŠå…„ã¡ã‚ƒã‚“ãŠå…„ã¡ã‚ƒã‚“ã€ã©ã“ã«è¡Œãã®ã§ã™ã‹ãŠå…„ã¡ã‚ƒã‚“ï¼Ÿ",
                "character": "å¦¹",
                "expected_patterns": ["ãŠå…„ã¡ã‚ƒã‚“ãŠå…„ã¡ã‚ƒã‚“"],
                "difficulty": "medium",
                "category": "same_word_repetition"
            },
            {
                "name": "é–¢è¥¿å¼åŒèªåå¾©",
                "input": "ãã‚„ãã‚„ãã‚„ã€ã‚ã‹ã‚“ã‚ã‹ã‚“ã€ã‚„ãªã‚„ãªãã‚Œã¯ã€‚",
                "character": "é–¢è¥¿å¼ã‚­ãƒ£ãƒ©",
                "expected_patterns": ["ãã‚„ãã‚„ãã‚„", "ã‚ã‹ã‚“ã‚ã‹ã‚“", "ã‚„ãªã‚„ãª"],
                "difficulty": "high",
                "category": "dialect_repetition"
            },
            {
                "name": "èªå°¾åå¾©è¤‡åˆ",
                "input": "ã§ã™ã§ã™ã­ã€ã¾ã™ã¾ã™ã‚ˆã€ã§ã—ã‚‡ã§ã—ã‚‡ã†ã€‚",
                "character": "ä¸å¯§èªã‚­ãƒ£ãƒ©",
                "expected_patterns": ["ã§ã™ã§ã™", "ã¾ã™ã¾ã™", "ã§ã—ã‚‡ã§ã—ã‚‡"],
                "difficulty": "medium",
                "category": "ending_repetition"
            },
            
            # === æ„Ÿå˜†è©éå¤šã‚±ãƒ¼ã‚¹ ===
            {
                "name": "é•·ã„æ„Ÿå˜†è©",
                "input": "ã‚ã‚ã‚ã‚ã‚ã‚ã‚ï¼ã†ã‚ã‚ã‚ã‚ã‚ã‚ã‚ï¼ãã‚ƒã‚ã‚ã‚ã‚ã‚ã‚ï¼",
                "character": "æ„Ÿæƒ…è±Šã‹ã‚­ãƒ£ãƒ©",
                "expected_patterns": ["ã‚ã‚ã‚ã‚ã‚ã‚ã‚", "ã†ã‚ã‚ã‚ã‚ã‚ã‚ã‚", "ãã‚ƒã‚ã‚ã‚ã‚ã‚ã‚"],
                "difficulty": "easy",
                "category": "interjection_overuse"
            },
            {
                "name": "è¨˜å·åå¾©",
                "input": "æœ¬å½“ã«ï¼ï¼ï¼ï¼ï¼Ÿï¼Ÿï¼Ÿï¼Ÿãã†ãªã®ã€œã€œã€œã€œã€œâ€¦â€¦â€¦â€¦",
                "character": "ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é«˜ã‚ã‚­ãƒ£ãƒ©",
                "expected_patterns": ["ï¼ï¼ï¼ï¼", "ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ", "ã€œã€œã€œã€œã€œ", "â€¦â€¦â€¦â€¦"],
                "difficulty": "easy",
                "category": "symbol_repetition"
            },
            
            # === è¤‡é›‘ãªã‚±ãƒ¼ã‚¹ï¼ˆå¤±æ•—ã—ã‚„ã™ã„ï¼‰ ===
            {
                "name": "æ–‡è„ˆä¾å­˜åå¾©",
                "input": "ã„ã„å¤©æ°—ã§ã™ã­ã€‚ã„ã„å¤©æ°—ã ã‹ã‚‰æ•£æ­©ã—ã¾ã—ã‚‡ã†ã€‚ã„ã„å¤©æ°—ã®æ—¥ã¯æ°—æŒã¡ã„ã„ã§ã™ã€‚",
                "character": "æ™®é€šã®äºº",
                "expected_patterns": ["ã„ã„å¤©æ°—"],
                "difficulty": "hard",
                "category": "context_dependent"
            },
            {
                "name": "æ„å‘³çš„é¡ä¼¼åå¾©",
                "input": "å¬‰ã—ã„å¬‰ã—ã„ã€æ¥½ã—ã„æ¥½ã—ã„ã€å¹¸ã›å¹¸ã›ã¨ã„ã†æ„Ÿã˜ã§ã™ã€‚",
                "character": "ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚­ãƒ£ãƒ©",
                "expected_patterns": ["å¬‰ã—ã„å¬‰ã—ã„", "æ¥½ã—ã„æ¥½ã—ã„", "å¹¸ã›å¹¸ã›"],
                "difficulty": "medium",
                "category": "semantic_repetition"
            },
            {
                "name": "n-gramé‡è¤‡",
                "input": "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã ã‹ã‚‰å¤–å‡ºã—ã¾ã—ã‚‡ã†ã€‚",
                "character": "æ™®é€šã®äºº",
                "expected_patterns": ["ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—"],
                "difficulty": "high",
                "category": "ngram_repetition"
            },
            
            # === ä¿³å¥ãƒ»çŸ­æ­Œã‚±ãƒ¼ã‚¹ï¼ˆéå‰°åœ§ç¸®å›é¿ãƒ†ã‚¹ãƒˆï¼‰ ===
            {
                "name": "ä¿³å¥é¢¨è¡¨ç¾",
                "input": "æ˜¥ã®é¢¨ã€èŠ±ã®é¦™ã‚Šé¦™ã‚Šã€é³¥ã®å£°ã€‚",
                "character": "è©©äºº",
                "expected_patterns": [],  # æ„å›³çš„ãªåå¾©ã¯ä¿è­·
                "difficulty": "high",
                "category": "poetry_protection"
            },
            {
                "name": "çŸ­æ­Œé¢¨è¡¨ç¾",
                "input": "å¤•æ—¥ã‹ãªå¤•æ—¥ã‹ãªã€å±±ã®å‘ã“ã†ã«æ²ˆã¿ã‚†ãã€‚",
                "character": "è©©äºº",
                "expected_patterns": ["å¤•æ—¥ã‹ãªå¤•æ—¥ã‹ãª"],  # ã“ã‚Œã¯éå‰°
                "difficulty": "high",
                "category": "poetry_protection"
            },
            
            # === æ¥µç«¯ã‚±ãƒ¼ã‚¹ ===
            {
                "name": "è¶…é•·æ–‡åå¾©",
                "input": "ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™ã€‚" * 3,
                "character": "ãƒ†ã‚¹ãƒˆç”¨",
                "expected_patterns": ["ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™"],
                "difficulty": "extreme",
                "category": "extreme_case"
            },
            {
                "name": "çŸ­æ–‡å¯†é›†åå¾©",
                "input": "ã‚ã‚ã€ã‚ã‚ã€ã‚ã‚ã€‚ã†ã†ã€ã†ã†ã€ã†ã†ã€‚ãŠãŠã€ãŠãŠã€ãŠãŠã€‚",
                "character": "æ··ä¹±ã‚­ãƒ£ãƒ©",
                "expected_patterns": ["ã‚ã‚", "ã†ã†", "ãŠãŠ"],
                "difficulty": "high",
                "category": "dense_repetition"
            }
        ]
    
    def run_comprehensive_test(self, 
                             configs: List[Dict] = None,
                             target_success_rate: float = 0.8) -> Dict:
        """
        åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆè¤‡æ•°è¨­å®šã§ã®æˆåŠŸç‡æ¯”è¼ƒï¼‰
        
        Args:
            configs: ãƒ†ã‚¹ãƒˆè¨­å®šãƒªã‚¹ãƒˆ
            target_success_rate: ç›®æ¨™æˆåŠŸç‡
        """
        if configs is None:
            configs = self._get_default_test_configs()
        
        print(f"ğŸ¯ ç›®æ¨™æˆåŠŸç‡: {target_success_rate:.1%}")
        print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°: {len(self.test_cases)}")
        print(f"âš™ï¸ è¨­å®šãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(configs)}")
        print("=" * 60)
        
        all_results = []
        
        for i, config in enumerate(configs):
            print(f"\nğŸ”§ è¨­å®š {i+1}/{len(configs)}: {config.get('name', f'Config-{i+1}')}")
            
            suppressor = AdvancedRepetitionSuppressorV3(config)
            config_results = []
            
            with tqdm(self.test_cases, desc=f"Config-{i+1}") as pbar:
                for test_case in pbar:
                    result = self._run_single_test(suppressor, test_case)
                    config_results.append(result)
                    
                    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®è©³ç´°æ›´æ–°
                    current_success_rate = sum(1 for r in config_results if r['success']) / len(config_results)
                    pbar.set_postfix({
                        'success_rate': f"{current_success_rate:.1%}",
                        'last_case': result['success']
                    })
            
            # è¨­å®šåˆ¥é›†è¨ˆ
            config_summary = self._analyze_config_results(config_results, config)
            config_summary['config'] = config
            all_results.append(config_summary)
            
            print(f"âœ… æˆåŠŸç‡: {config_summary['overall_success_rate']:.1%}")
            if config_summary['overall_success_rate'] >= target_success_rate:
                print(f"ğŸ‰ ç›®æ¨™é”æˆï¼ï¼ˆ{target_success_rate:.1%}ä»¥ä¸Šï¼‰")
            else:
                print(f"âŒ ç›®æ¨™æœªé”ï¼ˆç›®æ¨™: {target_success_rate:.1%}ï¼‰")
        
        # æœ€çµ‚åˆ†æ
        final_analysis = self._generate_final_analysis(all_results, target_success_rate)
        
        # çµæœä¿å­˜
        self._save_test_results(all_results, final_analysis)
        
        return final_analysis
    
    def _get_default_test_configs(self) -> List[Dict]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ†ã‚¹ãƒˆè¨­å®šç¾¤ï¼ˆå®Ÿè¨¼æ¸ˆã¿80%è¨­å®šã‚’å«ã‚€ï¼‰"""
        return [
            {
                "name": "ğŸ¯å®Ÿè¨¼æ¸ˆã¿80%è¨­å®š",
                "min_repeat_threshold": 1,
                "max_distance": 50,
                "similarity_threshold": 0.5,
                "phonetic_threshold": 0.7,
                "enable_aggressive_mode": True,
                "interjection_sensitivity": 0.3,
                "exact_match_priority": True,
                "character_repetition_limit": 2,
                "debug_mode": True,
                "ngram_block_size": 3,
                "enable_drp": False,
                "drp_alpha": 0.3,
                "use_jaccard_similarity": False
            },
            {
                "name": "v2åŸºæœ¬è¨­å®š",
                "min_repeat_threshold": 1,
                "max_distance": 30,
                "similarity_threshold": 0.68,
                "phonetic_threshold": 0.8,
                "enable_aggressive_mode": True,
                "interjection_sensitivity": 0.5,
                "exact_match_priority": True,
                "character_repetition_limit": 3,
                "debug_mode": True,
                "ngram_block_size": 4,
                "enable_drp": True,
                "drp_alpha": 0.5,
                "use_jaccard_similarity": True
            },
            {
                "name": "è¶…å³æ ¼ãƒ¢ãƒ¼ãƒ‰",
                "min_repeat_threshold": 1,
                "max_distance": 20,
                "similarity_threshold": 0.6,
                "phonetic_threshold": 0.7,
                "enable_aggressive_mode": True,
                "interjection_sensitivity": 0.4,
                "exact_match_priority": True,
                "character_repetition_limit": 2,
                "ngram_block_size": 5,
                "enable_drp": True,
                "drp_alpha": 0.6
            },
            {
                "name": "ãƒãƒ©ãƒ³ã‚¹é‡è¦–",
                "min_repeat_threshold": 2,
                "max_distance": 40,
                "similarity_threshold": 0.75,
                "phonetic_threshold": 0.85,
                "enable_aggressive_mode": False,
                "interjection_sensitivity": 0.7,
                "exact_match_priority": False,
                "character_repetition_limit": 4,
                "ngram_block_size": 3,
                "enable_drp": False
            },
            {
                "name": "n-gramç‰¹åŒ–",
                "min_repeat_threshold": 1,
                "max_distance": 35,
                "similarity_threshold": 0.70,
                "phonetic_threshold": 0.8,
                "enable_aggressive_mode": True,
                "ngram_block_size": 6,  # å¤§ããªn-gram
                "enable_drp": True,
                "drp_alpha": 0.7,
                "drp_window": 15
            },
            {
                "name": "è©©ä¿è­·ãƒ¢ãƒ¼ãƒ‰",
                "min_repeat_threshold": 2,
                "max_distance": 25,
                "similarity_threshold": 0.8,
                "enable_aggressive_mode": False,
                "character_repetition_limit": 5,
                "ngram_block_size": 0,  # n-gramãƒ–ãƒ­ãƒƒã‚¯ç„¡åŠ¹
                "enable_drp": False
            }
        ]
    
    def _run_single_test(self, suppressor: AdvancedRepetitionSuppressorV3, test_case: Dict) -> Dict:
        """å˜ä¸€ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®å®Ÿè¡Œ"""
        start_time = time.time()
        
        # ãƒ‡ãƒãƒƒã‚°å¼·åŒ–ç‰ˆã§å®Ÿè¡Œ
        try:
            result_text, metrics = suppressor.suppress_repetitions_with_debug_v3(
                test_case["input"], 
                test_case.get("character")
            )
            
            # æˆåŠŸåˆ¤å®š
            success = self._evaluate_success(test_case, result_text, metrics)
            
            return {
                "test_case": test_case["name"],
                "category": test_case["category"],
                "difficulty": test_case["difficulty"],
                "input": test_case["input"],
                "output": result_text,
                "metrics": metrics,
                "success": success,
                "processing_time": time.time() - start_time,
                "improvement_detected": len(test_case["input"]) != len(result_text)
            }
            
        except Exception as e:
            return {
                "test_case": test_case["name"],
                "category": test_case["category"],
                "difficulty": test_case["difficulty"],
                "input": test_case["input"],
                "output": test_case["input"],  # å¤±æ•—æ™‚ã¯å…ƒã®ã¾ã¾
                "error": str(e),
                "success": False,
                "processing_time": time.time() - start_time,
                "improvement_detected": False
            }
    
    def _evaluate_success(self, test_case: Dict, result_text: str, metrics: SuppressionMetricsV3) -> bool:
        """æˆåŠŸåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆv3å‘ã‘å®Ÿç”¨çš„åŸºæº–ï¼‰"""
        
        # v3ã‚·ã‚¹ãƒ†ãƒ ã®åŸºæœ¬æˆåŠŸæ¡ä»¶ï¼ˆã‚ˆã‚Šå¯›å®¹ï¼‰
        if metrics.success_rate < 0.3:  # 30%ã‹ã‚‰æˆåŠŸã¨ã¿ãªã™
            return False
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®æˆåŠŸåˆ¤å®šï¼ˆå®Ÿç”¨çš„åŸºæº–ï¼‰
        category = test_case["category"]
        input_text = test_case["input"]
        
        if category == "poetry_protection":
            # è©©çš„è¡¨ç¾ã¯ä¿è­·é‡è¦–ï¼ˆ80%ä»¥ä¸Šæ®‹ã™ï¼‰
            return len(result_text) >= len(input_text) * 0.8
        
        # åŸºæœ¬çš„ãªæ”¹å–„æ¤œå‡ºï¼ˆä½•ã‚‰ã‹ã®å¤‰åŒ–ãŒã‚ã‚Œã°éƒ¨åˆ†çš„æˆåŠŸï¼‰
        text_improved = len(result_text) < len(input_text)
        patterns_found = metrics.patterns_detected > 0
        patterns_suppressed = metrics.patterns_suppressed > 0
        
        if category in ["same_word_repetition", "dialect_repetition", "ending_repetition"]:
            # åŒèªåå¾©ç³»ï¼šéƒ¨åˆ†çš„ãªæ”¹å–„ã§ã‚‚æˆåŠŸ
            expected_patterns = test_case.get("expected_patterns", [])
            
            # å®Œå…¨é™¤å»ãƒã‚§ãƒƒã‚¯ï¼ˆç†æƒ³çš„ï¼‰
            complete_removal = True
            for pattern in expected_patterns:
                if result_text.count(pattern) > 1:
                    complete_removal = False
                    break
            
            if complete_removal:
                return True
            
            # éƒ¨åˆ†çš„æ”¹å–„ãƒã‚§ãƒƒã‚¯ï¼ˆå®Ÿç”¨çš„ï¼‰
            reduction_detected = False
            for pattern in expected_patterns:
                input_count = input_text.count(pattern)
                output_count = result_text.count(pattern)
                if output_count < input_count:
                    reduction_detected = True
                    break
            
            return reduction_detected or (text_improved and patterns_suppressed > 0)
        
        if category in ["interjection_overuse", "symbol_repetition"]:
            # æ„Ÿå˜†è©ãƒ»è¨˜å·ï¼š30%ä»¥ä¸ŠçŸ­ç¸®ã§æˆåŠŸ
            compression_achieved = len(result_text) <= len(input_text) * 0.7
            return compression_achieved or (text_improved and patterns_suppressed > 0)
        
        if category == "extreme_case":
            # æ¥µç«¯ã‚±ãƒ¼ã‚¹ï¼š40%ä»¥ä¸Šã®çŸ­ç¸®ã§æˆåŠŸ
            significant_compression = len(result_text) <= len(input_text) * 0.6
            return significant_compression or (text_improved and patterns_suppressed > 0)
        
        if category in ["context_dependent", "semantic_repetition", "ngram_repetition"]:
            # è¤‡é›‘ã‚±ãƒ¼ã‚¹ï¼šæ¤œå‡º+ä½•ã‚‰ã‹ã®æ”¹å–„ã§æˆåŠŸ
            detection_success = patterns_found and metrics.success_rate >= 0.5
            improvement_success = text_improved and patterns_suppressed > 0
            return detection_success or improvement_success
        
        if category == "dense_repetition":
            # å¯†é›†åå¾©ï¼š20%ä»¥ä¸Šã®çŸ­ç¸®ã§æˆåŠŸ
            moderate_compression = len(result_text) <= len(input_text) * 0.8
            return moderate_compression or (text_improved and patterns_suppressed > 0)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ¤å®šï¼ˆã‚ˆã‚Šå¯›å®¹ï¼‰
        basic_success = (patterns_suppressed > 0 or text_improved) and metrics.over_compressions == 0
        advanced_success = metrics.success_rate >= 0.5 and metrics.patterns_detected > 0
        
        return basic_success or advanced_success
    
    def _analyze_config_results(self, results: List[Dict], config: Dict) -> Dict:
        """è¨­å®šåˆ¥çµæœåˆ†æ"""
        successful_tests = [r for r in results if r['success']]
        failed_tests = [r for r in results if not r['success']]
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
        category_stats = {}
        for result in results:
            category = result['category']
            if category not in category_stats:
                category_stats[category] = {'total': 0, 'success': 0}
            category_stats[category]['total'] += 1
            if result['success']:
                category_stats[category]['success'] += 1
        
        # é›£æ˜“åº¦åˆ¥åˆ†æ
        difficulty_stats = {}
        for result in results:
            difficulty = result['difficulty']
            if difficulty not in difficulty_stats:
                difficulty_stats[difficulty] = {'total': 0, 'success': 0}
            difficulty_stats[difficulty]['total'] += 1
            if result['success']:
                difficulty_stats[difficulty]['success'] += 1
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é›†è¨ˆ
        if successful_tests:
            avg_success_metrics = {
                'avg_patterns_detected': sum(r['metrics'].patterns_detected for r in successful_tests) / len(successful_tests),
                'avg_patterns_suppressed': sum(r['metrics'].patterns_suppressed for r in successful_tests) / len(successful_tests),
                'avg_processing_time_ms': sum(r['metrics'].processing_time_ms for r in successful_tests) / len(successful_tests)
            }
        else:
            avg_success_metrics = {}
        
        return {
            'overall_success_rate': len(successful_tests) / len(results),
            'successful_tests': len(successful_tests),
            'failed_tests': len(failed_tests),
            'category_performance': {
                cat: stats['success'] / stats['total'] 
                for cat, stats in category_stats.items()
            },
            'difficulty_performance': {
                diff: stats['success'] / stats['total'] 
                for diff, stats in difficulty_stats.items()
            },
            'avg_metrics': avg_success_metrics,
            'failed_test_names': [r['test_case'] for r in failed_tests]
        }
    
    def _generate_final_analysis(self, all_results: List[Dict], target_success_rate: float) -> Dict:
        """æœ€çµ‚åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        # ãƒ™ã‚¹ãƒˆè¨­å®šã®ç‰¹å®š
        best_config = max(all_results, key=lambda x: x['overall_success_rate'])
        
        # ç›®æ¨™é”æˆè¨­å®šã®ç‰¹å®š
        achieving_configs = [r for r in all_results if r['overall_success_rate'] >= target_success_rate]
        
        # å•é¡Œã‚«ãƒ†ã‚´ãƒªã®ç‰¹å®š
        problem_categories = {}
        for result in all_results:
            for category, performance in result['category_performance'].items():
                if category not in problem_categories:
                    problem_categories[category] = []
                problem_categories[category].append(performance)
        
        avg_category_performance = {
            cat: sum(perfs) / len(perfs) 
            for cat, perfs in problem_categories.items()
        }
        
        problematic_categories = [
            cat for cat, perf in avg_category_performance.items() 
            if perf < 0.7
        ]
        
        # æ¨å¥¨è¨­å®šã®ç”Ÿæˆ
        if achieving_configs:
            recommended_config = min(achieving_configs, key=lambda x: x['avg_metrics'].get('avg_processing_time_ms', float('inf')))
        else:
            recommended_config = best_config
        
        return {
            'test_summary': {
                'total_configurations_tested': len(all_results),
                'target_success_rate': target_success_rate,
                'achieving_configurations': len(achieving_configs),
                'best_success_rate': best_config['overall_success_rate']
            },
            'best_configuration': {
                'name': best_config['config'].get('name', 'Unknown'),
                'success_rate': best_config['overall_success_rate'],
                'config': best_config['config']
            },
            'recommended_configuration': {
                'name': recommended_config['config'].get('name', 'Unknown'),
                'success_rate': recommended_config['overall_success_rate'],
                'config': recommended_config['config'],
                'reason': 'target_achieved_fastest' if achieving_configs else 'best_available'
            },
            'problem_analysis': {
                'problematic_categories': problematic_categories,
                'category_performance': avg_category_performance,
                'improvement_suggestions': self._generate_improvement_suggestions(problematic_categories, best_config)
            },
            'performance_metrics': {
                'avg_success_rate_all_configs': sum(r['overall_success_rate'] for r in all_results) / len(all_results),
                'success_rate_range': [
                    min(r['overall_success_rate'] for r in all_results),
                    max(r['overall_success_rate'] for r in all_results)
                ]
            }
        }
    
    def _generate_improvement_suggestions(self, problematic_categories: List[str], best_config: Dict) -> List[str]:
        """æ”¹å–„ææ¡ˆã®ç”Ÿæˆ"""
        suggestions = []
        
        if 'same_word_repetition' in problematic_categories:
            suggestions.append("åŒèªåå¾©æ¤œå‡ºã‚’å¼·åŒ–: min_repeat_threshold ã‚’ 1 ã«ã€similarity_threshold ã‚’ 0.65 ä»¥ä¸‹ã«èª¿æ•´")
        
        if 'dialect_repetition' in problematic_categories:
            suggestions.append("æ–¹è¨€å¯¾å¿œå¼·åŒ–: é–¢è¥¿å¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¾æ›¸ã«è¿½åŠ ã€éŸ³éŸ»é¡ä¼¼åº¦é–¾å€¤ã‚’ä¸‹ã’ã‚‹")
        
        if 'ngram_repetition' in problematic_categories:
            suggestions.append("n-gramãƒ–ãƒ­ãƒƒã‚¯å¼·åŒ–: ngram_block_size ã‚’ 5-6 ã«å¢—åŠ ")
        
        if 'context_dependent' in problematic_categories:
            suggestions.append("æ–‡è„ˆç†è§£å¼·åŒ–: MeCab/UniDicå°å…¥ã€æ„å‘³çš„é¡ä¼¼åº¦è¨ˆç®—ã®æ”¹å–„")
        
        if 'poetry_protection' in problematic_categories:
            suggestions.append("è©©çš„è¡¨ç¾ä¿è­·: éå‰°åœ§ç¸®æ¤œå‡ºã®å¼·åŒ–ã€ä¿³å¥ãƒ»çŸ­æ­Œãƒ‘ã‚¿ãƒ¼ãƒ³ã®é™¤å¤–ãƒ«ãƒ¼ãƒ«è¿½åŠ ")
        
        if not suggestions:
            suggestions.append("âœ… ä¸»è¦ã‚«ãƒ†ã‚´ãƒªã§ã®å•é¡Œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        # å…¨ä½“çš„ãªææ¡ˆ
        if best_config['overall_success_rate'] < 0.8:
            suggestions.append(f"ç›®æ¨™æœªé”ï¼ˆç¾åœ¨: {best_config['overall_success_rate']:.1%}ï¼‰: ã‚ˆã‚Šå³æ ¼ãªè¨­å®šã‚’è©¦è¡Œ")
        
        return suggestions
    
    def _save_test_results(self, all_results: List[Dict], final_analysis: Dict):
        """ãƒ†ã‚¹ãƒˆçµæœã®ä¿å­˜"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        # è©³ç´°çµæœ
        detailed_results = {
            'timestamp': timestamp,
            'test_results': all_results,
            'final_analysis': final_analysis
        }
        
        filename = f"logs/repetition_test_v2_{timestamp}.json"
        os.makedirs("logs", exist_ok=True)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“„ è©³ç´°çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
        
        # ç°¡æ˜“ãƒ¬ãƒãƒ¼ãƒˆ
        self._generate_simple_report(final_analysis, f"logs/repetition_report_v2_{timestamp}.txt")
    
    def _generate_simple_report(self, analysis: Dict, filename: str):
        """ç°¡æ˜“ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("=== åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v2 ãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆ ===\n\n")
            
            # ã‚µãƒãƒªãƒ¼
            summary = analysis['test_summary']
            f.write(f"ãƒ†ã‚¹ãƒˆè¨­å®šæ•°: {summary['total_configurations_tested']}\n")
            f.write(f"ç›®æ¨™æˆåŠŸç‡: {summary['target_success_rate']:.1%}\n")
            f.write(f"ç›®æ¨™é”æˆè¨­å®šæ•°: {summary['achieving_configurations']}\n")
            f.write(f"æœ€é«˜æˆåŠŸç‡: {summary['best_success_rate']:.1%}\n\n")
            
            # æ¨å¥¨è¨­å®š
            recommended = analysis['recommended_configuration']
            f.write("=== æ¨å¥¨è¨­å®š ===\n")
            f.write(f"åå‰: {recommended['name']}\n")
            f.write(f"æˆåŠŸç‡: {recommended['success_rate']:.1%}\n")
            f.write(f"é¸å®šç†ç”±: {recommended['reason']}\n\n")
            
            # å•é¡Œåˆ†æ
            problems = analysis['problem_analysis']
            f.write("=== å•é¡Œã‚«ãƒ†ã‚´ãƒª ===\n")
            for category in problems['problematic_categories']:
                perf = problems['category_performance'][category]
                f.write(f"- {category}: {perf:.1%}\n")
            f.write("\n")
            
            # æ”¹å–„ææ¡ˆ
            f.write("=== æ”¹å–„ææ¡ˆ ===\n")
            for suggestion in problems['improvement_suggestions']:
                f.write(f"- {suggestion}\n")
        
        print(f"ğŸ“‹ ç°¡æ˜“ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")


def run_v2_evaluation():
    """v2è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè¡Œ"""
    print("ğŸš€ åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v2 è©•ä¾¡é–‹å§‹")
    print("ç›®æ¨™: æˆåŠŸç‡ 67.9% â†’ 80%+ ã¸ã®æ”¹å–„ç¢ºèª")
    print("=" * 60)
    
    tester = RepetitionEnhancementTesterV2()
    
    # 80%ç›®æ¨™ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    final_analysis = tester.run_comprehensive_test(target_success_rate=0.8)
    
    # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print("\n" + "=" * 60)
    print("ğŸ¯ æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 60)
    
    best = final_analysis['best_configuration']
    recommended = final_analysis['recommended_configuration']
    
    print(f"âœ… æœ€é«˜æˆåŠŸç‡: {best['success_rate']:.1%} ({best['name']})")
    print(f"ğŸ–ï¸ æ¨å¥¨è¨­å®š: {recommended['name']} ({recommended['success_rate']:.1%})")
    
    if recommended['success_rate'] >= 0.8:
        print("ğŸ‰ ç›®æ¨™æˆåŠŸç‡ 80% ã‚’é”æˆã—ã¾ã—ãŸï¼")
    else:
        gap = 0.8 - recommended['success_rate']
        print(f"ğŸ“ˆ ã‚ã¨ {gap:.1%} ã§ç›®æ¨™é”æˆã§ã™")
    
    # æ”¹å–„ææ¡ˆè¡¨ç¤º
    suggestions = final_analysis['problem_analysis']['improvement_suggestions']
    if suggestions:
        print("\nğŸ’¡ æ”¹å–„ææ¡ˆ:")
        for suggestion in suggestions[:3]:  # ä¸Šä½3ã¤
            print(f"   â€¢ {suggestion}")
    
    return final_analysis


if __name__ == "__main__":
    result = run_v2_evaluation() 