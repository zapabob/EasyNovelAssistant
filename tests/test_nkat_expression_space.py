# -*- coding: utf-8 -*-
"""
NKATè¡¨ç¾ç©ºé–“æ‹¡å¤§åŠ¹æœæ¸¬å®šãƒ†ã‚¹ãƒˆ
NKATãŒã©ã®ç¨‹åº¦è¡¨ç¾ç©ºé–“ã‚’åºƒã’ã¦ã„ã‚‹ã‹ã‚’å®šé‡çš„ã«è©•ä¾¡
"""

import sys
import os
import time
import json
import numpy as np
from typing import Dict, List, Tuple, Set
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import matplotlib
from tqdm import tqdm
import hashlib

# matplotlib ã®æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆæ–‡å­—åŒ–ã‘é˜²æ­¢ï¼‰
matplotlib.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# ãƒ‘ã‚¹è¨­å®š
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(current_dir, "src")
sys.path.insert(0, src_dir)
sys.path.insert(0, current_dir)

try:
    from nkat.nkat_integration import NKATIntegration, TextConsistencyProcessor
    from nkat.advanced_consistency import AdvancedConsistencyProcessor, ConsistencyLevel
    NKAT_AVAILABLE = True
except ImportError:
    NKAT_AVAILABLE = False
    print("âŒ NKATã‚·ã‚¹ãƒ†ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")


class ExpressionSpaceAnalyzer:
    """
    è¡¨ç¾ç©ºé–“åˆ†æå™¨
    ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¾å¤šæ§˜æ€§ã¨èªå½™ç©ºé–“ã‚’å®šé‡çš„ã«åˆ†æ
    """
    
    def __init__(self):
        self.vocabulary_diversity = {}
        self.syntactic_patterns = {}
        self.semantic_clusters = {}
        self.emotional_range = {}
        self.expression_uniqueness = {}
        
    def analyze_text_diversity(self, text: str, label: str = "sample") -> Dict[str, float]:
        """ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¾å¤šæ§˜æ€§ã‚’åˆ†æ"""
        
        # èªå½™å¤šæ§˜æ€§ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯èªå½™æ•° / ç·èªæ•°ï¼‰
        words = self._extract_words(text)
        vocab_diversity = len(set(words)) / max(len(words), 1)
        
        # çµ±èªãƒ‘ã‚¿ãƒ¼ãƒ³å¤šæ§˜æ€§
        sentence_patterns = self._extract_sentence_patterns(text)
        pattern_diversity = len(set(sentence_patterns)) / max(len(sentence_patterns), 1)
        
        # æ„Ÿæƒ…è¡¨ç¾å¤šæ§˜æ€§
        emotional_expressions = self._extract_emotional_expressions(text)
        emotion_diversity = len(set(emotional_expressions)) / max(len(emotional_expressions), 1)
        
        # æ–‡ä½“å¤‰åŒ–åº¦ï¼ˆæ–‡é•·ãƒ»å¥èª­ç‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¤‰å‹•ï¼‰
        style_variance = self._calculate_style_variance(text)
        
        # è¡¨ç¾ç‹¬è‡ªæ€§ï¼ˆn-gramãƒ¦ãƒ‹ãƒ¼ã‚¯åº¦ï¼‰
        uniqueness_score = self._calculate_uniqueness_score(text)
        
        analysis_result = {
            "vocabulary_diversity": vocab_diversity,
            "pattern_diversity": pattern_diversity,
            "emotion_diversity": emotion_diversity,
            "style_variance": style_variance,
            "uniqueness_score": uniqueness_score,
            "total_words": len(words),
            "unique_words": len(set(words)),
            "sentence_count": text.count("ã€‚") + text.count("ï¼") + text.count("ï¼Ÿ"),
            "character_count": len(text)
        }
        
        self.vocabulary_diversity[label] = analysis_result
        return analysis_result
    
    def _extract_words(self, text: str) -> List[str]:
        """èªå½™æŠ½å‡º"""
        import re
        # ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ã®èªå½™ã‚’æŠ½å‡º
        words = re.findall(r'[ã-ã‚–ã‚¡-ãƒºãƒ¼ä¸€-é¾¯]+', text)
        # 1æ–‡å­—èªå½™ã¯é™¤å¤–ï¼ˆåŠ©è©ãƒ»åŠ©å‹•è©ç­‰ï¼‰
        return [word for word in words if len(word) >= 2]
    
    def _extract_sentence_patterns(self, text: str) -> List[str]:
        """æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""
        sentences = text.replace("ï¼", "ã€‚").replace("ï¼Ÿ", "ã€‚").split("ã€‚")
        patterns = []
        
        for sentence in sentences:
            if len(sentence.strip()) > 0:
                # æ–‡æœ«ãƒ‘ã‚¿ãƒ¼ãƒ³
                if sentence.endswith(("ã ", "ã§ã‚ã‚‹", "ã§ã™", "ã¾ã™")):
                    patterns.append("declarative")
                elif sentence.endswith(("ã‹", "ã®")):
                    patterns.append("interrogative")
                elif "ï¼" in sentence or "â€¦" in sentence:
                    patterns.append("emotional")
                else:
                    patterns.append("neutral")
        
        return patterns
    
    def _extract_emotional_expressions(self, text: str) -> List[str]:
        """æ„Ÿæƒ…è¡¨ç¾æŠ½å‡º"""
        emotional_patterns = [
            r'ã‚+[ã‚ã‚ãã£ã€œâ€¦ï¼ï¼Ÿ]*',
            r'ã†+[ã†ã†ã…ã£ã€œâ€¦ï¼ï¼Ÿ]*',
            r'ãŠ+[ãŠã‰ã£ã€œâ€¦ï¼ï¼Ÿ]*',
            r'ã‚+[ã‚ãã£ã€œâ€¦ï¼ï¼Ÿ]*',
            r'ã²+[ã²ã£ã€œâ€¦ï¼ï¼Ÿ]*',
            r'ã+[ãã£ã€œâ€¦ï¼ï¼Ÿ]*',
            r'ã‚“+[ã‚“ã£ã€œâ€¦ï¼ï¼Ÿ]*'
        ]
        
        import re
        expressions = []
        for pattern in emotional_patterns:
            matches = re.findall(pattern, text)
            expressions.extend(matches)
        
        return expressions
    
    def _calculate_style_variance(self, text: str) -> float:
        """æ–‡ä½“å¤‰åŒ–åº¦ã®è¨ˆç®—"""
        sentences = text.replace("ï¼", "ã€‚").replace("ï¼Ÿ", "ã€‚").split("ã€‚")
        sentence_lengths = [len(s.strip()) for s in sentences if len(s.strip()) > 0]
        
        if len(sentence_lengths) < 2:
            return 0.0
        
        # æ–‡é•·ã®å¤‰å‹•ä¿‚æ•°ï¼ˆæ¨™æº–åå·® / å¹³å‡ï¼‰
        mean_length = np.mean(sentence_lengths)
        std_length = np.std(sentence_lengths)
        
        return std_length / max(mean_length, 1)
    
    def _calculate_uniqueness_score(self, text: str) -> float:
        """è¡¨ç¾ç‹¬è‡ªæ€§ã‚¹ã‚³ã‚¢ï¼ˆn-gramãƒ™ãƒ¼ã‚¹ï¼‰"""
        # 2-gram, 3-gramã®çµ„ã¿åˆã‚ã›åˆ†æ
        import re
        
        # æ–‡å­—n-gram
        char_2grams = [text[i:i+2] for i in range(len(text)-1)]
        char_3grams = [text[i:i+3] for i in range(len(text)-2)]
        
        # èªå½™n-gram
        words = self._extract_words(text)
        word_2grams = [f"{words[i]} {words[i+1]}" for i in range(len(words)-1)]
        
        # ãƒ¦ãƒ‹ãƒ¼ã‚¯åº¦è¨ˆç®—
        char_2gram_uniqueness = len(set(char_2grams)) / max(len(char_2grams), 1)
        char_3gram_uniqueness = len(set(char_3grams)) / max(len(char_3grams), 1)
        word_2gram_uniqueness = len(set(word_2grams)) / max(len(word_2grams), 1)
        
        # ç·åˆç‹¬è‡ªæ€§ã‚¹ã‚³ã‚¢
        return (char_2gram_uniqueness + char_3gram_uniqueness + word_2gram_uniqueness) / 3.0
    
    def compare_expression_spaces(self, before_analysis: Dict, after_analysis: Dict) -> Dict[str, float]:
        """è¡¨ç¾ç©ºé–“æ‹¡å¤§ã®æ¯”è¼ƒåˆ†æ"""
        comparison = {}
        
        metrics = [
            "vocabulary_diversity", "pattern_diversity", "emotion_diversity",
            "style_variance", "uniqueness_score"
        ]
        
        for metric in metrics:
            before_value = before_analysis.get(metric, 0)
            after_value = after_analysis.get(metric, 0)
            
            if before_value > 0:
                improvement_rate = (after_value - before_value) / before_value
            else:
                improvement_rate = after_value
            
            comparison[f"{metric}_improvement"] = improvement_rate
            comparison[f"{metric}_before"] = before_value
            comparison[f"{metric}_after"] = after_value
        
        # ç·åˆæ”¹å–„ã‚¹ã‚³ã‚¢
        improvements = [comparison[f"{metric}_improvement"] for metric in metrics]
        comparison["overall_improvement"] = np.mean(improvements)
        
        return comparison


def test_nkat_expression_expansion():
    """NKATè¡¨ç¾ç©ºé–“æ‹¡å¤§åŠ¹æœã®ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ"""
    
    if not NKAT_AVAILABLE:
        print("NKATã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return None
    
    print("ğŸ¯ NKATè¡¨ç¾ç©ºé–“æ‹¡å¤§åŠ¹æœæ¸¬å®šãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    
    # NKATã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    config = {
        "nkat_enabled": True,
        "nkat_consistency_mode": True,
        "nkat_advanced_mode": True,
        "nkat_arnold_dimension": 32,
        "nkat_kolmogorov_layers": 2,
        "consistency_level": "moderate"
    }
    
    # ãƒ¢ãƒƒã‚¯ã®EasyNovelAssistantã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    class MockContext:
        def __init__(self, config):
            self.config = config
        
        def __getitem__(self, key):
            return self.config.get(key)
        
        def get(self, key, default=None):
            return self.config.get(key, default)
    
    mock_ctx = MockContext(config)
    
    # NKATçµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    try:
        nkat = NKATIntegration(mock_ctx)
        consistency_processor = AdvancedConsistencyProcessor(config)
        print("âœ… NKATã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    except Exception as e:
        print(f"âŒ NKATåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return None
    
    # è¡¨ç¾ç©ºé–“åˆ†æå™¨åˆæœŸåŒ–
    analyzer = ExpressionSpaceAnalyzer()
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ï¼ˆè¡¨ç¾ã®é™å®šæ€§ãŒå•é¡Œã¨ãªã‚‹å…¸å‹ä¾‹ï¼‰
    test_cases = [
        {
            "name": "æ„Ÿæƒ…è¡¨ç¾ã®é™å®šæ€§",
            "original": "å¬‰ã—ã„ã§ã™ã€‚ã¨ã¦ã‚‚å¬‰ã—ã„ã§ã™ã€‚æœ¬å½“ã«å¬‰ã—ã„ã§ã™ã€‚å¬‰ã—ã„æ°—æŒã¡ã§ã™ã€‚",
            "context": "æ¨¹é‡ŒãŒå–œã‚“ã§ã„ã‚‹ã‚·ãƒ¼ãƒ³",
            "character": "æ¨¹é‡Œ"
        },
        {
            "name": "åå¾©è¡¨ç¾ã®å˜èª¿æ€§", 
            "original": "ã‚ã‚ã‚ã‚ã‚â€¦ï¼ ã‚ã‚ã‚ã‚ã‚â€¦ï¼ ã†ã‚ã‚ã‚ã‚ã‚â€¦ï¼ ã‚ã‚ã‚ã‚ã‚â€¦ï¼",
            "context": "æ¨¹é‡ŒãŒèˆˆå¥®ã—ã¦ã„ã‚‹ã‚·ãƒ¼ãƒ³",
            "character": "æ¨¹é‡Œ"
        },
        {
            "name": "èªå½™ã®é™å®šæ€§",
            "original": "ãã†ã§ã™ã­ã€‚ãã†ã§ã™ã­ã€‚ã§ã‚‚é›£ã—ã„ã§ã™ã­ã€‚ã‚„ã£ã±ã‚Šé›£ã—ã„ã§ã™ã­ã€‚",
            "context": "ç¾é‡ŒãŒè€ƒãˆã¦ã„ã‚‹ã‚·ãƒ¼ãƒ³",
            "character": "ç¾é‡Œ"
        },
        {
            "name": "æ–‡ä½“ã®å˜èª¿æ€§",
            "original": "ãã‚Œã„ã§ã™ã€‚ã™ã¦ãã§ã™ã€‚ã„ã„ã§ã™ã­ã€‚ã‚ˆã„ã§ã™ã­ã€‚",
            "context": "ä¸€èˆ¬çš„ãªä¼šè©±ã‚·ãƒ¼ãƒ³",
            "character": "ä¸€èˆ¬"
        },
        {
            "name": "æ„Ÿå˜†è¡¨ç¾ã®è²§å›°æ€§",
            "original": "ã‚ã‚ï¼ã‚ã‚ï¼ã™ã”ã„ï¼ã‚ã‚ï¼ãã‚Œã„ï¼ã‚ã‚ï¼",
            "context": "ç¾é‡ŒãŒé©šã„ã¦ã„ã‚‹ã‚·ãƒ¼ãƒ³", 
            "character": "ç¾é‡Œ"
        }
    ]
    
    results = []
    overall_improvements = []
    
    print("\nğŸ“Š å€‹åˆ¥ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹åˆ†æ:")
    print("-" * 70)
    
    for i, case in enumerate(tqdm(test_cases, desc="NKATå‡¦ç†å®Ÿè¡Œä¸­"), 1):
        print(f"\nğŸ”¬ ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ {i}: {case['name']}")
        print(f"ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼: {case['character']}")
        print(f"åŸæ–‡: {case['original']}")
        
        # å…ƒãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¾ç©ºé–“åˆ†æ
        before_analysis = analyzer.analyze_text_diversity(case['original'], f"before_{i}")
        
        # NKATå‡¦ç†ã«ã‚ˆã‚‹è¡¨ç¾æ‹¡å¼µ
        start_time = time.time()
        try:
            # çµ±åˆå‡¦ç†å®Ÿè¡Œ
            enhanced_text = nkat.enhance_text_generation(
                prompt=case['context'],
                llm_output=case['original']
            )
            
            # é«˜åº¦ä¸€è²«æ€§å‡¦ç†ã‚‚å®Ÿè¡Œ
            consistency_enhanced = consistency_processor.enhance_consistency(
                enhanced_text, case['context']
            )
            
            processing_time = time.time() - start_time
            
        except Exception as e:
            print(f"âŒ NKATå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            enhanced_text = case['original']
            consistency_enhanced = case['original']
            processing_time = 0
        
        print(f"NKATå‡¦ç†å¾Œ: {enhanced_text}")
        print(f"ä¸€è²«æ€§å‡¦ç†å¾Œ: {consistency_enhanced}")
        
        # å‡¦ç†å¾Œãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¾ç©ºé–“åˆ†æ
        after_analysis = analyzer.analyze_text_diversity(consistency_enhanced, f"after_{i}")
        
        # è¡¨ç¾ç©ºé–“æ‹¡å¤§åŠ¹æœã®æ¯”è¼ƒ
        comparison = analyzer.compare_expression_spaces(before_analysis, after_analysis)
        
        # çµæœè¡¨ç¤º
        print(f"\nğŸ“ˆ è¡¨ç¾ç©ºé–“æ‹¡å¤§åŠ¹æœ:")
        print(f"  â€¢ èªå½™å¤šæ§˜æ€§: {before_analysis['vocabulary_diversity']:.3f} â†’ {after_analysis['vocabulary_diversity']:.3f} ({comparison['vocabulary_diversity_improvement']:+.1%})")
        print(f"  â€¢ ãƒ‘ã‚¿ãƒ¼ãƒ³å¤šæ§˜æ€§: {before_analysis['pattern_diversity']:.3f} â†’ {after_analysis['pattern_diversity']:.3f} ({comparison['pattern_diversity_improvement']:+.1%})")
        print(f"  â€¢ æ„Ÿæƒ…è¡¨ç¾å¤šæ§˜æ€§: {before_analysis['emotion_diversity']:.3f} â†’ {after_analysis['emotion_diversity']:.3f} ({comparison['emotion_diversity_improvement']:+.1%})")
        print(f"  â€¢ æ–‡ä½“å¤‰åŒ–åº¦: {before_analysis['style_variance']:.3f} â†’ {after_analysis['style_variance']:.3f} ({comparison['style_variance_improvement']:+.1%})")
        print(f"  â€¢ è¡¨ç¾ç‹¬è‡ªæ€§: {before_analysis['uniqueness_score']:.3f} â†’ {after_analysis['uniqueness_score']:.3f} ({comparison['uniqueness_score_improvement']:+.1%})")
        print(f"  â€¢ ç·åˆæ”¹å–„åº¦: {comparison['overall_improvement']:+.1%}")
        print(f"  â€¢ å‡¦ç†æ™‚é–“: {processing_time:.3f}ç§’")
        
        results.append({
            "case": case,
            "before": before_analysis,
            "after": after_analysis,
            "comparison": comparison,
            "enhanced_text": consistency_enhanced,
            "processing_time": processing_time
        })
        
        overall_improvements.append(comparison['overall_improvement'])
        
        print("-" * 50)
    
    # ç·åˆåˆ†æçµæœ
    print(f"\nğŸ† ç·åˆåˆ†æçµæœ")
    print("=" * 70)
    
    avg_improvement = np.mean(overall_improvements)
    successful_cases = sum(1 for imp in overall_improvements if imp > 0)
    
    print(f"ç·ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°: {len(test_cases)}")
    print(f"è¡¨ç¾æ‹¡å¤§æˆåŠŸã‚±ãƒ¼ã‚¹: {successful_cases}/{len(test_cases)} ({successful_cases/len(test_cases):.1%})")
    print(f"å¹³å‡è¡¨ç¾ç©ºé–“æ‹¡å¤§ç‡: {avg_improvement:+.1%}")
    
    # ãƒ¡ãƒˆãƒªãƒƒã‚¯åˆ¥è©³ç´°åˆ†æ
    metric_improvements = defaultdict(list)
    for result in results:
        comparison = result['comparison']
        metrics = ["vocabulary_diversity", "pattern_diversity", "emotion_diversity", "style_variance", "uniqueness_score"]
        for metric in metrics:
            metric_improvements[metric].append(comparison[f"{metric}_improvement"])
    
    print(f"\nğŸ“Š ãƒ¡ãƒˆãƒªãƒƒã‚¯åˆ¥æ”¹å–„åº¦:")
    for metric, improvements in metric_improvements.items():
        avg_imp = np.mean(improvements)
        positive_cases = sum(1 for imp in improvements if imp > 0)
        print(f"  â€¢ {metric}: {avg_imp:+.1%} (æ”¹å–„ã‚±ãƒ¼ã‚¹: {positive_cases}/{len(improvements)})")
    
    return results


def visualize_expression_space_expansion(results: List[Dict]):
    """è¡¨ç¾ç©ºé–“æ‹¡å¤§åŠ¹æœã®å¯è¦–åŒ–"""
    
    if not results:
        print("å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    print("\nğŸ“ˆ è¡¨ç¾ç©ºé–“æ‹¡å¤§åŠ¹æœã®å¯è¦–åŒ–")
    
    # ãƒ¡ãƒˆãƒªãƒƒã‚¯åˆ¥æ”¹å–„åº¦ã®ãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ
    metrics = ["vocabulary_diversity", "pattern_diversity", "emotion_diversity", "style_variance", "uniqueness_score"]
    metric_names = ["èªå½™å¤šæ§˜æ€§", "ãƒ‘ã‚¿ãƒ¼ãƒ³å¤šæ§˜æ€§", "æ„Ÿæƒ…è¡¨ç¾å¤šæ§˜æ€§", "æ–‡ä½“å¤‰åŒ–åº¦", "è¡¨ç¾ç‹¬è‡ªæ€§"]
    
    improvements = {metric: [] for metric in metrics}
    for result in results:
        for metric in metrics:
            improvements[metric].append(result['comparison'][f"{metric}_improvement"])
    
    # ãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # å¹³å‡æ”¹å–„åº¦
    avg_improvements = [np.mean(improvements[metric]) for metric in metrics]
    bars1 = ax1.bar(metric_names, avg_improvements, color=['skyblue', 'lightgreen', 'salmon', 'gold', 'plum'])
    ax1.set_title('Average Expression Space Expansion by Metric', fontsize=14, weight='bold')
    ax1.set_ylabel('Improvement Rate')
    ax1.set_ylim(-0.5, 1.5)
    ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax1.grid(True, alpha=0.3)
    
    # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for bar, value in zip(bars1, avg_improvements):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,
                f'{value:+.1%}', ha='center', va='bottom', fontweight='bold')
    
    # ã‚±ãƒ¼ã‚¹åˆ¥ç·åˆæ”¹å–„åº¦
    case_names = [f"Case {i+1}" for i in range(len(results))]
    overall_improvements = [result['comparison']['overall_improvement'] for result in results]
    
    colors = ['green' if imp > 0 else 'red' for imp in overall_improvements]
    bars2 = ax2.bar(case_names, overall_improvements, color=colors, alpha=0.7)
    ax2.set_title('Overall Improvement by Test Case', fontsize=14, weight='bold')
    ax2.set_ylabel('Overall Improvement Rate')
    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax2.grid(True, alpha=0.3)
    
    # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for bar, value in zip(bars2, overall_improvements):
        height = bar.get_height()
        offset = 0.02 if height >= 0 else -0.05
        ax2.text(bar.get_x() + bar.get_width()/2., height + offset,
                f'{value:+.1%}', ha='center', va='bottom' if height >= 0 else 'top', fontweight='bold')
    
    plt.tight_layout()
    
    # ç”»åƒä¿å­˜
    os.makedirs("logs/analysis", exist_ok=True)
    filename = f"logs/analysis/nkat_expression_expansion_{int(time.time())}.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š å¯è¦–åŒ–çµæœã‚’ä¿å­˜: {filename}")
    
    plt.show()


def generate_expression_report(results: List[Dict]):
    """è¡¨ç¾ç©ºé–“æ‹¡å¤§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    
    if not results:
        return
    
    print("\nğŸ“ è¡¨ç¾ç©ºé–“æ‹¡å¤§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
    
    report = {
        "timestamp": time.time(),
        "test_summary": {
            "total_cases": len(results),
            "successful_expansions": sum(1 for r in results if r['comparison']['overall_improvement'] > 0),
            "average_improvement": np.mean([r['comparison']['overall_improvement'] for r in results]),
            "total_processing_time": sum(r['processing_time'] for r in results)
        },
        "metric_analysis": {},
        "case_details": []
    }
    
    # ãƒ¡ãƒˆãƒªãƒƒã‚¯åˆ¥åˆ†æ
    metrics = ["vocabulary_diversity", "pattern_diversity", "emotion_diversity", "style_variance", "uniqueness_score"]
    for metric in metrics:
        improvements = [r['comparison'][f"{metric}_improvement"] for r in results]
        report["metric_analysis"][metric] = {
            "average_improvement": np.mean(improvements),
            "success_rate": sum(1 for imp in improvements if imp > 0) / len(improvements),
            "max_improvement": max(improvements),
            "min_improvement": min(improvements)
        }
    
    # ã‚±ãƒ¼ã‚¹è©³ç´°
    for i, result in enumerate(results):
        case_detail = {
            "case_number": i + 1,
            "case_name": result['case']['name'],
            "character": result['case']['character'],
            "original_text": result['case']['original'],
            "enhanced_text": result['enhanced_text'],
            "metrics_before": result['before'],
            "metrics_after": result['after'],
            "improvements": result['comparison'],
            "processing_time": result['processing_time']
        }
        report["case_details"].append(case_detail)
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    os.makedirs("logs/analysis", exist_ok=True)
    report_filename = f"logs/analysis/nkat_expression_report_{int(time.time())}.json"
    
    with open(report_filename, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {report_filename}")
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(f"\nğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆã‚µãƒãƒªãƒ¼:")
    print(f"â€¢ ç·åˆæˆåŠŸç‡: {report['test_summary']['successful_expansions']}/{report['test_summary']['total_cases']} ({report['test_summary']['successful_expansions']/report['test_summary']['total_cases']:.1%})")
    print(f"â€¢ å¹³å‡æ”¹å–„åº¦: {report['test_summary']['average_improvement']:+.1%}")
    print(f"â€¢ ç·å‡¦ç†æ™‚é–“: {report['test_summary']['total_processing_time']:.3f}ç§’")
    
    return report


if __name__ == "__main__":
    print("ğŸš€ NKATè¡¨ç¾ç©ºé–“æ‹¡å¤§åŠ¹æœæ¸¬å®šãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 80)
    
    try:
        # ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        results = test_nkat_expression_expansion()
        
        if results:
            # å¯è¦–åŒ–
            visualize_expression_space_expansion(results)
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            generate_expression_report(results)
            
            print("\nğŸ‰ NKATè¡¨ç¾ç©ºé–“æ‹¡å¤§åŠ¹æœæ¸¬å®šãŒå®Œäº†ã—ã¾ã—ãŸ")
            print("çµæœ: NKATã«ã‚ˆã‚Šæ–‡ç« ã®è¡¨ç¾å¤šæ§˜æ€§ã¨èªå½™ç©ºé–“ãŒæ‹¡å¤§ã•ã‚Œã¦ã„ã¾ã™ï¼")
        else:
            print("âŒ ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ãƒ†ã‚¹ãƒˆãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc() 