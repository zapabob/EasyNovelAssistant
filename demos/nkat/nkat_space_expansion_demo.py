# -*- coding: utf-8 -*-
"""
NKATè¡¨ç¾ç©ºé–“æ‹¡å¤§å®Ÿè¨¼ãƒ‡ãƒ¢
NKATãŒã©ã®ç¨‹åº¦è¡¨ç¾ç©ºé–“ã‚’åºƒã’ã¦ã„ã‚‹ã‹ã‚’å®šé‡çš„ãƒ»è¦–è¦šçš„ã«å®Ÿè¨¼
"""

import sys
import os
import time
import json
import re
import random
from typing import Dict, List, Tuple
from collections import defaultdict
import numpy as np

# æ‹¡å¼µã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from enhanced_nkat_expression_engine import ExpressionExpansionEngine, NKATExpressionIntegration
    ENHANCED_ENGINE_AVAILABLE = True
except ImportError:
    ENHANCED_ENGINE_AVAILABLE = False

class NKATSpaceExpansionDemo:
    """NKATè¡¨ç¾ç©ºé–“æ‹¡å¤§å®Ÿè¨¼ãƒ‡ãƒ¢"""
    
    def __init__(self):
        self.expansion_engine = ExpressionExpansionEngine() if ENHANCED_ENGINE_AVAILABLE else None
        self.test_results = []
        
        print("ğŸŒŸ NKATè¡¨ç¾ç©ºé–“æ‹¡å¤§å®Ÿè¨¼ãƒ‡ãƒ¢åˆæœŸåŒ–å®Œäº†")
        print(f"Enhanced Engine: {'âœ… åˆ©ç”¨å¯èƒ½' if ENHANCED_ENGINE_AVAILABLE else 'âŒ åˆ©ç”¨ä¸å¯'}")
    
    def analyze_text_diversity(self, text: str) -> Dict[str, float]:
        """ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¾å¤šæ§˜æ€§ã‚’è©³ç´°åˆ†æ"""
        
        if not text or len(text.strip()) == 0:
            return {"vocabulary_diversity": 0, "pattern_diversity": 0, "emotion_diversity": 0, 
                   "style_variance": 0, "uniqueness_score": 0}
        
        # èªå½™æŠ½å‡º
        words = re.findall(r'[ã-ã‚–ã‚¡-ãƒºãƒ¼ä¸€-é¾¯]+', text)
        words = [word for word in words if len(word) >= 2]  # åŠ©è©ç­‰é™¤å¤–
        
        # 1. èªå½™å¤šæ§˜æ€§ (Type-Token Ratio)
        vocab_diversity = len(set(words)) / max(len(words), 1)
        
        # 2. æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³å¤šæ§˜æ€§
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        sentence_patterns = []
        for sentence in sentences:
            if len(sentence.strip()) > 0:
                if sentence.endswith(("ã§ã™", "ã¾ã™")):
                    sentence_patterns.append("polite")
                elif sentence.endswith(("ã ", "ã§ã‚ã‚‹")):
                    sentence_patterns.append("assertive")
                elif "â€¦" in sentence or "ã€œ" in sentence:
                    sentence_patterns.append("emotional")
                else:
                    sentence_patterns.append("neutral")
        
        pattern_diversity = len(set(sentence_patterns)) / max(len(sentence_patterns), 1)
        
        # 3. æ„Ÿæƒ…è¡¨ç¾å¤šæ§˜æ€§
        emotional_expressions = []
        # æ„Ÿå˜†ç¬¦ãƒ‘ã‚¿ãƒ¼ãƒ³
        emotional_expressions.extend(re.findall(r'[ï¼ï¼Ÿâ€¦ã€œãƒ¼]+', text))
        # æ„Ÿæƒ…èªå½™
        emotion_words = ["å¬‰ã—ã„", "æ‚²ã—ã„", "æ€’ã‚Š", "é©šã", "ã‚ã‚", "ã™ã”ã„", "ãã‚Œã„"]
        for word in emotion_words:
            if word in text:
                emotional_expressions.append(word)
        
        emotion_diversity = len(set(emotional_expressions)) / max(len(emotional_expressions), 1) if emotional_expressions else 0
        
        # 4. æ–‡ä½“å¤‰åŒ–åº¦ï¼ˆæ–‡é•·ã®å¤‰å‹•ä¿‚æ•°ï¼‰
        sentence_lengths = [len(s.strip()) for s in sentences if len(s.strip()) > 0]
        if len(sentence_lengths) >= 2:
            mean_length = np.mean(sentence_lengths)
            std_length = np.std(sentence_lengths)
            style_variance = std_length / max(mean_length, 1)
        else:
            style_variance = 0
        
        # 5. è¡¨ç¾ç‹¬è‡ªæ€§ï¼ˆn-gramãƒ¦ãƒ‹ãƒ¼ã‚¯åº¦ï¼‰
        if len(text) >= 3:
            char_2grams = [text[i:i+2] for i in range(len(text)-1)]
            char_3grams = [text[i:i+3] for i in range(len(text)-2)]
            
            char_2gram_uniqueness = len(set(char_2grams)) / max(len(char_2grams), 1)
            char_3gram_uniqueness = len(set(char_3grams)) / max(len(char_3grams), 1)
            
            uniqueness_score = (char_2gram_uniqueness + char_3gram_uniqueness) / 2
        else:
            uniqueness_score = 1.0
        
        return {
            "vocabulary_diversity": vocab_diversity,
            "pattern_diversity": pattern_diversity,
            "emotion_diversity": emotion_diversity,
            "style_variance": style_variance,
            "uniqueness_score": uniqueness_score,
            "total_words": len(words),
            "unique_words": len(set(words)),
            "sentence_count": len([s for s in sentences if s.strip()]),
            "character_count": len(text)
        }
    
    def calculate_expansion_rate(self, before: Dict, after: Dict) -> Dict[str, float]:
        """è¡¨ç¾ç©ºé–“æ‹¡å¤§ç‡ã®è¨ˆç®—"""
        
        improvements = {}
        metrics = ["vocabulary_diversity", "pattern_diversity", "emotion_diversity", 
                  "style_variance", "uniqueness_score"]
        
        for metric in metrics:
            before_val = before.get(metric, 0)
            after_val = after.get(metric, 0)
            
            if before_val > 0:
                improvement = (after_val - before_val) / before_val
            else:
                improvement = after_val
            
            improvements[f"{metric}_improvement"] = improvement
            improvements[f"{metric}_before"] = before_val
            improvements[f"{metric}_after"] = after_val
        
        # ç·åˆæ‹¡å¤§ç‡
        improvement_values = [improvements[f"{metric}_improvement"] for metric in metrics]
        improvements["overall_expansion"] = np.mean(improvement_values)
        
        return improvements
    
    def run_expansion_demo(self):
        """è¡¨ç¾ç©ºé–“æ‹¡å¤§ãƒ‡ãƒ¢ã®å®Ÿè¡Œ"""
        
        print("\nğŸš€ NKATè¡¨ç¾ç©ºé–“æ‹¡å¤§å®Ÿè¨¼ãƒ‡ãƒ¢é–‹å§‹")
        print("=" * 80)
        
        # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®šç¾©
        test_cases = [
            {
                "name": "æ„Ÿæƒ…è¡¨ç¾ã®å˜èª¿æ€§",
                "original": "å¬‰ã—ã„ã§ã™ã€‚ã¨ã¦ã‚‚å¬‰ã—ã„ã§ã™ã€‚æœ¬å½“ã«å¬‰ã—ã„ã§ã™ã€‚å¬‰ã—ã„æ°—æŒã¡ã§ã™ã€‚",
                "character": "æ¨¹é‡Œ",
                "description": "åŒä¸€æ„Ÿæƒ…è¡¨ç¾ã®é€£ç¶šä½¿ç”¨ã«ã‚ˆã‚‹è¡¨ç¾åŠ›ã®é™å®š"
            },
            {
                "name": "åå¾©è¡¨ç¾ã®å˜ç´”æ€§",
                "original": "ã‚ã‚ã‚ã‚ã‚â€¦ï¼ ã‚ã‚ã‚ã‚ã‚â€¦ï¼ ã†ã‚ã‚ã‚ã‚ã‚â€¦ï¼ ã‚ã‚ã‚ã‚ã‚â€¦ï¼",
                "character": "æ¨¹é‡Œ", 
                "description": "æ„Ÿå˜†è©ã®å˜ç´”åå¾©ã«ã‚ˆã‚‹æ„Ÿæƒ…è¡¨ç¾ã®è²§å›°åŒ–"
            },
            {
                "name": "èªå½™é¸æŠã®é™å®šæ€§",
                "original": "ãã†ã§ã™ã­ã€‚ãã†ã§ã™ã­ã€‚ã§ã‚‚é›£ã—ã„ã§ã™ã­ã€‚ã‚„ã£ã±ã‚Šé›£ã—ã„ã§ã™ã­ã€‚",
                "character": "ç¾é‡Œ",
                "description": "æ¥ç¶šè©ãƒ»å‰¯è©ã®åå¾©ã«ã‚ˆã‚‹èªå½™å¤šæ§˜æ€§ã®æ¬ å¦‚"
            },
            {
                "name": "æ–‡ä½“ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‡ä¸€æ€§",
                "original": "ãã‚Œã„ã§ã™ã€‚ã™ã¦ãã§ã™ã€‚ã„ã„ã§ã™ã­ã€‚ã‚ˆã„ã§ã™ã­ã€‚",
                "character": "ç¾é‡Œ",
                "description": "æ•¬èªãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‡ä¸€åŒ–ã«ã‚ˆã‚‹æ–‡ä½“å¤‰åŒ–ã®æ¬ å¦‚"
            },
            {
                "name": "æ„Ÿå˜†è¡¨ç¾ã®è²§å›°æ€§",
                "original": "ã‚ã‚ï¼ã‚ã‚ï¼ã™ã”ã„ï¼ã‚ã‚ï¼ãã‚Œã„ï¼ã‚ã‚ï¼",
                "character": "ç¾é‡Œ",
                "description": "é™å®šçš„ãªæ„Ÿå˜†èªã«ã‚ˆã‚‹æ„Ÿæƒ…è¡¨ç¾ã®å˜èª¿åŒ–"
            }
        ]
        
        successful_expansions = 0
        total_expansion_rate = 0
        
        for i, case in enumerate(test_cases, 1):
            print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ {i}: {case['name']}")
            print(f"ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼: {case['character']}")
            print(f"å•é¡Œ: {case['description']}")
            print(f"åŸæ–‡: ã€Œ{case['original']}ã€")
            
            # æ‹¡å¼µå‰åˆ†æ
            before_analysis = self.analyze_text_diversity(case['original'])
            
            # NKATè¡¨ç¾æ‹¡å¼µå®Ÿè¡Œ
            if ENHANCED_ENGINE_AVAILABLE:
                start_time = time.time()
                expanded_text = self.expansion_engine.expand_expression(
                    case['original'], case['character']
                )
                processing_time = time.time() - start_time
            else:
                expanded_text = case['original']
                processing_time = 0
            
            print(f"æ‹¡å¼µå¾Œ: ã€Œ{expanded_text}ã€")
            
            # æ‹¡å¼µå¾Œåˆ†æ
            after_analysis = self.analyze_text_diversity(expanded_text)
            
            # æ‹¡å¤§åŠ¹æœè¨ˆç®—
            expansion_results = self.calculate_expansion_rate(before_analysis, after_analysis)
            
            # çµæœè¡¨ç¤º
            print(f"\nğŸ“Š è¡¨ç¾ç©ºé–“æ‹¡å¤§åŠ¹æœ:")
            print(f"  ğŸ”¤ èªå½™å¤šæ§˜æ€§: {before_analysis['vocabulary_diversity']:.3f} â†’ {after_analysis['vocabulary_diversity']:.3f} ({expansion_results['vocabulary_diversity_improvement']:+.1%})")
            print(f"  ğŸ“ æ–‡å‹å¤šæ§˜æ€§: {before_analysis['pattern_diversity']:.3f} â†’ {after_analysis['pattern_diversity']:.3f} ({expansion_results['pattern_diversity_improvement']:+.1%})")
            print(f"  ğŸ’– æ„Ÿæƒ…å¤šæ§˜æ€§: {before_analysis['emotion_diversity']:.3f} â†’ {after_analysis['emotion_diversity']:.3f} ({expansion_results['emotion_diversity_improvement']:+.1%})")
            print(f"  ğŸ­ æ–‡ä½“å¤‰åŒ–åº¦: {before_analysis['style_variance']:.3f} â†’ {after_analysis['style_variance']:.3f} ({expansion_results['style_variance_improvement']:+.1%})")
            print(f"  âœ¨ è¡¨ç¾ç‹¬è‡ªæ€§: {before_analysis['uniqueness_score']:.3f} â†’ {after_analysis['uniqueness_score']:.3f} ({expansion_results['uniqueness_score_improvement']:+.1%})")
            print(f"  ğŸ† ç·åˆæ‹¡å¤§ç‡: {expansion_results['overall_expansion']:+.1%}")
            print(f"  â±ï¸ å‡¦ç†æ™‚é–“: {processing_time:.3f}ç§’")
            
            # æ‹¡å¤§æˆåŠŸåˆ¤å®š
            if expansion_results['overall_expansion'] > 0:
                successful_expansions += 1
                print("  âœ… è¡¨ç¾ç©ºé–“æ‹¡å¤§æˆåŠŸ")
            else:
                print("  âŒ è¡¨ç¾ç©ºé–“æ‹¡å¤§ãªã—")
            
            total_expansion_rate += expansion_results['overall_expansion']
            
            # çµæœä¿å­˜
            self.test_results.append({
                "case": case,
                "before": before_analysis,
                "after": after_analysis,
                "expansion": expansion_results,
                "expanded_text": expanded_text,
                "processing_time": processing_time
            })
            
            print("-" * 70)
        
        # ç·åˆçµæœ
        avg_expansion_rate = total_expansion_rate / len(test_cases)
        success_rate = successful_expansions / len(test_cases)
        
        print(f"\nğŸ† NKATè¡¨ç¾ç©ºé–“æ‹¡å¤§ç·åˆçµæœ")
        print("=" * 80)
        print(f"ğŸ“Š ç·ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°: {len(test_cases)}")
        print(f"âœ… æ‹¡å¤§æˆåŠŸã‚±ãƒ¼ã‚¹: {successful_expansions}/{len(test_cases)} ({success_rate:.1%})")
        print(f"ğŸ“ˆ å¹³å‡æ‹¡å¤§ç‡: {avg_expansion_rate:+.1%}")
        
        # è©³ç´°åˆ†æ
        if ENHANCED_ENGINE_AVAILABLE:
            engine_stats = self.expansion_engine.get_expansion_stats()
            print(f"\nğŸ”§ NKATæ‹¡å¼µã‚¨ãƒ³ã‚¸ãƒ³çµ±è¨ˆ:")
            print(f"  - ç·æ‹¡å¼µå®Ÿè¡Œæ•°: {engine_stats['total_expansions']}")
            print(f"  - èªå½™è¿½åŠ æ•°: {engine_stats['vocabulary_additions']}")
            print(f"  - æ–‡ä½“å¤‰åŒ–æ•°: {engine_stats['style_variations']}")
            print(f"  - æ„Ÿæƒ…å¼·åŒ–æ•°: {engine_stats['emotional_enhancements']}")
            print(f"  - ãƒ‘ã‚¿ãƒ¼ãƒ³å¤šæ§˜åŒ–æ•°: {engine_stats['pattern_diversifications']}")
        
        # çµè«–
        print(f"\nğŸ¯ çµè«–:")
        if success_rate >= 0.6:
            print("âœ… NKATã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹è¡¨ç¾ç©ºé–“ã®æ‹¡å¤§ãŒç¢ºèªã•ã‚Œã¾ã—ãŸï¼")
            print("ğŸ“ˆ æ–‡ç« ã®è¡¨ç¾å¤šæ§˜æ€§ã¨èªå½™ç©ºé–“ãŒæœ‰æ„ã«å‘ä¸Šã—ã¦ã„ã¾ã™ã€‚")
        elif success_rate >= 0.3:
            print("ğŸ”¶ NKATã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹éƒ¨åˆ†çš„ãªè¡¨ç¾ç©ºé–“æ‹¡å¤§ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚")
            print("ğŸ”§ æ›´ãªã‚‹æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚")
        else:
            print("âŒ ç¾åœ¨ã®NKATã‚·ã‚¹ãƒ†ãƒ ã§ã¯ååˆ†ãªè¡¨ç¾ç©ºé–“æ‹¡å¤§ãŒç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            print("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ ã®è¦‹ç›´ã—ãŒå¿…è¦ã§ã™ã€‚")
        
        return self.test_results
    
    def save_detailed_report(self, filepath: str = "logs/nkat_expansion_report.json"):
        """è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜"""
        
        if not self.test_results:
            print("âŒ ä¿å­˜å¯èƒ½ãªãƒ†ã‚¹ãƒˆçµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        report = {
            "timestamp": time.time(),
            "test_summary": {
                "total_cases": len(self.test_results),
                "successful_expansions": sum(1 for r in self.test_results if r['expansion']['overall_expansion'] > 0),
                "average_expansion": np.mean([r['expansion']['overall_expansion'] for r in self.test_results]),
                "enhanced_engine_used": ENHANCED_ENGINE_AVAILABLE
            },
            "detailed_results": self.test_results
        }
        
        # JSONä¿å­˜
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {filepath}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("ğŸŒŸ NKATè¡¨ç¾ç©ºé–“æ‹¡å¤§å®Ÿè¨¼ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 80)
    
    # ãƒ‡ãƒ¢å®Ÿè¡Œ
    demo = NKATSpaceExpansionDemo()
    results = demo.run_expansion_demo()
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    demo.save_detailed_report()
    
    print("\nğŸ‰ NKATè¡¨ç¾ç©ºé–“æ‹¡å¤§å®Ÿè¨¼ãƒ‡ãƒ¢ãŒå®Œäº†ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main() 