# -*- coding: utf-8 -*-
"""
LoRA Ã— NKAT å”èª¿ã‚·ã‚¹ãƒ†ãƒ  v2.5 é«˜åº¦ãƒ‡ãƒ¢
Î¸ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿæ§‹ã«ã‚ˆã‚‹å®Ÿéš›ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡
"""

import time
import json
import os
import sys
from typing import Dict, List, Any
from tqdm import tqdm

# ãƒ‘ã‚¹è¨­å®š
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(current_dir, "src")
sys.path.insert(0, src_dir)

try:
    from integration.lora_nkat_coordinator import (
        LoRANKATCoordinator, 
        StyleFeedbackConfig, 
        StyleMetrics
    )
    print("OK: v2.5 LoRAÃ—NKATã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼èª­ã¿è¾¼ã¿æˆåŠŸ")
except ImportError as e:
    print(f"ERROR: LoRAÃ—NKATã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    print("   åŸºæœ¬å®Ÿè£…ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
    
    # æœ€å°é™ã®äº’æ›å®Ÿè£…
    from dataclasses import dataclass
    
    @dataclass
    class StyleFeedbackConfig:
        theta_learning_rate: float = 0.001
        style_weight_sensitivity: float = 0.8
        feedback_momentum: float = 0.9
        bleurt_target: float = 0.87
        character_consistency_threshold: float = 0.95
        max_feedback_iterations: int = 100
        convergence_threshold: float = 1e-5
    
    @dataclass
    class StyleMetrics:
        bleurt_score: float
        character_consistency: float
        style_coherence: float
        readability_score: float
        emotional_stability: float
        theta_convergence: float
        feedback_efficiency: float
    
    class LoRANKATCoordinator:
        def __init__(self, config):
            self.config = config
            print("âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰ˆLoRAÃ—NKATã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼ä½¿ç”¨")
        
        def initialize_style_space(self, model_dim=768):
            return {'base': 0.5, 'style': 0.5, 'character': 0.5}
        
        def compute_style_metrics(self, text, character="default"):
            return StyleMetrics(0.8, 0.85, 0.8, 0.75, 0.8, 0.5, 0.7)
        
        def optimize_style_coordination(self, text, profile, max_iterations=None):
            return {'optimized': 0.8, 'enhanced': 0.75}
        
        def get_optimization_report(self):
            return {'status': 'fallback'}


class RealWorldStyleDemo:
    """å®Ÿä¸–ç•Œã§ã®æ–‡ä½“åˆ¶å¾¡ãƒ‡ãƒ¢ - å®Ÿéš›ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡ç‰ˆ"""
    
    def __init__(self):
        self.demo_name = "LoRAÃ—NKATå®Ÿä¸–ç•Œæ–‡ä½“åˆ¶å¾¡ v2.5"
        self.test_texts = self.create_realistic_test_texts()
        
    def create_realistic_test_texts(self) -> List[Dict[str, Any]]:
        """å®Ÿéš›ã®å°èª¬é¢¨ãƒ†ã‚­ã‚¹ãƒˆã§ã®ãƒ†ã‚¹ãƒˆ"""
        return [
            {
                "name": "ä¸å¯§èªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼",
                "character": "èŠ±å­",
                "text": "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ã€‚ä»Šæ—¥ã¯ã¨ã¦ã‚‚è‰¯ã„ãŠå¤©æ°—ã§ã™ã­ã€‚æ•£æ­©ã§ã‚‚ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ï¼Ÿ",
                "expected_formality": 0.9,
                "expected_emotion": 0.4,
                "context": "æœã®æŒ¨æ‹¶ã‚·ãƒ¼ãƒ³"
            },
            {
                "name": "é–¢è¥¿å¼ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼", 
                "character": "å¤ªéƒ",
                "text": "ãŠãŠã€ã‚ã£ã¡ã‚ƒãˆãˆå¤©æ°—ã‚„ã‚“ï¼ä»Šæ—¥ã¯çµ¶å¯¾å¤–å‡ºã‚„ã§ã€œã€‚",
                "expected_formality": 0.2,
                "expected_emotion": 0.8,
                "context": "é–¢è¥¿å¼ã§ã®æ—¥å¸¸ä¼šè©±"
            },
            {
                "name": "å­¦è¡“çš„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼",
                "character": "å…ˆç”Ÿ",
                "text": "æœ¬æ—¥ã®ç ”ç©¶çµæœã«ã¤ã„ã¦èª¬æ˜ã„ãŸã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿åˆ†æã«ã‚ˆã‚Šã€èˆˆå‘³æ·±ã„å‚¾å‘ãŒåˆ¤æ˜ã„ãŸã—ã¾ã—ãŸã€‚",
                "expected_formality": 0.95,
                "expected_emotion": 0.2,
                "context": "å­¦è¡“ç™ºè¡¨ã‚·ãƒ¼ãƒ³"
            },
            {
                "name": "å­ä¾›ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼",
                "character": "å­ä¾›",
                "text": "ã‚ãƒ¼ã„ï¼ä»Šæ—¥ã¯å…¬åœ’ã§éŠã¼ã†ã‚ˆï¼ãƒ–ãƒ©ãƒ³ã‚³ã¨ã‹æ»‘ã‚Šå°ã€æ¥½ã—ãã†ã ã­ã€œï¼",
                "expected_formality": 0.1,
                "expected_emotion": 0.9,
                "context": "å­ä¾›ã®æ—¥å¸¸ä¼šè©±"
            },
            {
                "name": "æ„Ÿæƒ…è¡¨ç¾è±Šã‹",
                "character": "æ„Ÿæƒ…è±Šã‹ã‚­ãƒ£ãƒ©",
                "text": "ã†ã‚ã‚ã‚ã‚ï¼æœ¬å½“ã«ã³ã£ãã‚Šã—ã¾ã—ãŸï¼ã“ã‚“ãªã“ã¨ã£ã¦ã‚ã‚‹ã‚“ã§ã™ã­ï¼Ÿï¼Ÿ",
                "expected_formality": 0.6,
                "expected_emotion": 1.0,
                "context": "é©šãã®ã‚·ãƒ¼ãƒ³"
            },
            {
                "name": "ã‚¯ãƒ¼ãƒ«ç³»ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼",
                "character": "ã‚¯ãƒ¼ãƒ«",
                "text": "ãã†ã§ã™ã‹ã€‚èˆˆå‘³æ·±ã„è©±ã§ã™ã­ã€‚ã—ã‹ã—ã€ã‚‚ã†å°‘ã—æ¤œè¨ãŒå¿…è¦ã§ã—ã‚‡ã†ã€‚",
                "expected_formality": 0.7,
                "expected_emotion": 0.1,
                "context": "å†·é™ãªåˆ†æã‚·ãƒ¼ãƒ³"
            }
        ]
    
    def run_real_metrics_demo(self, config: StyleFeedbackConfig) -> Dict[str, Any]:
        """å®Ÿéš›ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡ãƒ‡ãƒ¢å®Ÿè¡Œ"""
        print(f"DEMO: {self.demo_name}")
        print(f"   å®Ÿéš›ã®æ–‡ä½“å“è³ªè©•ä¾¡æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™")
        print("=" * 60)
        
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        coordinator = LoRANKATCoordinator(config)
        
        results = []
        total_start_time = time.time()
        
        print(f"\nğŸ“ å®Ÿãƒ†ã‚­ã‚¹ãƒˆæ–‡ä½“è©•ä¾¡ãƒ†ã‚¹ãƒˆ ({len(self.test_texts)}ä»¶)")
        
        for i, test_case in enumerate(self.test_texts, 1):
            print(f"\n--- ãƒ†ã‚¹ãƒˆ {i}/{len(self.test_texts)}: {test_case['name']} ---")
            print(f"ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼: {test_case['character']}")
            print(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {test_case['context']}")
            print(f"ãƒ†ã‚­ã‚¹ãƒˆ: \"{test_case['text']}\"")
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            start_time = time.time()
            metrics = coordinator.compute_style_metrics(
                test_case['text'], 
                test_case['character']
            )
            processing_time = time.time() - start_time
            
            # æœŸå¾…å€¤ã¨ã®æ¯”è¼ƒ
            expected_formality = test_case.get('expected_formality', 0.5)
            expected_emotion = test_case.get('expected_emotion', 0.5)
            
            # çµæœè¡¨ç¤º
            print(f"\nğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµæœ:")
            print(f"   BLEURTä»£æ›¿ã‚¹ã‚³ã‚¢: {metrics.bleurt_score:.3f}")
            print(f"   ã‚­ãƒ£ãƒ©ä¸€è²«æ€§: {metrics.character_consistency:.3f}")
            print(f"   æ–‡ä½“çµæŸæ€§: {metrics.style_coherence:.3f}")
            print(f"   å¯èª­æ€§: {metrics.readability_score:.3f}")
            print(f"   æ„Ÿæƒ…å®‰å®šæ€§: {metrics.emotional_stability:.3f}")
            print(f"   Î¸åæŸåº¦: {metrics.theta_convergence:.3f}")
            print(f"   FBåŠ¹ç‡: {metrics.feedback_efficiency:.3f}")
            print(f"   å‡¦ç†æ™‚é–“: {processing_time*1000:.1f}ms")
            
            # å“è³ªè©•ä¾¡
            overall_quality = (
                metrics.bleurt_score * 0.25 +
                metrics.character_consistency * 0.25 +
                metrics.style_coherence * 0.2 +
                metrics.readability_score * 0.15 +
                metrics.emotional_stability * 0.15
            )
            
            quality_grade = self.grade_quality(overall_quality)
            print(f"   ç·åˆå“è³ª: {overall_quality:.3f} ({quality_grade})")
            
            # çµæœè¨˜éŒ²
            result = {
                'test_name': test_case['name'],
                'character': test_case['character'],
                'text': test_case['text'],
                'context': test_case['context'],
                'metrics': {
                    'bleurt_score': metrics.bleurt_score,
                    'character_consistency': metrics.character_consistency,
                    'style_coherence': metrics.style_coherence,
                    'readability_score': metrics.readability_score,
                    'emotional_stability': metrics.emotional_stability,
                    'theta_convergence': metrics.theta_convergence,
                    'feedback_efficiency': metrics.feedback_efficiency
                },
                'overall_quality': overall_quality,
                'quality_grade': quality_grade,
                'processing_time_ms': processing_time * 1000,
                'expected_vs_actual': {
                    'expected_formality': expected_formality,
                    'expected_emotion': expected_emotion
                }
            }
            
            results.append(result)
        
        total_time = time.time() - total_start_time
        
        # çµ±è¨ˆåˆ†æ
        overall_stats = self.compute_detailed_statistics(results)
        
        print(f"\n" + "=" * 60)
        print(f"ğŸ“ˆ å®Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡ - ç·åˆçµ±è¨ˆ")
        print(f"   å¹³å‡BLEURT: {overall_stats['avg_bleurt']:.3f}")
        print(f"   å¹³å‡ã‚­ãƒ£ãƒ©ä¸€è²«æ€§: {overall_stats['avg_consistency']:.3f}")
        print(f"   å¹³å‡æ–‡ä½“çµæŸæ€§: {overall_stats['avg_coherence']:.3f}")
        print(f"   å¹³å‡å¯èª­æ€§: {overall_stats['avg_readability']:.3f}")
        print(f"   å¹³å‡æ„Ÿæƒ…å®‰å®šæ€§: {overall_stats['avg_emotional']:.3f}")
        print(f"   å¹³å‡ç·åˆå“è³ª: {overall_stats['avg_overall']:.3f}")
        print(f"   æœ€é«˜å“è³ª: {overall_stats['max_quality']:.3f}")
        print(f"   æœ€ä½å“è³ª: {overall_stats['min_quality']:.3f}")
        print(f"   å¹³å‡å‡¦ç†æ™‚é–“: {overall_stats['avg_processing_time']:.1f}ms")
        print(f"   ç·å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
        
        # å“è³ªåˆ†å¸ƒ
        print(f"\nğŸ† å“è³ªã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ:")
        grade_distribution = overall_stats['grade_distribution']
        for grade, count in grade_distribution.items():
            percentage = count / len(results) * 100
            print(f"   {grade}: {count}ä»¶ ({percentage:.1f}%)")
        
        # å®Ÿç”¨æ€§è©•ä¾¡
        usability_assessment = self.assess_practical_usability(overall_stats)
        print(f"\nğŸ’¼ å®Ÿç”¨æ€§è©•ä¾¡: {usability_assessment['level']}")
        print(f"   {usability_assessment['comment']}")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
        performance_assessment = self.assess_performance(overall_stats)
        print(f"\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡: {performance_assessment['level']}")
        print(f"   {usability_assessment['comment']}")
        
        return {
            'demo_name': self.demo_name,
            'test_results': results,
            'overall_statistics': overall_stats,
            'usability_assessment': usability_assessment,
            'performance_assessment': performance_assessment,
            'total_execution_time': total_time
        }
    
    def grade_quality(self, quality_score: float) -> str:
        """å“è³ªã‚¹ã‚³ã‚¢ã®ã‚°ãƒ¬ãƒ¼ãƒ‰åŒ–"""
        if quality_score >= 0.9:
            return "S (å“è¶Š)"
        elif quality_score >= 0.8:
            return "A (å„ªç§€)"
        elif quality_score >= 0.7:
            return "B (è‰¯å¥½)"
        elif quality_score >= 0.6:
            return "C (æ™®é€š)"
        elif quality_score >= 0.5:
            return "D (è¦æ”¹å–„)"
        else:
            return "F (ä¸è‰¯)"
    
    def compute_detailed_statistics(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è©³ç´°çµ±è¨ˆã®è¨ˆç®—"""
        if not results:
            return {}
        
        # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å¹³å‡å€¤
        avg_bleurt = sum(r['metrics']['bleurt_score'] for r in results) / len(results)
        avg_consistency = sum(r['metrics']['character_consistency'] for r in results) / len(results)
        avg_coherence = sum(r['metrics']['style_coherence'] for r in results) / len(results)
        avg_readability = sum(r['metrics']['readability_score'] for r in results) / len(results)
        avg_emotional = sum(r['metrics']['emotional_stability'] for r in results) / len(results)
        avg_overall = sum(r['overall_quality'] for r in results) / len(results)
        avg_processing_time = sum(r['processing_time_ms'] for r in results) / len(results)
        
        # æœ€é«˜ãƒ»æœ€ä½å“è³ª
        max_quality = max(r['overall_quality'] for r in results)
        min_quality = min(r['overall_quality'] for r in results)
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ
        grade_distribution = {}
        for result in results:
            grade = result['quality_grade'].split(' ')[0]  # "A (å„ªç§€)" -> "A"
            grade_distribution[grade] = grade_distribution.get(grade, 0) + 1
        
        return {
            'avg_bleurt': avg_bleurt,
            'avg_consistency': avg_consistency,
            'avg_coherence': avg_coherence,
            'avg_readability': avg_readability,
            'avg_emotional': avg_emotional,
            'avg_overall': avg_overall,
            'max_quality': max_quality,
            'min_quality': min_quality,
            'avg_processing_time': avg_processing_time,
            'grade_distribution': grade_distribution
        }
    
    def assess_practical_usability(self, stats: Dict[str, Any]) -> Dict[str, str]:
        """å®Ÿç”¨æ€§è©•ä¾¡"""
        avg_quality = stats.get('avg_overall', 0.0)
        
        if avg_quality >= 0.85:
            level = "å•†ç”¨ãƒ¬ãƒ™ãƒ«"
            comment = "å®Ÿéš›ã®å°èª¬ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã§å³åº§ã«åˆ©ç”¨å¯èƒ½ã§ã™ã€‚æ–‡ä½“åˆ¶å¾¡ã®ç²¾åº¦ãŒéå¸¸ã«é«˜ãã€å•†ç”¨ã‚µãƒ¼ãƒ“ã‚¹ã«é©ç”¨ã§ãã¾ã™ã€‚"
        elif avg_quality >= 0.75:
            level = "å®Ÿç”¨ãƒ¬ãƒ™ãƒ«"
            comment = "å€‹äººãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚„è©¦ä½œå“ã§å®Ÿç”¨å¯èƒ½ã§ã™ã€‚ã•ã‚‰ãªã‚‹èª¿æ•´ã§å•†ç”¨ãƒ¬ãƒ™ãƒ«ã«åˆ°é”ã§ãã‚‹è¦‹è¾¼ã¿ã§ã™ã€‚"
        elif avg_quality >= 0.65:
            level = "é–‹ç™ºãƒ¬ãƒ™ãƒ«"
            comment = "å®Ÿé¨“ãƒ»é–‹ç™ºç’°å¢ƒã§ã®ä½¿ç”¨ã«é©ã—ã¦ã„ã¾ã™ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã«ã‚ˆã‚Šå®Ÿç”¨ãƒ¬ãƒ™ãƒ«ã«æ”¹å–„å¯èƒ½ã§ã™ã€‚"
        else:
            level = "ç ”ç©¶ãƒ¬ãƒ™ãƒ«"
            comment = "ç ”ç©¶ãƒ»å®Ÿé¨“ç›®çš„ã§ã®ä½¿ç”¨ã«ç•™ã¾ã‚Šã¾ã™ã€‚åŸºæœ¬ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è¦‹ç›´ã—ãŒå¿…è¦ã§ã™ã€‚"
        
        return {'level': level, 'comment': comment}
    
    def assess_performance(self, stats: Dict[str, Any]) -> Dict[str, str]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡"""
        avg_time = stats.get('avg_processing_time', 0.0)
        
        if avg_time <= 50:
            level = "é«˜é€Ÿ"
            comment = f"å¹³å‡{avg_time:.1f}ms - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆã«é©ã—ãŸé«˜é€Ÿå‡¦ç†ã§ã™ã€‚"
        elif avg_time <= 100:
            level = "æ¨™æº–"
            comment = f"å¹³å‡{avg_time:.1f}ms - ä¸€èˆ¬çš„ãªç”¨é€”ã«é©ã—ãŸå‡¦ç†é€Ÿåº¦ã§ã™ã€‚"
        elif avg_time <= 200:
            level = "ã‚„ã‚„ä½é€Ÿ"
            comment = f"å¹³å‡{avg_time:.1f}ms - ãƒãƒƒãƒå‡¦ç†ã«é©ã—ã¦ã„ã¾ã™ãŒã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«ã¯ä¸å‘ãã§ã™ã€‚"
        else:
            level = "ä½é€Ÿ"
            comment = f"å¹³å‡{avg_time:.1f}ms - æœ€é©åŒ–ãŒå¿…è¦ã§ã™ã€‚GPUä¸¦åˆ—åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
        
        return {'level': level, 'comment': comment}


def save_demo_report(report: Dict[str, Any], filename: str = None):
    """ãƒ‡ãƒ¢ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜"""
    if filename is None:
        timestamp = int(time.time())
        filename = f"lora_nkat_real_metrics_demo_report_{timestamp}.json"
    
    try:
        os.makedirs('logs/lora_nkat_demos', exist_ok=True)
        filepath = f'logs/lora_nkat_demos/{filename}'
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ’¾ ãƒ‡ãƒ¢ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {filepath}")
    except Exception as e:
        print(f"\nâŒ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å¤±æ•—: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸ”¥ LoRA Ã— NKAT v2.5 å®Ÿä¸–ç•Œæ–‡ä½“åˆ¶å¾¡ãƒ‡ãƒ¢")
    print("   å®Ÿéš›ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ï¼")
    print("=" * 60)
    
    # è¨­å®š
    config = StyleFeedbackConfig(
        theta_learning_rate=0.002,
        style_weight_sensitivity=0.85,
        bleurt_target=0.87,
        character_consistency_threshold=0.90,
        max_feedback_iterations=30,
        convergence_threshold=1e-4
    )
    
    print("âš™ï¸ è¨­å®š:")
    print(f"   Î¸å­¦ç¿’ç‡: {config.theta_learning_rate}")
    print(f"   æ–‡ä½“æ„Ÿåº¦: {config.style_weight_sensitivity}")
    print(f"   BLEURTç›®æ¨™: {config.bleurt_target}")
    print(f"   ä¸€è²«æ€§é–¾å€¤: {config.character_consistency_threshold}")
    
    # ãƒ‡ãƒ¢å®Ÿè¡Œ
    demo = RealWorldStyleDemo()
    report = demo.run_real_metrics_demo(config)
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    save_demo_report(report)
    
    # æœ€çµ‚è©•ä¾¡
    overall_quality = report['overall_statistics']['avg_overall']
    usability_level = report['usability_assessment']['level']
    
    print(f"\nğŸ‰ v2.5å®Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡ãƒ‡ãƒ¢å®Œäº†ï¼")
    print(f"   ç·åˆå“è³ª: {overall_quality:.3f}")
    print(f"   å®Ÿç”¨æ€§: {usability_level}")
    
    if overall_quality >= 0.8:
        print(f"   ğŸ† å®Ÿç”¨ãƒ¬ãƒ™ãƒ«é”æˆï¼LoRAÃ—NKATæ–‡ä½“åˆ¶å¾¡ãŒå®Ÿä¸–ç•Œã§ä½¿ç”¨å¯èƒ½ã§ã™ï¼")
    else:
        print(f"   ğŸ“ˆ ã•ã‚‰ãªã‚‹æ”¹å–„ã§å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ã«åˆ°é”å¯èƒ½ã§ã™")
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 