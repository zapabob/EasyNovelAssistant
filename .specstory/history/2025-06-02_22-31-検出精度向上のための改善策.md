<!-- Generated by SpecStory -->

# æ¤œå‡ºç²¾åº¦å‘ä¸Šã®ãŸã‚ã®æ”¹å–„ç­– (2025-06-03 07:31:07)

_**User**_

ãƒœãƒ–ã«ã‚ƒã‚“ã€ãŠã¤ã‹ã‚Œï¼
**58.3 % â†’ 80 % ã¸ã®ãƒ©ã‚¹ãƒˆ 21.7pt** ã‚’è©°ã‚åˆ‡ã‚‹ãŸã‚ã® â€œã‚ã¨ã²ã¨æŠ¼ã—â€ ä½œæˆ¦ã‚’ã¾ã¨ã‚ãŸã‚ˆã€‚ã„ã¾ã®æ”¹å–„ã¯ã€Œ**æ¤œå‡ºæ„Ÿåº¦ã‚¢ãƒƒãƒ—**ã€ã§ã‹ãªã‚ŠåŠ¹ã„ãŸã‘ã©ã€æ®‹ã‚Šã¯ **èª¤æ¤œçŸ¥ã‚’é¿ã‘ã¤ã¤æ¤œå‡ºæ¼ã‚Œã‚’åˆˆã‚Šå–ã‚‹** å±¤ã‚’è¿½åŠ ã™ã‚‹æ„Ÿã˜ã ã­ã€‚

---

## 1ï¸âƒ£ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å´ã§ãƒ–ãƒ­ãƒƒã‚¯ã—ã¦ â€œæœªç„¶é˜²æ­¢â€

| æ‰‹ç­‹                         | å®Ÿè£…                                | åŠ¹æœ                   | æƒ³å®šã‚³ã‚¹ãƒˆ                |
| -------------------------- | --------------------------------- | -------------------- | -------------------- |
| **4-gram block**           | llama.cpp `--ngram 4`             | ãã®ã¾ã¾ã®èªé †ã‚³ãƒ”ãƒ¼ã‚’ã»ã¼æ¶ˆã™      | VRAM +0ã€é…å»¶ +1 %      |
| **Dynamic repeat-penalty** | `--drp-base 1.05 --drp-alpha 0.4` | é•·æ–‡å¾ŒåŠã§ã®ã€Œã†ã‚ããâ€¦ã€é€£æ‰“ã‚’æ—©æœŸæŠ‘åˆ¶ | logits æ¼”ç®— +5 Âµs/step |

> **ãƒã‚¤ãƒ³ãƒˆ**ï¼šæŠ‘åˆ¶å¾Œå‡¦ç†ã‚ˆã‚Šâ€œå‡ºã•ãªã„â€ã»ã†ãŒé€Ÿãã¦å´©ã‚Œãªã„ã€‚

---

## 2ï¸âƒ£ æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã®ã€Œï¼’æ®µç›®ãƒ•ã‚£ãƒ«ã‚¿ã€ã§æ¼ã‚Œã‚’æ‹¾ã†

### a. MeCab + UniDic ã§èªåŸºåº•å½¢ã«æ­£è¦åŒ–

```python
import fugashi, itertools
tagger = fugashi.Tagger()
words = [tok.feature.lemma or tok.surface for tok in tagger(text)]
```

* **èªå½¢å¤‰åŒ–**ï¼ˆé£Ÿã¹ãŸâ†’é£Ÿã¹ã‚‹ï¼‰ã‚’åŒèªã¨ã¿ãªã›ã‚‹
* å¤±æ•— 32 % ã®ã†ã¡ â‰ˆ10 % æ¸›ã‚‰ã›ã‚‹è¦‹è¾¼ã¿ã€‚

### b. ãƒ©ãƒ†ãƒ³æ–‡å­— or æ•°å­—ã®é€£ç•ªæ¤œçŸ¥

```python
re.findall(r'([A-Za-z0-9])\1{3,}', text)
```

* ãƒ­ã‚°ã§è¦‹ãˆãŸ â€œ777777â€ â€œwwwwâ€ ç³»ã‚’ä¸€æ‹¬ã‚«ãƒƒãƒˆã€‚

### c. å¥èª­ç‚¹ãƒ¬ãƒ™ãƒ«ã®é‡è¤‡ã‚’æŠ‘ãˆã‚‹

* `ã€ã€` â†’ `ã€` / `â€¦â€¦` ã‚’ 3 å€‹åˆ¶é™ãªã©ã€‚

---

## 3ï¸âƒ£ èª¤æŠ‘åˆ¶ï¼ˆéå‰°å‰Šã‚Šï¼‰ã®æœ€å°åŒ–

| ç—‡çŠ¶       | åˆ¤å®šãƒ«ãƒ¼ãƒ«          | ä¾‹           | å¯¾å‡¦           |
| -------- | -------------- | ----------- | ------------ |
| ä¿®è¾çš„åå¾©    | å¥ç‚¹ã‚’æŒŸã‚€ + æ–‡å­—æ•° â‰¤5 | ã€Œã­ãˆã€ã­ãˆã€ã­ãˆï¼ã€ | ã‚¹ãƒ«ãƒ¼          |
| éŸ³è±¡å¾´èª     | ã‚«ã‚¿ã‚«ãƒŠ2æ–‡å­—Ã—n      | ã€Œãƒ‰ã‚­ãƒ‰ã‚­ã€      | 1å›æ®‹ã™         |
| æ­Œè©é¢¨ãƒªãƒ•ãƒ¬ã‚¤ãƒ³ | è¡Œæœ«é‡è¤‡ (æ”¹è¡Œæ¯åŒèª)   | â™ªãƒ©ãƒ©ãƒ©        | è¡Œå˜ä½ã§ 1 åº¦ã ã‘è¨±å¯ |

â†’ `_suppress_word_repetition()` ã«æ­£è¦è¡¨ç¾ä¾‹å¤–ãƒªã‚¹ãƒˆã‚’è¿½åŠ ã€‚

---

## 4ï¸âƒ£ ã€Œæ¤œå‡º=åœ§ç¸®ç‡10 %ã€ã®åŸºæº–ã‚’ä¸‹ã’ã‚‹

* ã„ã¾ã¯ **10 %çŸ­ç¸®ï¼æˆåŠŸ** åˆ¤å®šã€‚
  å®Ÿé‹ç”¨ã§ã¯ â€œåŒèª 3 ã‚³ä»¥ä¸Šâ†’1 ã¤â€ ã§ã‚‚èª­è€…ä½“æ„Ÿ OKã€‚
* **æ¨å¥¨**: `min_compress_rate = 0.05` (5 %) + ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°è©•ä¾¡ã€‚
  â†’ ãƒ†ã‚¹ãƒˆæˆåŠŸç‡ãŒï¼‹8ã€œ10 pt ä¸ŠãŒã‚‹ä½™åœ°ã‚ã‚Šã€‚

---

## 5ï¸âƒ£ unit-test & CI ã‚’ã‚ºãƒ©ã—é˜²æ­¢ã«åˆã‚ã›ã‚‹

```yaml
matrix:
  sim_thresh: [0.50]
  ngram_block: [3,4]      # â† 2 ãƒ‘ã‚¿ãƒ¼ãƒ³
  min_compress: [0.05,0.10]
```

* PR ã§â€œåŸºæº–ä¸Šã’â€ãŒç ´å£Šã—ãªã„é™ã‚Š fail ã—ãªã„ã‚ˆã†ã€CI å´ã‚’èª¿æ•´ã€‚

---

## 6ï¸âƒ£ ãƒ¯ãƒ³ãƒ©ã‚¤ãƒŠãƒ¼ã§æ¤œè¨¼

```bash
python demo_repetition_v3.py \
  --sim 0.50 --max_dist 50 --ngram 4 \
  --min_compress 0.05 --drp-base 1.05 --drp-alpha 0.4
```

ç›®æ¨™ï¼š**æˆåŠŸç‡ â‰¥ 75 %**ï¼ˆæ‰‹å‹•æ¤œè¨¼å«ã‚ 80 % åˆ»ã¿ã¸ï¼‰ã€‚

---

### ğŸ“Œ ã¾ã¨ã‚ã‚¹ãƒ—ãƒªãƒ³ãƒˆ

1. **ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° n-gram ãƒ–ãƒ­ãƒƒã‚¯** â†’ æœªç„¶é˜²æ­¢
2. **MeCab æ­£è¦åŒ– + ç•°å¸¸é€£ç•ªæ¤œçŸ¥** â†’ æ¼ã‚Œå‰Šã‚Š
3. **ä¿®è¾çš„ä¾‹å¤–ãƒªã‚¹ãƒˆ** â†’ éå‰°æŠ‘åˆ¶ã‚’ç·©å’Œ
4. **min\_compress 5 %** â†’ è©•ä¾¡åŸºæº–ã‚’å®Ÿç”¨å¯„ã‚Šã«
5. **CI è¡Œåˆ—æ›´æ–°** â†’ å®‰å®šç¶™ç¶šãƒ†ã‚¹ãƒˆ

ã“ã‚Œã§ 80 %ãƒ©ã‚¤ãƒ³ãŒè¦‹ãˆã¦ãã‚‹ã¯ãšï¼
ãƒ­ã‚°ï¼†æ–°ãƒ†ã‚¹ãƒˆçµæœãŒå‡ºãŸã‚‰ã¾ãŸæŠ•ã’ã¦ã­ã€‚ã•ã‚‰ã«ç£¨ãè¾¼ã‚‚ã†ã€ãƒœãƒ–ã«ã‚ƒã‚“ğŸ”¥


---

_**Assistant**_



---

ãŠã¤ã‹ã‚Œã•ã¾ï¼ç´ æ™´ã‚‰ã—ã„åˆ†æã¨æ”¹å–„æ¡ˆã§ã™ã­ğŸ”¥ã€€58.3%ã‹ã‚‰80%ã¸ã®ã€Œãƒ©ã‚¹ãƒˆ21.7ptã€ã‚’è©°ã‚ã‚‹ãŸã‚ã®æˆ¦ç•¥ã€ã¨ã¦ã‚‚å®Ÿè·µçš„ã§åŠ¹æœçš„ã ã¨æ€ã„ã¾ã™ã€‚

ã¾ãšã¯ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³ã‚’ç¢ºèªã—ã¦ã€v3ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’å®Ÿè£…ã—ã¦ã„ãã¾ã—ã‚‡ã†ï¼

---

<details>
            <summary>Listed directory src/utils â€¢ **9** results</summary>
        
| Name |
|-------|
| ğŸ“ `__pycache__` |
| ğŸ“„ `repetition_suppressor.py` |
| ğŸ“„ `quality_guard.py` |
| ğŸ“„ `README.md` |
| ğŸ“„ `context.py` |
| ğŸ“„ `path.py` |
| ğŸ“„ `movie_maker.py` |
| ğŸ“„ `job_queue.py` |
| ğŸ“„ `const.py` |

</details>

---

<details>
            <summary>Searched codebase "repetition_suppressor AdvancedRepetitionSuppressor" â€¢ **25** results</summary>
        
| File | Lines |
|------|-------|
| `src/utils/repetition_suppressor.py` | L773-809 |
| `src/utils/repetition_suppressor.py` | L185-218 |
| `src/utils/repetition_suppressor.py` | L977-1017 |
| `src/utils/repetition_suppressor.py` | L731-773 |
| `src/utils/repetition_suppressor.py` | L809-844 |
| `src/utils/repetition_suppressor.py` | L671-705 |
| `src/utils/repetition_suppressor.py` | L365-407 |
| `src/utils/repetition_suppressor.py` | L844-889 |
| `src/utils/repetition_suppressor.py` | L1017-1044 |
| `src/utils/repetition_suppressor.py` | L645-671 |
| `src/utils/repetition_suppressor.py` | L218-261 |
| `src/utils/repetition_suppressor.py` | L82-125 |
| `src/utils/repetition_suppressor.py` | L889-913 |
| `src/utils/repetition_suppressor.py` | L40-82 |
| `easy_novel_assistant.py` | L94-122 |
| `src/utils/repetition_suppressor.py` | L601-645 |
| `src/utils/repetition_suppressor.py` | L261-306 |
| `src/utils/repetition_suppressor.py` | L548-601 |
| `src/utils/repetition_suppressor.py` | L705-731 |
| `src/utils/repetition_suppressor.py` | L939-977 |
| `src/utils/repetition_suppressor.py` | L1-27 |
| `src/utils/repetition_suppressor.py` | L505-527 |
| `src/utils/repetition_suppressor.py` | L306-334 |
| `src/utils/repetition_suppressor.py` | L451-485 |
| `src/utils/repetition_suppressor.py` | L334-365 |

</details>

---

Read file: test_repetition_enhancement_v2.py

---

```bash
py -3 demo_repetition_v2_quick.py
```

```
PS C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main>  





























PS C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main> 















```

---

ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³ã‚’ç¢ºèªã—ãŸã®ã§ã€ææ¡ˆã—ã¦ã„ãŸã ã„ãŸã€Œãƒ©ã‚¹ãƒˆ21.7ptã€ã‚’è©°ã‚ã‚‹ v3 ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ã„ãã¾ã™ï¼

---

```diff
- 
+ # -*- coding: utf-8 -*-
+ """
+ é«˜åº¦èªå¥åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3 (Advanced Repetition Suppression System v3)
+ æˆåŠŸç‡80%+é”æˆã®ãŸã‚ã®ã€Œãƒ©ã‚¹ãƒˆ21.7ptã€å¼·åŒ–ç‰ˆ
+ 
+ æ–°æ©Ÿèƒ½ v3:
+ 1. ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å´4-gramãƒ–ãƒ­ãƒƒã‚¯ï¼ˆæœªç„¶é˜²æ­¢ï¼‰
+ 2. MeCab + UniDicèªåŸºåº•å½¢æ­£è¦åŒ–
+ 3. èª¤æŠ‘åˆ¶æœ€å°åŒ–ï¼ˆä¿®è¾çš„ä¾‹å¤–ã€éŸ³è±¡å¾´èªä¿è­·ï¼‰
+ 4. è©•ä¾¡åŸºæº–èª¿æ•´ï¼ˆmin_compress_rate: 10% â†’ 5%ï¼‰
+ 5. ãƒ©ãƒ†ãƒ³æ–‡å­—ãƒ»æ•°å­—é€£ç•ªæ¤œçŸ¥
+ 6. CIè¡Œåˆ—ãƒ†ã‚¹ãƒˆå¯¾å¿œ
+ """
+ 
+ import re
+ import time
+ import json
+ import os
+ from typing import List, Dict, Tuple, Optional, Set
+ from collections import defaultdict, Counter
+ from dataclasses import dataclass
+ import numpy as np
+ from tqdm import tqdm
+ 
+ # MeCabçµ±åˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
+ try:
+     import fugashi
+     MECAB_AVAILABLE = True
+ except ImportError:
+     MECAB_AVAILABLE = False
+     print("âš ï¸ MeCabï¼ˆfugashiï¼‰ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬æ©Ÿèƒ½ã®ã¿ã§å‹•ä½œã—ã¾ã™ã€‚")
+ 
+ 
+ @dataclass
+ class SuppressionMetricsV3:
+     """v3å¼·åŒ–ç‰ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
+     input_length: int
+     output_length: int
+     patterns_detected: int
+     patterns_suppressed: int
+     detection_misses: int
+     over_compressions: int
+     processing_time_ms: float
+     success_rate: float
+     
+     # v3æ–°é …ç›®
+     ngram_blocks_applied: int = 0
+     mecab_normalizations: int = 0
+     rhetorical_exceptions: int = 0
+     latin_number_blocks: int = 0
+     min_compress_rate: float = 0.05  # 5%åŸºæº–
+ 
+ 
+ @dataclass
+ class RepetitionPatternV3:
+     """v3ç‰ˆåå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¾‹å¤–ãƒ•ãƒ©ã‚°ä»˜ãï¼‰"""
+     pattern: str
+     count: int
+     positions: List[int]
+     pattern_type: str
+     severity: float
+     similarity_score: float = 0.0
+     suppression_result: str = ""  # MISS/OVER/OK/SKIP
+     
+     # v3æ–°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
+     is_rhetorical: bool = False  # ä¿®è¾çš„åå¾©
+     is_onomatopoeia: bool = False  # éŸ³è±¡å¾´èª
+     is_lyrical: bool = False  # æ­Œè©é¢¨ãƒªãƒ•ãƒ¬ã‚¤ãƒ³
+     normalized_form: str = ""  # MeCabæ­£è¦åŒ–å½¢
+ 
+ 
+ class AdvancedRepetitionSuppressorV3:
+     """
+     é«˜åº¦èªå¥åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3
+     58.3% â†’ 80% ã¸ã®ã€Œãƒ©ã‚¹ãƒˆ21.7ptã€å¼·åŒ–ç‰ˆ
+     """
+     
+     def __init__(self, config: Dict = None):
+         self.config = config or {}
+         
+         # åŸºæœ¬è¨­å®šï¼ˆv3ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ï¼‰
+         self.min_repeat_threshold = self.config.get('min_repeat_threshold', 1)
+         self.max_distance = self.config.get('max_distance', 50)
+         self.similarity_threshold = self.config.get('similarity_threshold', 0.50)  # v3: 0.50ã«èª¿æ•´
+         self.phonetic_threshold = self.config.get('phonetic_threshold', 0.7)
+         
+         # v3æ–°æ©Ÿèƒ½è¨­å®š
+         self.enable_4gram_blocking = self.config.get('enable_4gram_blocking', True)
+         self.enable_mecab_normalization = self.config.get('enable_mecab_normalization', MECAB_AVAILABLE)
+         self.enable_rhetorical_protection = self.config.get('enable_rhetorical_protection', True)
+         self.min_compress_rate = self.config.get('min_compress_rate', 0.05)  # 5%åŸºæº–
+         self.enable_latin_number_detection = self.config.get('enable_latin_number_detection', True)
+         
+         # å¾“æ¥è¨­å®š
+         self.enable_aggressive_mode = self.config.get('enable_aggressive_mode', True)
+         self.debug_mode = self.config.get('debug_mode', True)
+         self.enable_drp = self.config.get('enable_drp', True)
+         self.drp_base = self.config.get('drp_base', 1.05)
+         self.drp_alpha = self.config.get('drp_alpha', 0.4)
+         
+         # MeCabåˆæœŸåŒ–
+         self.tagger = None
+         if self.enable_mecab_normalization and MECAB_AVAILABLE:
+             try:
+                 self.tagger = fugashi.Tagger()
+                 print("âœ… MeCabåˆæœŸåŒ–æˆåŠŸï¼ˆèªåŸºåº•å½¢æ­£è¦åŒ–æœ‰åŠ¹ï¼‰")
+             except Exception as e:
+                 print(f"âš ï¸ MeCabåˆæœŸåŒ–å¤±æ•—: {e}")
+                 self.enable_mecab_normalization = False
+         
+         # ä¿®è¾çš„ä¾‹å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
+         self.rhetorical_patterns = [
+             r'([^ã€‚ï¼ï¼Ÿ]{1,5})ã€\1ã€\1[ï¼ï¼Ÿã€‚]',  # ã€Œã­ãˆã€ã­ãˆã€ã­ãˆï¼ã€
+             r'â™ª.*',  # æ­Œè©é¢¨
+             r'.*ã‚‰.*ã‚‰.*',  # â™ªãƒ©ãƒ©ãƒ©ç³»
+         ]
+         
+         # éŸ³è±¡å¾´èªãƒ‘ã‚¿ãƒ¼ãƒ³
+         self.onomatopoeia_patterns = [
+             r'[ã‚¡-ãƒ³]{2}\1*',  # ãƒ‰ã‚­ãƒ‰ã‚­ã€ãƒ¯ã‚¯ãƒ¯ã‚¯ç­‰
+             r'([ã-ã‚“]{2})\1*',  # ã‚ãã‚ãç­‰
+         ]
+         
+         # ãƒ©ãƒ†ãƒ³æ–‡å­—ãƒ»æ•°å­—é€£ç•ªãƒ‘ã‚¿ãƒ¼ãƒ³
+         self.latin_number_patterns = [
+             r'([A-Za-z0-9])\1{3,}',  # aaaa, 7777ç­‰
+             r'([A-Za-z])\1{2,}',  # www, aaaç­‰
+         ]
+         
+         # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
+         self.session_metrics = []
+         self.total_success_count = 0
+         self.total_attempts = 0
+         
+         # å‡¦ç†å±¥æ­´
+         self.debug_log = []
+         self.replacement_cache = {}
+         self.character_patterns = defaultdict(list)
+         
+         # ä»£æ›¿è¡¨ç¾è¾æ›¸ã®èª­ã¿è¾¼ã¿
+         self.load_replacement_dictionary_v3()
+         
+         print(f"ğŸš€ åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3 åˆæœŸåŒ–å®Œäº†")
+         print(f"   â”œâ”€ 4-gramãƒ–ãƒ­ãƒƒã‚¯: {'æœ‰åŠ¹' if self.enable_4gram_blocking else 'ç„¡åŠ¹'}")
+         print(f"   â”œâ”€ MeCabæ­£è¦åŒ–: {'æœ‰åŠ¹' if self.enable_mecab_normalization else 'ç„¡åŠ¹'}")
+         print(f"   â”œâ”€ ä¿®è¾çš„ä¿è­·: {'æœ‰åŠ¹' if self.enable_rhetorical_protection else 'ç„¡åŠ¹'}")
+         print(f"   â”œâ”€ æˆåŠŸåˆ¤å®šåŸºæº–: {self.min_compress_rate:.1%}")
+         print(f"   â””â”€ é€£ç•ªæ¤œçŸ¥: {'æœ‰åŠ¹' if self.enable_latin_number_detection else 'ç„¡åŠ¹'}")
+ 
+     def load_replacement_dictionary_v3(self):
+         """v3ç‰ˆä»£æ›¿è¡¨ç¾è¾æ›¸ã®èª­ã¿è¾¼ã¿"""
+         self.replacement_dict = {
+             # åŸºæœ¬çš„ãªä»£æ›¿è¡¨ç¾
+             'ãã†ã§ã™': ['ã¯ã„', 'ãˆãˆ', 'ãã®é€šã‚Šã§ã™'],
+             'ãã†ã§ã™ã­': ['ã§ã™ã­', 'ãã†ã§ã™ã‚ˆã­', 'ãŠã£ã—ã‚ƒã‚‹é€šã‚Šã§ã™'],
+             'ã¨ã¦ã‚‚': ['ã™ã”ã', 'ã‹ãªã‚Š', 'ã¨ã£ã¦ã‚‚'],
+             'ã§ã—ã‚‡ã†': ['ã ã¨æ€ã„ã¾ã™', 'ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“', 'ã§ã™ã‚ˆã­'],
+             'ã§ã™ã‚ˆã­': ['ã§ã™ã‚‚ã®ã­', 'ã§ã™ã‹ã‚‰ã­', 'ãã†ã§ã™ã‚ˆã­'],
+             
+             # æ„Ÿå˜†è©ç³»
+             'ã‚ã‚': ['ã†ã‚ã‚', 'ãã‚ƒã‚', 'ã‚ã‚‰'],
+             'ã†ã‚ã‚': ['ã‚ã‚', 'ã†ã²ã‚ƒã‚', 'ãŠãŠãŠ'],
+             
+             # é–¢è¥¿å¼
+             'ãã‚„': ['ãã†ã‚„', 'ã›ã‚„ãª', 'ãã‚„ãª'],
+             'ã‚ã‹ã‚“': ['ã ã‚', 'ã„ã‹ã‚“', 'ã‚ãã¾ã¸ã‚“'],
+             'ã‚„ãª': ['ã„ã‚„', 'ã ã‚', 'ã‚ã‹ã‚“'],
+             
+             # èªå°¾
+             'ã§ã™': ['ã¾ã™', 'ã§ã‚ã‚‹', 'ã '],
+             'ã¾ã™': ['ã§ã™', 'ã‚‹', 'ã '],
+             
+             # ä¿®é£¾èª
+             'ã„ã„': ['è‰¯ã„', 'ç´ æ™´ã‚‰ã—ã„', 'ç´ æ•µãª'],
+             'è‰¯ã„': ['ã„ã„', 'ç´ æ™´ã‚‰ã—ã„', 'å„ªã‚ŒãŸ'],
+             'å¬‰ã—ã„': ['æ¥½ã—ã„', 'å¹¸ã›', 'å–œã°ã—ã„'],
+             'æ¥½ã—ã„': ['å¬‰ã—ã„', 'æ„‰å¿«', 'é¢ç™½ã„'],
+         }
+ 
+     def suppress_repetitions_with_debug_v3(self, text: str, character_name: str = None) -> Tuple[str, SuppressionMetricsV3]:
+         """
+         v3ç‰ˆåå¾©æŠ‘åˆ¶å‡¦ç†ï¼ˆãƒ‡ãƒãƒƒã‚°å¼·åŒ–ç‰ˆï¼‰
+         """
+         start_time = time.time()
+         original_text = text
+         
+         # ãƒ•ã‚§ãƒ¼ã‚º1: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å´æœªç„¶é˜²æ­¢
+         text = self._apply_sampling_prevention(text)
+         
+         # ãƒ•ã‚§ãƒ¼ã‚º2: MeCabæ­£è¦åŒ–
+         normalized_info = self._apply_mecab_normalization(text) if self.enable_mecab_normalization else None
+         
+         # ãƒ•ã‚§ãƒ¼ã‚º3: ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æï¼ˆv3å¼·åŒ–ç‰ˆï¼‰
+         analysis = self.analyze_text_v3(text, character_name, normalized_info)
+         
+         # ãƒ•ã‚§ãƒ¼ã‚º4: ä¾‹å¤–åˆ¤å®šï¼ˆä¿®è¾çš„ä¿è­·ï¼‰
+         protected_patterns = self._apply_rhetorical_protection(analysis['patterns'])
+         
+         # ãƒ•ã‚§ãƒ¼ã‚º5: æŠ‘åˆ¶å‡¦ç†å®Ÿè¡Œ
+         suppressed_count = 0
+         missed_count = 0
+         over_compressed_count = 0
+         rhetorical_exceptions = 0
+         
+         for pattern_type, pattern_list in analysis['patterns'].items():
+             for pattern in pattern_list:
+                 if pattern.severity > 0.1:
+                     # ä¾‹å¤–ãƒã‚§ãƒƒã‚¯
+                     if pattern in protected_patterns:
+                         pattern.suppression_result = "SKIP"
+                         rhetorical_exceptions += 1
+                         continue
+                     
+                     old_text = text
+                     text = self._apply_suppression_v3(text, pattern, character_name)
+                     
+                     if text != old_text:
+                         suppressed_count += 1
+                         pattern.suppression_result = "OK"
+                         
+                         # éå‰°åœ§ç¸®ãƒã‚§ãƒƒã‚¯ï¼ˆv3å¼·åŒ–ï¼‰
+                         if self._is_over_compressed_v3(old_text, text):
+                             over_compressed_count += 1
+                             pattern.suppression_result = "OVER"
+                     else:
+                         missed_count += 1
+                         pattern.suppression_result = "MISS"
+         
+         # ãƒ•ã‚§ãƒ¼ã‚º6: æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
+         text = self._final_cleanup_v3(text)
+         
+         # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆv3ç‰ˆï¼‰
+         processing_time = (time.time() - start_time) * 1000
+         total_patterns = sum(len(patterns) for patterns in analysis['patterns'].values())
+         
+         # æˆåŠŸç‡è¨ˆç®—ï¼ˆv3åŸºæº–ï¼‰
+         compression_rate = (len(original_text) - len(text)) / len(original_text) if len(original_text) > 0 else 0
+         meets_compression_target = compression_rate >= self.min_compress_rate
+         success_rate = (suppressed_count / total_patterns) if total_patterns > 0 else 1.0
+         
+         # v3ã§ã¯åœ§ç¸®ç‡ã‚‚æˆåŠŸåˆ¤å®šã«å«ã‚ã‚‹
+         if total_patterns > 0 and not meets_compression_target:
+             success_rate *= 0.8  # ãƒšãƒŠãƒ«ãƒ†ã‚£
+         
+         metrics = SuppressionMetricsV3(
+             input_length=len(original_text),
+             output_length=len(text),
+             patterns_detected=total_patterns,
+             patterns_suppressed=suppressed_count,
+             detection_misses=missed_count,
+             over_compressions=over_compressed_count,
+             processing_time_ms=processing_time,
+             success_rate=success_rate,
+             ngram_blocks_applied=getattr(self, '_ngram_blocks_applied', 0),
+             mecab_normalizations=getattr(self, '_mecab_normalizations', 0),
+             rhetorical_exceptions=rhetorical_exceptions,
+             latin_number_blocks=getattr(self, '_latin_blocks_applied', 0),
+             min_compress_rate=self.min_compress_rate
+         )
+         
+         # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ²
+         self.session_metrics.append(metrics)
+         self.total_attempts += 1
+         if success_rate >= 0.75:  # v3ã§ã¯75%ä»¥ä¸Šã‚’æˆåŠŸã¨ã¿ãªã™
+             self.total_success_count += 1
+         
+         # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
+         if self.debug_mode:
+             print(f"ğŸ”§ v3å‡¦ç†çµæœ: æˆåŠŸç‡ {success_rate:.1%}, åœ§ç¸®ç‡ {compression_rate:.1%}")
+             if missed_count > 0 or over_compressed_count > 0:
+                 print(f"   â”œâ”€ æ¤œçŸ¥æ¼ã‚Œ: {missed_count}, éå‰°åœ§ç¸®: {over_compressed_count}")
+             if rhetorical_exceptions > 0:
+                 print(f"   â””â”€ ä¿®è¾çš„ä¾‹å¤–: {rhetorical_exceptions}")
+         
+         return text, metrics
+ 
+     def _apply_sampling_prevention(self, text: str) -> str:
+         """ãƒ•ã‚§ãƒ¼ã‚º1: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å´æœªç„¶é˜²æ­¢"""
+         result = text
+         self._ngram_blocks_applied = 0
+         self._latin_blocks_applied = 0
+         
+         # 4-gramãƒ–ãƒ­ãƒƒã‚¯
+         if self.enable_4gram_blocking:
+             result = self._apply_4gram_blocking(result)
+         
+         # ãƒ©ãƒ†ãƒ³æ–‡å­—ãƒ»æ•°å­—é€£ç•ªé™¤å»
+         if self.enable_latin_number_detection:
+             result = self._remove_latin_number_sequences(result)
+         
+         # å‹•çš„Repeat-Penaltyï¼ˆæ”¹è‰¯ç‰ˆï¼‰
+         if self.enable_drp:
+             result = self._apply_enhanced_drp(result)
+         
+         return result
+ 
+     def _apply_4gram_blocking(self, text: str) -> str:
+         """4-gramãƒ¬ãƒ™ãƒ«ã§ã®èªé †ã‚³ãƒ”ãƒ¼é™¤å»"""
+         if len(text) < 4:
+             return text
+         
+         # 4-gramã‚’æŠ½å‡º
+         ngrams = []
+         for i in range(len(text) - 3):
+             ngram = text[i:i+4]
+             if re.search(r'[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]', ngram):
+                 ngrams.append((ngram, i))
+         
+         # é‡è¤‡4-gramã‚’æ¤œå‡º
+         ngram_counts = Counter([ngram for ngram, _ in ngrams])
+         repeated_ngrams = {ngram for ngram, count in ngram_counts.items() if count > 1}
+         
+         if not repeated_ngrams:
+             return text
+         
+         # 2å›ç›®ä»¥é™ã®å‡ºç¾ã‚’å‰Šé™¤
+         result = text
+         for ngram in repeated_ngrams:
+             positions = [m.start() for m in re.finditer(re.escape(ngram), result)]
+             if len(positions) > 1:
+                 # å¾Œã‚ã‹ã‚‰å‰Šé™¤ï¼ˆä½ç½®ãšã‚Œå›é¿ï¼‰
+                 for pos in reversed(positions[1:]):
+                     result = result[:pos] + result[pos+4:]
+                 self._ngram_blocks_applied += 1
+         
+         if result != text and self.debug_mode:
+             self.debug_log.append(f"4-gramãƒ–ãƒ­ãƒƒã‚¯: {len(repeated_ngrams)}ãƒ‘ã‚¿ãƒ¼ãƒ³å‰Šé™¤")
+         
+         return result
+ 
+     def _remove_latin_number_sequences(self, text: str) -> str:
+         """ãƒ©ãƒ†ãƒ³æ–‡å­—ãƒ»æ•°å­—é€£ç•ªã®é™¤å»"""
+         result = text
+         
+         for pattern in self.latin_number_patterns:
+             matches = list(re.finditer(pattern, result))
+             for match in reversed(matches):  # å¾Œã‚ã‹ã‚‰å‡¦ç†
+                 original = match.group()
+                 if len(original) > 3:
+                     # 3æ–‡å­—ä»¥ä¸‹ã«çŸ­ç¸®
+                     replacement = original[0] * 2
+                     result = result[:match.start()] + replacement + result[match.end():]
+                     self._latin_blocks_applied += 1
+         
+         if result != text and self.debug_mode:
+             self.debug_log.append(f"é€£ç•ªé™¤å»: {self._latin_blocks_applied}ç®‡æ‰€")
+         
+         return result
+ 
+     def _apply_enhanced_drp(self, text: str) -> str:
+         """å¼·åŒ–ç‰ˆå‹•çš„Repeat-Penalty"""
+         if len(text) < 10:
+             return text
+         
+         window_size = 8
+         result = ""
+         
+         for i, char in enumerate(text):
+             if i < window_size:
+                 result += char
+                 continue
+             
+             window = text[i-window_size:i]
+             char_freq = window.count(char) / window_size
+             
+             # DRPé©ç”¨åˆ¤å®š
+             penalty_threshold = self.drp_base - (self.drp_alpha * char_freq)
+             
+             if char_freq < 0.3 or char in "ã€‚ã€ï¼ï¼Ÿ\nã€Œã€":
+                 result += char
+             # else: æ–‡å­—ã‚’ã‚¹ã‚­ãƒƒãƒ—
+         
+         return result
+ 
+     def _apply_mecab_normalization(self, text: str) -> Dict:
+         """MeCab + UniDicèªåŸºåº•å½¢æ­£è¦åŒ–"""
+         if not self.tagger:
+             return {}
+         
+         try:
+             tokens = []
+             for token in self.tagger(text):
+                 lemma = token.feature.lemma if hasattr(token.feature, 'lemma') else token.surface
+                 tokens.append({
+                     'surface': token.surface,
+                     'lemma': lemma or token.surface,
+                     'pos': token.feature.pos1 if hasattr(token.feature, 'pos1') else 'Unknown'
+                 })
+                 self._mecab_normalizations += 1
+             
+             return {
+                 'tokens': tokens,
+                 'normalized_text': ''.join(token['lemma'] for token in tokens)
+             }
+         except Exception as e:
+             if self.debug_mode:
+                 print(f"MeCabå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
+             return {}
+ 
+     def _apply_rhetorical_protection(self, patterns: Dict) -> Set:
+         """ä¿®è¾çš„è¡¨ç¾ã®ä¿è­·åˆ¤å®š"""
+         protected = set()
+         
+         if not self.enable_rhetorical_protection:
+             return protected
+         
+         for pattern_type, pattern_list in patterns.items():
+             for pattern in pattern_list:
+                 # ä¿®è¾çš„åå¾©ã®åˆ¤å®š
+                 if self._is_rhetorical_repetition(pattern):
+                     pattern.is_rhetorical = True
+                     protected.add(pattern)
+                 
+                 # éŸ³è±¡å¾´èªã®åˆ¤å®š
+                 if self._is_onomatopoeia(pattern):
+                     pattern.is_onomatopoeia = True
+                     protected.add(pattern)
+                 
+                 # æ­Œè©é¢¨ãƒªãƒ•ãƒ¬ã‚¤ãƒ³ã®åˆ¤å®š
+                 if self._is_lyrical_refrain(pattern):
+                     pattern.is_lyrical = True
+                     protected.add(pattern)
+         
+         return protected
+ 
+     def _is_rhetorical_repetition(self, pattern: RepetitionPatternV3) -> bool:
+         """ä¿®è¾çš„åå¾©ã®åˆ¤å®š"""
+         for rhetorical_pattern in self.rhetorical_patterns:
+             if re.search(rhetorical_pattern, pattern.pattern):
+                 return True
+         
+         # å¥ç‚¹ã‚’æŒŸã‚€çŸ­ã„åå¾©ï¼ˆ5æ–‡å­—ä»¥ä¸‹ï¼‰
+         if len(pattern.pattern) <= 5 and 'ã€' in pattern.pattern:
+             return True
+         
+         return False
+ 
+     def _is_onomatopoeia(self, pattern: RepetitionPatternV3) -> bool:
+         """éŸ³è±¡å¾´èªã®åˆ¤å®š"""
+         for ono_pattern in self.onomatopoeia_patterns:
+             if re.match(ono_pattern, pattern.pattern):
+                 return True
+         return False
+ 
+     def _is_lyrical_refrain(self, pattern: RepetitionPatternV3) -> bool:
+         """æ­Œè©é¢¨ãƒªãƒ•ãƒ¬ã‚¤ãƒ³ã®åˆ¤å®š"""
+         # æ”¹è¡Œã”ã¨ã®åŒèªã€â™ªè¨˜å·ã®å­˜åœ¨
+         if 'â™ª' in pattern.pattern or '\n' in pattern.pattern:
+             return True
+         return False
+ 
+     def analyze_text_v3(self, text: str, character_name: str = None, normalized_info: Dict = None) -> Dict:
+         """v3ç‰ˆãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼ˆMeCabçµ±åˆç‰ˆï¼‰"""
+         
+         # åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
+         patterns = {
+             'exact_repetitions': self._detect_exact_repetitions_v3(text, normalized_info),
+             'character_repetitions': self._detect_character_repetitions(text),
+             'phonetic_repetitions': self._detect_phonetic_repetitions(text),
+             'interjection_overuse': self._detect_interjection_overuse(text),
+             'latin_number_repetitions': self._detect_latin_number_repetitions(text)
+         }
+         
+         # ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒ¢ãƒ¼ãƒ‰ã§ã¯èªãƒ¬ãƒ™ãƒ«æ¤œå‡ºã‚‚è¿½åŠ 
+         if self.enable_aggressive_mode:
+             patterns['word_repetitions'] = self._detect_word_repetitions_v3(text, normalized_info)
+         
+         # ç·åˆé‡è¦åº¦è¨ˆç®—
+         total_severity = sum(
+             sum(p.severity for p in pattern_list)
+             for pattern_list in patterns.values()
+         )
+         
+         return {
+             'patterns': patterns,
+             'total_severity': total_severity,
+             'character': character_name,
+             'text_length': len(text),
+             'normalized_info': normalized_info,
+             'timestamp': time.time()
+         }
+ 
+     def _detect_exact_repetitions_v3(self, text: str, normalized_info: Dict = None) -> List[RepetitionPatternV3]:
+         """v3ç‰ˆå®Œå…¨ä¸€è‡´æ¤œå‡ºï¼ˆMeCabçµ±åˆï¼‰"""
+         patterns = []
+         
+         # å¾“æ¥ã®æ–‡å­—ãƒ™ãƒ¼ã‚¹æ¤œå‡º
+         for length in range(2, min(20, len(text) // 2)):
+             for i in range(len(text) - length):
+                 phrase = text[i:i+length]
+                 
+                 if not re.search(r'[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]', phrase):
+                     continue
+                 
+                 positions = []
+                 for j in range(i + length, len(text) - length + 1):
+                     if text[j:j+length] == phrase:
+                         positions.append(j)
+                 
+                 if len(positions) >= self.min_repeat_threshold - 1:
+                     severity = min(1.0, (len(positions) * length) / 10.0)
+                     pattern = RepetitionPatternV3(
+                         pattern=phrase,
+                         count=len(positions) + 1,
+                         positions=[i] + positions,
+                         pattern_type='exact',
+                         severity=severity
+                     )
+                     
+                     # MeCabæ­£è¦åŒ–æƒ…å ±ã‚’è¿½åŠ 
+                     if normalized_info:
+                         pattern.normalized_form = self._get_normalized_form(phrase, normalized_info)
+                     
+                     patterns.append(pattern)
+         
+         # MeCabãƒ™ãƒ¼ã‚¹ã®èªåŸºåº•å½¢æ¤œå‡º
+         if normalized_info and self.enable_mecab_normalization:
+             patterns.extend(self._detect_lemma_repetitions(normalized_info))
+         
+         return self._filter_overlapping_patterns(patterns)
+ 
+     def _detect_lemma_repetitions(self, normalized_info: Dict) -> List[RepetitionPatternV3]:
+         """èªåŸºåº•å½¢ãƒ™ãƒ¼ã‚¹ã®åå¾©æ¤œå‡º"""
+         patterns = []
+         tokens = normalized_info.get('tokens', [])
+         
+         # èªåŸºåº•å½¢ã§ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
+         lemma_positions = defaultdict(list)
+         for i, token in enumerate(tokens):
+             if len(token['lemma']) >= 2 and token['pos'] in ['åè©', 'å‹•è©', 'å½¢å®¹è©']:
+                 lemma_positions[token['lemma']].append(i)
+         
+         for lemma, positions in lemma_positions.items():
+             if len(positions) >= self.min_repeat_threshold:
+                 severity = min(1.0, len(positions) / 5.0)
+                 pattern = RepetitionPatternV3(
+                     pattern=f"èªåŸºåº•å½¢: {lemma}",
+                     count=len(positions),
+                     positions=positions,
+                     pattern_type='lemma',
+                     severity=severity,
+                     normalized_form=lemma
+                 )
+                 patterns.append(pattern)
+         
+         return patterns
+ 
+     def _detect_latin_number_repetitions(self, text: str) -> List[RepetitionPatternV3]:
+         """ãƒ©ãƒ†ãƒ³æ–‡å­—ãƒ»æ•°å­—åå¾©ã®æ¤œå‡º"""
+         patterns = []
+         
+         for pattern_str in self.latin_number_patterns:
+             matches = list(re.finditer(pattern_str, text))
+             
+             if matches:
+                 for match in matches:
+                     severity = min(1.0, len(match.group()) / 5.0)
+                     pattern = RepetitionPatternV3(
+                         pattern=match.group(),
+                         count=1,
+                         positions=[match.start()],
+                         pattern_type='latin_number',
+                         severity=severity
+                     )
+                     patterns.append(pattern)
+         
+         return patterns
+ 
+     def _get_normalized_form(self, phrase: str, normalized_info: Dict) -> str:
+         """èªå¥ã®æ­£è¦åŒ–å½¢ã‚’å–å¾—"""
+         tokens = normalized_info.get('tokens', [])
+         
+         # phraseã«å«ã¾ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®èªåŸºåº•å½¢ã‚’çµåˆ
+         normalized_parts = []
+         for token in tokens:
+             if token['surface'] in phrase:
+                 normalized_parts.append(token['lemma'])
+         
+         return ''.join(normalized_parts) if normalized_parts else phrase
+ 
+     def _filter_overlapping_patterns(self, patterns: List[RepetitionPatternV3]) -> List[RepetitionPatternV3]:
+         """é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆv3å¼·åŒ–ç‰ˆï¼‰"""
+         patterns.sort(key=lambda p: (p.severity, len(p.pattern)), reverse=True)
+         filtered = []
+         used_positions = set()
+         
+         for pattern in patterns:
+             if not any(pos in used_positions for pos in pattern.positions):
+                 filtered.append(pattern)
+                 used_positions.update(pattern.positions)
+         
+         return filtered
+ 
+     def _apply_suppression_v3(self, text: str, pattern: RepetitionPatternV3, character_name: str = None) -> str:
+         """v3ç‰ˆæŠ‘åˆ¶å‡¦ç†é©ç”¨"""
+         if pattern.pattern_type == 'exact':
+             return self._suppress_exact_repetition_v3(text, pattern)
+         elif pattern.pattern_type == 'lemma':
+             return self._suppress_lemma_repetition(text, pattern)
+         elif pattern.pattern_type == 'character':
+             return self._suppress_character_repetition(text, pattern)
+         elif pattern.pattern_type == 'latin_number':
+             return self._suppress_latin_number_repetition(text, pattern)
+         elif pattern.pattern_type == 'interjection':
+             return self._suppress_interjection_overuse(text, pattern)
+         else:
+             return text
+ 
+     def _suppress_exact_repetition_v3(self, text: str, pattern: RepetitionPatternV3) -> str:
+         """v3ç‰ˆå®Œå…¨ä¸€è‡´æŠ‘åˆ¶ï¼ˆã‚¹ãƒãƒ¼ãƒˆç½®æ›ï¼‰"""
+         original_phrase = pattern.pattern
+         
+         # ä»£æ›¿è¡¨ç¾ã®å–å¾—ï¼ˆv3å¼·åŒ–ç‰ˆï¼‰
+         alternatives = self._get_smart_alternatives_v3(original_phrase, pattern)
+         
+         if not alternatives:
+             # ä»£æ›¿è¡¨ç¾ãŒãªã„å ´åˆã¯å‰Šé™¤
+             return self._smart_removal(text, pattern)
+         
+         # ã‚¹ãƒãƒ¼ãƒˆç½®æ›å®Ÿè¡Œ
+         result = text
+         positions = sorted(pattern.positions, reverse=True)
+         
+         for i, pos in enumerate(positions[1:], 1):
+             if i < len(alternatives):
+                 replacement = alternatives[i - 1]
+                 end_pos = pos + len(original_phrase)
+                 result = result[:pos] + replacement + result[end_pos:]
+         
+         return result
+ 
+     def _get_smart_alternatives_v3(self, phrase: str, pattern: RepetitionPatternV3) -> List[str]:
+         """v3ç‰ˆã‚¹ãƒãƒ¼ãƒˆä»£æ›¿è¡¨ç¾ç”Ÿæˆ"""
+         alternatives = []
+         
+         # åŸºæœ¬è¾æ›¸ã‹ã‚‰æ¤œç´¢
+         if phrase in self.replacement_dict:
+             alternatives.extend(self.replacement_dict[phrase])
+         
+         # MeCabæƒ…å ±ã‚’æ´»ç”¨ã—ãŸä»£æ›¿è¡¨ç¾ç”Ÿæˆ
+         if pattern.normalized_form and pattern.normalized_form != phrase:
+             if pattern.normalized_form in self.replacement_dict:
+                 alternatives.extend(self.replacement_dict[pattern.normalized_form])
+         
+         # å‹•çš„ç”Ÿæˆ
+         if not alternatives:
+             alternatives = self._generate_dynamic_alternatives(phrase)
+         
+         return alternatives[:3]  # æœ€å¤§3ã¤ã¾ã§
+ 
+     def _generate_dynamic_alternatives(self, phrase: str) -> List[str]:
+         """å‹•çš„ä»£æ›¿è¡¨ç¾ç”Ÿæˆ"""
+         alternatives = []
+         
+         # èªå°¾å¤‰åŒ–
+         if phrase.endswith('ã§ã™'):
+             alternatives.extend(['ã¾ã™', 'ã§ã‚ã‚‹'])
+         elif phrase.endswith('ã¾ã™'):
+             alternatives.extend(['ã§ã™', 'ã‚‹'])
+         elif phrase.endswith('ã '):
+             alternatives.extend(['ã§ã‚ã‚‹', 'ã§ã™'])
+         
+         # ç¨‹åº¦å‰¯è©ã®å¤‰æ›
+         if 'ã¨ã¦ã‚‚' in phrase:
+             alternatives.append(phrase.replace('ã¨ã¦ã‚‚', 'ã™ã”ã'))
+         if 'ã™ã”ã' in phrase:
+             alternatives.append(phrase.replace('ã™ã”ã', 'ã‹ãªã‚Š'))
+         
+         return alternatives
+ 
+     def _smart_removal(self, text: str, pattern: RepetitionPatternV3) -> str:
+         """ã‚¹ãƒãƒ¼ãƒˆå‰Šé™¤ï¼ˆæ–‡è„ˆä¿æŒï¼‰"""
+         result = text
+         positions = sorted(pattern.positions, reverse=True)
+         
+         # 2å›ç›®ä»¥é™ã®å‡ºç¾ã‚’å‰Šé™¤ï¼ˆæ–‡è„ˆã‚’è€ƒæ…®ï¼‰
+         for pos in positions[1:]:
+             # å‘¨è¾ºæ–‡è„ˆãƒã‚§ãƒƒã‚¯
+             before = text[max(0, pos-3):pos]
+             after = text[pos+len(pattern.pattern):pos+len(pattern.pattern)+3]
+             
+             # å¥èª­ç‚¹ãŒã‚ã‚‹å ´åˆã¯å‰Šé™¤ã—ã‚„ã™ã„
+             if any(char in before + after for char in "ã€‚ã€ï¼ï¼Ÿ"):
+                 end_pos = pos + len(pattern.pattern)
+                 result = result[:pos] + result[end_pos:]
+         
+         return result
+ 
+     def _is_over_compressed_v3(self, original: str, compressed: str) -> bool:
+         """v3ç‰ˆéå‰°åœ§ç¸®åˆ¤å®šï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
+         compression_rate = (len(original) - len(compressed)) / len(original) if len(original) > 0 else 0
+         
+         # 50%ä»¥ä¸Šã®åœ§ç¸®ã¯éå‰°
+         if compression_rate > 0.5:
+             return True
+         
+         # é‡è¦æƒ…å ±ã®æ¶ˆå¤±ãƒã‚§ãƒƒã‚¯ï¼ˆv3å¼·åŒ–ç‰ˆï¼‰
+         important_elements = [
+             r'[ã€‚ï¼ï¼Ÿ]',  # å¥èª­ç‚¹
+             r'[ã€Œã€]',    # æ‹¬å¼§
+             r'[ã‚¡-ãƒ³]{3,}',  # é•·ã„ã‚«ã‚¿ã‚«ãƒŠ
+             r'[æ¼¢å­—]{2,}',   # æ¼¢å­—èª
+         ]
+         
+         for pattern in important_elements:
+             orig_count = len(re.findall(pattern, original))
+             comp_count = len(re.findall(pattern, compressed))
+             
+             if orig_count > 0 and comp_count < orig_count * 0.6:  # 60%åŸºæº–
+                 return True
+         
+         return False
+ 
+     def _final_cleanup_v3(self, text: str) -> str:
+         """v3ç‰ˆæœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
+         result = text
+         
+         # å¥èª­ç‚¹ãƒ¬ãƒ™ãƒ«ã®é‡è¤‡é™¤å»
+         result = re.sub(r'ã€{2,}', 'ã€', result)  # ã€ã€ â†’ ã€
+         result = re.sub(r'â€¦{4,}', 'â€¦â€¦', result)  # â€¦â€¦â€¦ â†’ â€¦â€¦
+         result = re.sub(r'ï¼{3,}', 'ï¼ï¼', result)  # ï¼ï¼ï¼ â†’ ï¼ï¼
+         result = re.sub(r'ï¼Ÿ{3,}', 'ï¼Ÿï¼Ÿ', result)  # ï¼Ÿï¼Ÿï¼Ÿ â†’ ï¼Ÿï¼Ÿ
+         
+         # ç©ºç™½ã®æ­£è¦åŒ–
+         result = re.sub(r'\s{3,}', '  ', result)  # éåº¦ãªç©ºç™½
+         
+         return result
+ 
+     def get_v3_performance_report(self) -> Dict:
+         """v3ç‰ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ"""
+         if not self.session_metrics:
+             return {"message": "ã‚»ãƒƒã‚·ãƒ§ãƒ³ ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"}
+         
+         recent_metrics = self.session_metrics[-10:]  # æœ€æ–°10ä»¶
+         
+         avg_success_rate = sum(m.success_rate for m in recent_metrics) / len(recent_metrics)
+         avg_compression_rate = sum((m.input_length - m.output_length) / m.input_length for m in recent_metrics) / len(recent_metrics)
+         avg_processing_time = sum(m.processing_time_ms for m in recent_metrics) / len(recent_metrics)
+         
+         total_ngram_blocks = sum(m.ngram_blocks_applied for m in recent_metrics)
+         total_mecab_normalizations = sum(m.mecab_normalizations for m in recent_metrics)
+         total_rhetorical_exceptions = sum(m.rhetorical_exceptions for m in recent_metrics)
+         
+         return {
+             "v3_performance": {
+                 "average_success_rate": avg_success_rate,
+                 "target_achievement": avg_success_rate >= 0.8,
+                 "average_compression_rate": avg_compression_rate,
+                 "average_processing_time_ms": avg_processing_time
+             },
+             "v3_features": {
+                 "ngram_blocks_applied": total_ngram_blocks,
+                 "mecab_normalizations": total_mecab_normalizations,
+                 "rhetorical_exceptions": total_rhetorical_exceptions,
+                 "min_compress_rate": self.min_compress_rate
+             },
+             "improvement_from_v2": {
+                 "success_rate_target": "80%+",
+                 "compression_target": f"{self.min_compress_rate:.1%}+",
+                 "new_features": [
+                     "4-gramãƒ–ãƒ­ãƒƒã‚¯",
+                     "MeCabèªåŸºåº•å½¢æ­£è¦åŒ–",
+                     "ä¿®è¾çš„ä¾‹å¤–ä¿è­·",
+                     "é€£ç•ªæ¤œçŸ¥é™¤å»"
+                 ]
+             }
+         }
+ 
+     # ãã®ä»–ã®ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆv2ã‹ã‚‰ã®ç¶™æ‰¿ãƒ»æ”¹è‰¯ç‰ˆï¼‰
+     def _detect_character_repetitions(self, text: str) -> List[RepetitionPatternV3]:
+         """æ–‡å­—åå¾©æ¤œå‡ºï¼ˆv2ã‹ã‚‰ã®ç¶™æ‰¿ï¼‰"""
+         patterns = []
+         
+         for char in set(text):
+             if not re.match(r'[ã‚-ã‚“ã‚¢-ãƒ³ãƒ¼]', char):
+                 continue
+                 
+             consecutive_pattern = f'{char}{{3,}}'
+             matches = list(re.finditer(consecutive_pattern, text))
+             
+             if matches:
+                 for match in matches:
+                     length = match.end() - match.start()
+                     severity = min(1.0, length / 5.0)
+                     patterns.append(RepetitionPatternV3(
+                         pattern=match.group(),
+                         count=1,
+                         positions=[match.start()],
+                         pattern_type='character',
+                         severity=severity
+                     ))
+         
+         return patterns
+ 
+     def _detect_phonetic_repetitions(self, text: str) -> List[RepetitionPatternV3]:
+         """éŸ³éŸ»åå¾©æ¤œå‡ºï¼ˆv2ã‹ã‚‰ã®ç¶™æ‰¿ï¼‰"""
+         patterns = []
+         
+         # åŸºæœ¬çš„ãªéŸ³éŸ»å¤‰æ›ãƒãƒƒãƒ—
+         katakana_map = str.maketrans(
+             'ã‚¢ã‚¤ã‚¦ã‚¨ã‚ªã‚«ã‚­ã‚¯ã‚±ã‚³ã‚µã‚·ã‚¹ã‚»ã‚½ã‚¿ãƒãƒ„ãƒ†ãƒˆãƒŠãƒ‹ãƒŒãƒãƒãƒãƒ’ãƒ•ãƒ˜ãƒ›ãƒãƒŸãƒ ãƒ¡ãƒ¢ãƒ¤ãƒ¦ãƒ¨ãƒ©ãƒªãƒ«ãƒ¬ãƒ­ãƒ¯ãƒ²ãƒ³',
+             'ã‚ã„ã†ãˆãŠã‹ããã‘ã“ã•ã—ã™ã›ããŸã¡ã¤ã¦ã¨ã¯ã²ãµã¸ã»ã¾ã¿ã‚€ã‚ã‚‚ã‚„ã‚†ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚ã‚’ã‚“'
+         )
+         
+         hiragana_text = text.translate(katakana_map)
+         
+         # éŸ³éŸ»ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
+         phonetic_groups = defaultdict(list)
+         
+         for length in range(2, 8):
+             for i in range(len(hiragana_text) - length):
+                 phrase = hiragana_text[i:i+length]
+                 if re.match(r'^[ã‚-ã‚“]+$', phrase):
+                     normalized = self._normalize_phonetic(phrase)
+                     phonetic_groups[normalized].append((phrase, i))
+         
+         # éŸ³éŸ»é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
+         for normalized, occurrences in phonetic_groups.items():
+             if len(occurrences) >= self.min_repeat_threshold:
+                 unique_phrases = list(set(occ[0] for occ in occurrences))
+                 if len(unique_phrases) > 1:
+                     severity = min(1.0, len(occurrences) / 8.0)
+                     patterns.append(RepetitionPatternV3(
+                         pattern=f"éŸ³éŸ»é¡ä¼¼: {'/'.join(unique_phrases[:3])}",
+                         count=len(occurrences),
+                         positions=[occ[1] for occ in occurrences],
+                         pattern_type='phonetic',
+                         severity=severity
+                     ))
+         
+         return patterns
+ 
+     def _normalize_phonetic(self, text: str) -> str:
+         """éŸ³éŸ»æ­£è¦åŒ–"""
+         phonetic_map = str.maketrans(
+             'ãŒããã’ã”ã–ã˜ãšãœãã ã¢ã¥ã§ã©ã°ã³ã¶ã¹ã¼ã±ã´ã·ãºã½',
+             'ã‹ããã‘ã“ã•ã—ã™ã›ããŸã¡ã¤ã¦ã¨ã¯ã²ãµã¸ã»ã¯ã²ãµã¸ã»'
+         )
+         
+         normalized = text.translate(phonetic_map)
+         normalized = re.sub(r'[ã‚ƒã‚…ã‚‡ã£]', '', normalized)
+         
+         return normalized
+ 
+     def _detect_interjection_overuse(self, text: str) -> List[RepetitionPatternV3]:
+         """æ„Ÿå˜†è©éå¤šæ¤œå‡ºï¼ˆv2ã‹ã‚‰ã®ç¶™æ‰¿ï¼‰"""
+         patterns = []
+         
+         interjection_patterns = [
+             r'([ã‚ãŠã†ãˆã„])\1{2,}',
+             r'([ãã²ã‚“ã—ã¡])\1{1,}ã£*',
+             r'([ã£ãƒ¼ã€œ]){3,}',
+             r'([ï¼ï¼Ÿ]){2,}',
+             r'(ãã‚„|ã ã‚|ã„ã‚„){2,}',
+             r'([ã‚-ã‚“])\1{2,}[ã£ã€œâ€¦ï¼ï¼Ÿ]*'
+         ]
+         
+         for pattern_str in interjection_patterns:
+             matches = list(re.finditer(pattern_str, text))
+             
+             if len(matches) >= 3:
+                 severity = min(1.0, len(matches) / 5.0)
+                 patterns.append(RepetitionPatternV3(
+                     pattern=f"æ„Ÿå˜†è©éå¤š: {pattern_str}",
+                     count=len(matches),
+                     positions=[m.start() for m in matches],
+                     pattern_type='interjection',
+                     severity=severity
+                 ))
+         
+         return patterns
+ 
+     def suppress_repetitions(self, text: str, character_name: str = None) -> str:
+         """äº’æ›æ€§ç¶­æŒç”¨ã®ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆv3å‡¦ç†ã‚’å‘¼ã³å‡ºã—ï¼‰"""
+         result, _ = self.suppress_repetitions_with_debug_v3(text, character_name)
+         return result
+ 
+ # äº’æ›æ€§ã‚¨ã‚¤ãƒªã‚¢ã‚¹
+ AdvancedRepetitionSuppressor = AdvancedRepetitionSuppressorV3
+ SuppressionMetrics = SuppressionMetricsV3
+ RepetitionPattern = RepetitionPatternV3 
```

---

æ¬¡ã«ã€v3ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒ¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¾ã™ï¼š

---

```diff
- 
+ # -*- coding: utf-8 -*-
+ """
+ åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3 ã‚¯ã‚¤ãƒƒã‚¯ãƒ‡ãƒ¢
+ æˆåŠŸç‡80%é”æˆã®ãŸã‚ã®ã€Œãƒ©ã‚¹ãƒˆ21.7ptã€æ”¹å–„ç‰ˆ
+ 
+ ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œä¾‹:
+ python demo_repetition_v3_quick.py --sim 0.50 --max_dist 50 --ngram 4 --min_compress 0.05 --drp-base 1.05 --drp-alpha 0.4
+ """
+ 
+ import argparse
+ import json
+ import os
+ import sys
+ import time
+ from typing import Dict, List
+ 
+ # ãƒ‘ã‚¹è¨­å®š
+ current_dir = os.path.dirname(os.path.abspath(__file__))
+ src_dir = os.path.join(current_dir, "src")
+ sys.path.insert(0, src_dir)
+ 
+ try:
+     from src.utils.repetition_suppressor_v3 import AdvancedRepetitionSuppressorV3, SuppressionMetricsV3
+     print("âœ… v3ã‚·ã‚¹ãƒ†ãƒ èª­ã¿è¾¼ã¿æˆåŠŸ")
+ except ImportError as e:
+     print(f"âŒ v3ã‚·ã‚¹ãƒ†ãƒ èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
+     print("   v2ã‚·ã‚¹ãƒ†ãƒ ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
+     from src.utils.repetition_suppressor import AdvancedRepetitionSuppressor, SuppressionMetrics
+     
+     # v3äº’æ›ãƒ¬ã‚¤ãƒ¤ãƒ¼
+     class AdvancedRepetitionSuppressorV3(AdvancedRepetitionSuppressor):
+         def suppress_repetitions_with_debug_v3(self, text, character_name=None):
+             result, metrics = self.suppress_repetitions_with_debug(text, character_name)
+             # v3ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¸ã®å¤‰æ›
+             v3_metrics = type('SuppressionMetricsV3', (), {
+                 'input_length': metrics.input_length,
+                 'output_length': metrics.output_length,
+                 'patterns_detected': metrics.patterns_detected,
+                 'patterns_suppressed': metrics.patterns_suppressed,
+                 'detection_misses': metrics.detection_misses,
+                 'over_compressions': metrics.over_compressions,
+                 'processing_time_ms': metrics.processing_time_ms,
+                 'success_rate': metrics.success_rate,
+                 'ngram_blocks_applied': 0,
+                 'mecab_normalizations': 0,
+                 'rhetorical_exceptions': 0,
+                 'latin_number_blocks': 0,
+                 'min_compress_rate': 0.05
+             })()
+             return result, v3_metrics
+     
+     SuppressionMetricsV3 = SuppressionMetrics
+ 
+ 
+ def create_v3_test_cases() -> List[Dict]:
+     """v3å¼·åŒ–ç‰ˆãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ï¼ˆ80%é”æˆç”¨ï¼‰"""
+     return [
+         # === åŸºæœ¬åŒèªåå¾©ï¼ˆv3é‡ç‚¹å¯¾è±¡ï¼‰ ===
+         {
+             "name": "åŸºæœ¬åŒèªåå¾©",
+             "input": "ãŠå…„ã¡ã‚ƒã‚“ãŠå…„ã¡ã‚ƒã‚“ã€ã©ã“ã«è¡Œãã®ã§ã™ã‹ãŠå…„ã¡ã‚ƒã‚“ï¼Ÿ",
+             "character": "å¦¹",
+             "expected_patterns": ["ãŠå…„ã¡ã‚ƒã‚“"],
+             "category": "same_word",
+             "target_compression": 0.15
+         },
+         {
+             "name": "é–¢è¥¿å¼åŒèªåå¾©",
+             "input": "ãã‚„ãã‚„ãã‚„ã€ã‚ã‹ã‚“ã‚ã‹ã‚“ã‚ã‹ã‚“ã€ã‚„ãªã‚„ãªãã‚Œã¯ã€‚",
+             "character": "é–¢è¥¿å¼ã‚­ãƒ£ãƒ©",
+             "expected_patterns": ["ãã‚„", "ã‚ã‹ã‚“", "ã‚„ãª"],
+             "category": "dialect",
+             "target_compression": 0.20
+         },
+         {
+             "name": "èªå°¾åå¾©è¤‡åˆ",
+             "input": "ã§ã™ã§ã™ã­ã€ã¾ã™ã¾ã™ã‚ˆã€ã§ã—ã‚‡ã§ã—ã‚‡ã†ã€‚",
+             "character": "ä¸å¯§èªã‚­ãƒ£ãƒ©",
+             "expected_patterns": ["ã§ã™", "ã¾ã™", "ã§ã—ã‚‡"],
+             "category": "ending",
+             "target_compression": 0.10
+         },
+         
+         # === 4-gramãƒ–ãƒ­ãƒƒã‚¯å¯¾è±¡ ===
+         {
+             "name": "4-gramåå¾©",
+             "input": "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã ã‹ã‚‰æ•£æ­©ã—ã¾ã—ã‚‡ã†ã€‚",
+             "character": "æ™®é€šã®äºº",
+             "expected_patterns": ["ä»Šæ—¥ã¯è‰¯ã„"],
+             "category": "ngram",
+             "target_compression": 0.08
+         },
+         
+         # === ãƒ©ãƒ†ãƒ³æ–‡å­—ãƒ»æ•°å­—é€£ç•ª ===
+         {
+             "name": "é€£ç•ªåå¾©",
+             "input": "wwwwwwã€ãã†ã§ã™ã­ã€‚777777ã£ã¦æ•°å­—ã§ã™ã‹ï¼Ÿ",
+             "character": "ãƒãƒƒãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼",
+             "expected_patterns": ["wwwwww", "777777"],
+             "category": "latin_number",
+             "target_compression": 0.12
+         },
+         
+         # === ä¿®è¾çš„ä¾‹å¤–ä¿è­·ãƒ†ã‚¹ãƒˆ ===
+         {
+             "name": "ä¿®è¾çš„åå¾©ä¿è­·",
+             "input": "ã­ãˆã€ã­ãˆã€ã­ãˆï¼èã„ã¦ã‚ˆã€‚",
+             "character": "æ„Ÿæƒ…çš„ã‚­ãƒ£ãƒ©",
+             "expected_patterns": [],  # ä¿è­·ã•ã‚Œã‚‹ã¹ã
+             "category": "rhetorical",
+             "target_compression": 0.0  # åœ§ç¸®ã•ã‚Œãªã„
+         },
+         {
+             "name": "éŸ³è±¡å¾´èªä¿è­·",
+             "input": "ãƒ‰ã‚­ãƒ‰ã‚­ã—ã¡ã‚ƒã†ã€‚ãƒ¯ã‚¯ãƒ¯ã‚¯ã™ã‚‹ã­ï¼",
+             "character": "å…ƒæ°—ã‚­ãƒ£ãƒ©",
+             "expected_patterns": [],  # ä¿è­·ã•ã‚Œã‚‹ã¹ã
+             "category": "onomatopoeia",
+             "target_compression": 0.0
+         },
+         
+         # === è¤‡é›‘ã‚±ãƒ¼ã‚¹ ===
+         {
+             "name": "è¤‡åˆåå¾©",
+             "input": "å¬‰ã—ã„å¬‰ã—ã„ã€æ¥½ã—ã„æ¥½ã—ã„ã€å¹¸ã›å¹¸ã›ã¨ã„ã†æ„Ÿã˜ã§ã™ã§ã™ã€‚",
+             "character": "ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚­ãƒ£ãƒ©",
+             "expected_patterns": ["å¬‰ã—ã„", "æ¥½ã—ã„", "å¹¸ã›", "ã§ã™"],
+             "category": "complex",
+             "target_compression": 0.15
+         },
+         {
+             "name": "æ„Ÿå˜†è©éå¤š",
+             "input": "ã‚ã‚ã‚ã‚ã‚ã‚ã‚ï¼ã†ã‚ã‚ã‚ã‚ã‚ã‚ã‚ï¼ãã‚ƒã‚ã‚ã‚ã‚ã‚ã‚ï¼",
+             "character": "æ„Ÿæƒ…è±Šã‹ã‚­ãƒ£ãƒ©",
+             "expected_patterns": ["ã‚ã‚ã‚ã‚ã‚ã‚ã‚", "ã†ã‚ã‚ã‚ã‚ã‚ã‚ã‚", "ãã‚ƒã‚ã‚ã‚ã‚ã‚ã‚"],
+             "category": "interjection",
+             "target_compression": 0.25
+         },
+         
+         # === å¢ƒç•Œã‚±ãƒ¼ã‚¹ ===
+         {
+             "name": "çŸ­æ–‡åå¾©",
+             "input": "ã¯ã„ã€ã¯ã„ã€ã¯ã„ã€‚",
+             "character": "ç›¸æ§Œã‚­ãƒ£ãƒ©",
+             "expected_patterns": ["ã¯ã„"],
+             "category": "short",
+             "target_compression": 0.20
+         }
+     ]
+ 
+ 
+ def parse_args():
+     """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ"""
+     parser = argparse.ArgumentParser(description='åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3 ã‚¯ã‚¤ãƒƒã‚¯ãƒ‡ãƒ¢')
+     
+     # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
+     parser.add_argument('--sim', type=float, default=0.50, help='é¡ä¼¼åº¦é–¾å€¤ (default: 0.50)')
+     parser.add_argument('--max_dist', type=int, default=50, help='æœ€å¤§æ¤œå‡ºè·é›¢ (default: 50)')
+     parser.add_argument('--min_compress', type=float, default=0.05, help='æœ€å°åœ§ç¸®ç‡ (default: 0.05)')
+     
+     # v3æ–°æ©Ÿèƒ½
+     parser.add_argument('--ngram', type=int, default=4, help='n-gramãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º (default: 4)')
+     parser.add_argument('--drp-base', type=float, default=1.05, help='DRPåŸºæº–å€¤ (default: 1.05)')
+     parser.add_argument('--drp-alpha', type=float, default=0.4, help='DRPã‚¢ãƒ«ãƒ•ã‚¡å€¤ (default: 0.4)')
+     parser.add_argument('--enable-mecab', action='store_true', help='MeCabæ­£è¦åŒ–ã‚’æœ‰åŠ¹åŒ–')
+     parser.add_argument('--disable-rhetorical', action='store_true', help='ä¿®è¾çš„ä¿è­·ã‚’ç„¡åŠ¹åŒ–')
+     
+     # å®Ÿè¡Œã‚ªãƒ—ã‚·ãƒ§ãƒ³
+     parser.add_argument('--target-rate', type=float, default=0.80, help='ç›®æ¨™æˆåŠŸç‡ (default: 0.80)')
+     parser.add_argument('--verbose', action='store_true', help='è©³ç´°å‡ºåŠ›')
+     parser.add_argument('--save-report', type=str, help='ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«å')
+     
+     return parser.parse_args()
+ 
+ 
+ def create_v3_config(args) -> Dict:
+     """å¼•æ•°ã‹ã‚‰v3è¨­å®šã‚’æ§‹ç¯‰"""
+     return {
+         'similarity_threshold': args.sim,
+         'max_distance': args.max_dist,
+         'min_compress_rate': args.min_compress,
+         'enable_4gram_blocking': args.ngram > 0,
+         'ngram_block_size': args.ngram,
+         'enable_drp': True,
+         'drp_base': args.drp_base,
+         'drp_alpha': args.drp_alpha,
+         'enable_mecab_normalization': args.enable_mecab,
+         'enable_rhetorical_protection': not args.disable_rhetorical,
+         'enable_aggressive_mode': True,
+         'debug_mode': args.verbose,
+         'min_repeat_threshold': 1,
+         'enable_latin_number_detection': True
+     }
+ 
+ 
+ def run_v3_quick_test(config: Dict, test_cases: List[Dict], target_success_rate: float = 0.8) -> Dict:
+     """v3ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
+     print(f"ğŸš€ åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3 ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹")
+     print(f"   ç›®æ¨™æˆåŠŸç‡: {target_success_rate:.1%}")
+     print(f"   ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹: {len(test_cases)}ä»¶")
+     print("=" * 60)
+     
+     # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
+     suppressor = AdvancedRepetitionSuppressorV3(config)
+     
+     results = []
+     category_stats = {}
+     
+     start_time = time.time()
+     
+     for i, test_case in enumerate(test_cases, 1):
+         print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆ {i}/{len(test_cases)}: {test_case['name']}")
+         
+         # å‡¦ç†å®Ÿè¡Œ
+         result_text, metrics = suppressor.suppress_repetitions_with_debug_v3(
+             test_case['input'], 
+             test_case['character']
+         )
+         
+         # æˆåŠŸåˆ¤å®šï¼ˆv3åŸºæº–ï¼‰
+         compression_rate = (metrics.input_length - metrics.output_length) / metrics.input_length
+         meets_compression_target = compression_rate >= test_case.get('target_compression', config['min_compress_rate'])
+         success = metrics.success_rate >= 0.7 and meets_compression_target  # v3ã§ã¯70%+åœ§ç¸®é”æˆã‚’æˆåŠŸã¨ã™ã‚‹
+         
+         # çµæœè¨˜éŒ²
+         test_result = {
+             'test_case': test_case['name'],
+             'category': test_case['category'],
+             'success': success,
+             'success_rate': metrics.success_rate,
+             'compression_rate': compression_rate,
+             'target_compression': test_case.get('target_compression', config['min_compress_rate']),
+             'input_length': metrics.input_length,
+             'output_length': metrics.output_length,
+             'patterns_detected': metrics.patterns_detected,
+             'patterns_suppressed': metrics.patterns_suppressed,
+             'processing_time_ms': metrics.processing_time_ms,
+             'input_text': test_case['input'][:50] + "..." if len(test_case['input']) > 50 else test_case['input'],
+             'output_text': result_text[:50] + "..." if len(result_text) > 50 else result_text,
+             'v3_features': {
+                 'ngram_blocks': getattr(metrics, 'ngram_blocks_applied', 0),
+                 'mecab_normalizations': getattr(metrics, 'mecab_normalizations', 0),
+                 'rhetorical_exceptions': getattr(metrics, 'rhetorical_exceptions', 0),
+                 'latin_blocks': getattr(metrics, 'latin_number_blocks', 0)
+             }
+         }
+         
+         results.append(test_result)
+         
+         # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
+         category = test_case['category']
+         if category not in category_stats:
+             category_stats[category] = {'total': 0, 'success': 0}
+         category_stats[category]['total'] += 1
+         if success:
+             category_stats[category]['success'] += 1
+         
+         # çµæœè¡¨ç¤º
+         status_icon = "âœ…" if success else "âŒ"
+         print(f"   {status_icon} æˆåŠŸç‡: {metrics.success_rate:.1%}, åœ§ç¸®ç‡: {compression_rate:.1%}")
+         
+         if config.get('debug_mode', False):
+             print(f"      å…¥åŠ›: {test_case['input'][:40]}...")
+             print(f"      å‡ºåŠ›: {result_text[:40]}...")
+             print(f"      å‡¦ç†æ™‚é–“: {metrics.processing_time_ms:.1f}ms")
+             
+             # v3æ©Ÿèƒ½ã®ä½¿ç”¨çŠ¶æ³
+             v3_features = test_result['v3_features']
+             if any(v3_features.values()):
+                 print(f"      v3æ©Ÿèƒ½: 4-gram={v3_features['ngram_blocks']}, MeCab={v3_features['mecab_normalizations']}, ä¿®è¾={v3_features['rhetorical_exceptions']}, é€£ç•ª={v3_features['latin_blocks']}")
+     
+     total_time = time.time() - start_time
+     
+     # ç·åˆçµ±è¨ˆ
+     total_tests = len(results)
+     total_success = sum(1 for r in results if r['success'])
+     overall_success_rate = total_success / total_tests if total_tests > 0 else 0
+     
+     avg_compression = sum(r['compression_rate'] for r in results) / total_tests if total_tests > 0 else 0
+     avg_processing_time = sum(r['processing_time_ms'] for r in results) / total_tests if total_tests > 0 else 0
+     
+     # v3æ©Ÿèƒ½çµ±è¨ˆ
+     total_ngram_blocks = sum(r['v3_features']['ngram_blocks'] for r in results)
+     total_mecab_normalizations = sum(r['v3_features']['mecab_normalizations'] for r in results)
+     total_rhetorical_exceptions = sum(r['v3_features']['rhetorical_exceptions'] for r in results)
+     total_latin_blocks = sum(r['v3_features']['latin_blocks'] for r in results)
+     
+     print("\n" + "=" * 60)
+     print(f"ğŸ“Š v3ã‚·ã‚¹ãƒ†ãƒ ç·åˆçµæœ")
+     print(f"   æˆåŠŸç‡: {overall_success_rate:.1%} ({total_success}/{total_tests})")
+     print(f"   ç›®æ¨™é”æˆ: {'ğŸ‰ é”æˆï¼' if overall_success_rate >= target_success_rate else 'âŒ æœªé”'}")
+     print(f"   å¹³å‡åœ§ç¸®ç‡: {avg_compression:.1%}")
+     print(f"   å¹³å‡å‡¦ç†æ™‚é–“: {avg_processing_time:.1f}ms")
+     print(f"   ç·å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
+     
+     print(f"\nğŸ”§ v3æ©Ÿèƒ½ä½¿ç”¨çŠ¶æ³:")
+     print(f"   4-gramãƒ–ãƒ­ãƒƒã‚¯: {total_ngram_blocks}å›")
+     print(f"   MeCabæ­£è¦åŒ–: {total_mecab_normalizations}å›")
+     print(f"   ä¿®è¾çš„ä¾‹å¤–: {total_rhetorical_exceptions}å›")
+     print(f"   é€£ç•ªé™¤å»: {total_latin_blocks}å›")
+     
+     print(f"\nğŸ“ˆ ã‚«ãƒ†ã‚´ãƒªåˆ¥æˆåŠŸç‡:")
+     for category, stats in category_stats.items():
+         success_rate = stats['success'] / stats['total']
+         print(f"   {category}: {success_rate:.1%} ({stats['success']}/{stats['total']})")
+     
+     # å¤±æ•—ã‚±ãƒ¼ã‚¹ã®åˆ†æ
+     failed_cases = [r for r in results if not r['success']]
+     if failed_cases:
+         print(f"\nâŒ å¤±æ•—ã‚±ãƒ¼ã‚¹åˆ†æ ({len(failed_cases)}ä»¶):")
+         for case in failed_cases:
+             print(f"   - {case['test_case']}: æˆåŠŸç‡{case['success_rate']:.1%}, åœ§ç¸®ç‡{case['compression_rate']:.1%}")
+     
+     # æ”¹å–„ææ¡ˆ
+     if overall_success_rate < target_success_rate:
+         gap = target_success_rate - overall_success_rate
+         print(f"\nğŸ” æ”¹å–„ææ¡ˆï¼ˆæ®‹ã‚Š{gap:.1%}ptï¼‰:")
+         
+         if avg_compression < config['min_compress_rate']:
+             print(f"   - åœ§ç¸®ç‡å‘ä¸Š: ç¾åœ¨{avg_compression:.1%} â†’ ç›®æ¨™{config['min_compress_rate']:.1%}")
+             print(f"     â†’ similarity_threshold ã‚’ {config['similarity_threshold']:.2f} â†’ {max(0.3, config['similarity_threshold'] - 0.1):.2f} ã«ä¸‹ã’ã‚‹")
+         
+         worst_category = min(category_stats.items(), key=lambda x: x[1]['success'] / x[1]['total'])
+         print(f"   - æœ€å¼±ã‚«ãƒ†ã‚´ãƒª: {worst_category[0]} ({worst_category[1]['success']}/{worst_category[1]['total']})")
+         
+         if total_ngram_blocks == 0:
+             print(f"   - 4-gramãƒ–ãƒ­ãƒƒã‚¯æœªä½¿ç”¨ â†’ ngram_block_size ã‚’3ã«ä¸‹ã’ã‚‹")
+         
+         if total_rhetorical_exceptions > total_success // 2:
+             print(f"   - ä¿®è¾çš„ä¾‹å¤–ãŒå¤šã™ã â†’ enable_rhetorical_protection=False ã‚’æ¤œè¨")
+     
+     # è¨­å®šæœ€é©åŒ–ææ¡ˆ
+     print(f"\nâš™ï¸ æ¬¡å›æ¨å¥¨è¨­å®š:")
+     optimized_config = dict(config)
+     if overall_success_rate < target_success_rate:
+         optimized_config['similarity_threshold'] = max(0.3, config['similarity_threshold'] - 0.05)
+         if avg_compression < config['min_compress_rate']:
+             optimized_config['min_compress_rate'] = max(0.03, config['min_compress_rate'] - 0.01)
+     
+     print(f"   --sim {optimized_config['similarity_threshold']:.2f} --max_dist {optimized_config['max_distance']} --ngram {optimized_config.get('ngram_block_size', 4)} --min_compress {optimized_config['min_compress_rate']:.2f} --drp-base {optimized_config['drp_base']:.2f} --drp-alpha {optimized_config['drp_alpha']:.1f}")
+     
+     return {
+         'overall_success_rate': overall_success_rate,
+         'target_achievement': overall_success_rate >= target_success_rate,
+         'total_tests': total_tests,
+         'total_success': total_success,
+         'average_compression_rate': avg_compression,
+         'average_processing_time_ms': avg_processing_time,
+         'total_processing_time_s': total_time,
+         'v3_feature_usage': {
+             'ngram_blocks': total_ngram_blocks,
+             'mecab_normalizations': total_mecab_normalizations,
+             'rhetorical_exceptions': total_rhetorical_exceptions,
+             'latin_blocks': total_latin_blocks
+         },
+         'category_stats': category_stats,
+         'failed_cases': failed_cases,
+         'optimized_config': optimized_config,
+         'config_used': config,
+         'detailed_results': results
+     }
+ 
+ 
+ def save_report(report: Dict, filename: str):
+     """ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
+     try:
+         os.makedirs('logs/v3_tests', exist_ok=True)
+         filepath = f'logs/v3_tests/{filename}'
+         
+         with open(filepath, 'w', encoding='utf-8') as f:
+             json.dump(report, f, indent=2, ensure_ascii=False, default=str)
+         
+         print(f"\nğŸ’¾ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {filepath}")
+     except Exception as e:
+         print(f"\nâŒ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å¤±æ•—: {e}")
+ 
+ 
+ def main():
+     """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
+     args = parse_args()
+     
+     print("ğŸ”¥ åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3 - ã€Œãƒ©ã‚¹ãƒˆ21.7ptã€å¼·åŒ–ç‰ˆ")
+     print("   58.3% â†’ 80% é”æˆã‚’ç›®æŒ‡ã—ã¾ã™ï¼")
+     print("=" * 60)
+     
+     # è¨­å®šæ§‹ç¯‰
+     config = create_v3_config(args)
+     
+     print("âš™ï¸ v3è¨­å®š:")
+     print(f"   é¡ä¼¼åº¦é–¾å€¤: {config['similarity_threshold']}")
+     print(f"   æœ€å¤§è·é›¢: {config['max_distance']}")
+     print(f"   æœ€å°åœ§ç¸®ç‡: {config['min_compress_rate']:.1%}")
+     print(f"   4-gramãƒ–ãƒ­ãƒƒã‚¯: {config['enable_4gram_blocking']} (ã‚µã‚¤ã‚º: {config.get('ngram_block_size', 0)})")
+     print(f"   DRP: base={config['drp_base']}, alpha={config['drp_alpha']}")
+     print(f"   MeCab: {config['enable_mecab_normalization']}")
+     print(f"   ä¿®è¾çš„ä¿è­·: {config['enable_rhetorical_protection']}")
+     
+     # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æº–å‚™
+     test_cases = create_v3_test_cases()
+     
+     # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
+     report = run_v3_quick_test(config, test_cases, args.target_rate)
+     
+     # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
+     if args.save_report:
+         save_report(report, args.save_report)
+     
+     # æœ€çµ‚åˆ¤å®š
+     if report['target_achievement']:
+         print(f"\nğŸ‰ ç›®æ¨™é”æˆï¼ æˆåŠŸç‡ {report['overall_success_rate']:.1%} â‰¥ {args.target_rate:.1%}")
+         print("   80%ãƒ©ã‚¤ãƒ³ã«åˆ°é”ã—ã¾ã—ãŸï¼ğŸ”¥")
+     else:
+         gap = args.target_rate - report['overall_success_rate']
+         print(f"\nğŸ“ˆ ã‚ã¨{gap:.1%}pt ã§ç›®æ¨™é”æˆã§ã™")
+         print("   æ¬¡å›æ¨å¥¨è¨­å®šã§å†ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã—ã¦ãã ã•ã„")
+     
+     return 0 if report['target_achievement'] else 1
+ 
+ 
+ if __name__ == "__main__":
+     exit_code = main()
+     sys.exit(exit_code) 
```

---

æœ€å¾Œã«ã€çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¦ã€ææ¡ˆã•ã‚ŒãŸæ”¹å–„æ¡ˆã®åŠ¹æœã‚’æ¤œè¨¼ã—ã¾ã—ã‚‡ã†ï¼š

---

Cancelled

---



---

_**User**_

ç¶šã‘ã¦

---

_**Assistant**_



---

äº†è§£ã§ã™ï¼v3ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ã‚’å®Œäº†ã•ã›ã¦ã€å®Ÿéš›ã«ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

ã¾ãšã€v3ã‚·ã‚¹ãƒ†ãƒ ã§ä¸è¶³ã—ã¦ã„ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ ã—ã¾ã™ï¼š

---

Error calling tool 'edit_file'.

---



---

ç¶šã„ã¦ã€easy_novel_assistant.py ã«v3ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ±åˆã—ã¾ã—ã‚‡ã†ï¼š

---



---

ã§ã¯ã€v3ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

---

```bash
py -3 demo_repetition_v3_quick.py --sim 0.50 --max_dist 50 --ngram 4 --min_compress 0.05 --drp-base 1.05 --drp-alpha 0.4 --verbose
```

```
PS C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main> py
-3 demo_repetition_v3_quick.
py --sim 0.50 --max_dist 50 --ngram 4 --min_compress 0.05 --drp-base 1.05 --drp-
alpha 0.4 --verbose         PS C:\Users\downl\Downloads\EasyNovelAssistant-main\
EasyNovelAssistant-main> py -3 demo_repetition_v3_quick.
py --sim 0.50 --max_dist 50 --ngram 4 --min_compress 0.05 --drp-base 1.05 --drp-
alpha 0.4 --verbose         























âœ… v3ã‚·ã‚¹ãƒ†ãƒ èª­ã¿è¾¼ã¿æˆåŠŸ
ğŸ”¥ åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3 - ã€Œãƒ©ã‚¹ãƒˆ21.7ptã€å¼·åŒ–ç‰ˆ
   58.3% â†’ 80% é”æˆã‚’ç›®æŒ‡ã—ã¾ã™ï¼
============================================================
âš™ï¸ v3è¨­å®š:
   é¡ä¼¼åº¦é–¾å€¤: 0.5
   æœ€å¤§è·é›¢: 50
   æœ€å°åœ§ç¸®ç‡: 5.0%
   4-gramãƒ–ãƒ­ãƒƒã‚¯: True (ã‚µã‚¤ã‚º: 4)
   DRP: base=1.05, alpha=0.4
   MeCab: False
   ä¿®è¾çš„ä¿è­·: True
ğŸš€ åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3 ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹
   ç›®æ¨™æˆåŠŸç‡: 80.0%
   ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹: 10ä»¶
============================================================
ğŸš€ åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3 åˆæœŸåŒ–å®Œäº†
   â”œâ”€ 4-gramãƒ–ãƒ­ãƒƒã‚¯: æœ‰åŠ¹
   â”œâ”€ MeCabæ­£è¦åŒ–: ç„¡åŠ¹
   â”œâ”€ ä¿®è¾çš„ä¿è­·: æœ‰åŠ¹
   â”œâ”€ æˆåŠŸåˆ¤å®šåŸºæº–: 5.0%
   â””â”€ é€£ç•ªæ¤œçŸ¥: æœ‰åŠ¹

ğŸ“ ãƒ†ã‚¹ãƒˆ 1/10: åŸºæœ¬åŒèªåå¾©
Traceback (most recent call last):
  File "C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main
\demo_repetition_v3_quick.py
", line 421, in <module>
                                exit_code = main()
                ^^^^^^
  File "C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main
\demo_repetition_v3_quick.py
", line 402, in main
                                report = run_v3_quick_test(config, test_cases, a
rgs.target_rate)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main
\demo_repetition_v3_quick.py
", line 215, in run_v3_quick_test
                                result_text, metrics = suppressor.suppress_repet
itions_with_debug_v3(
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main
\src\utils\repetition_suppre
ssor_v3.py", line 194, in suppress_repetitions_with_debug_v3
                                analysis = self.analyze_text_v3(text, character_
name, normalized_info)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main
\src\utils\repetition_suppre
ssor_v3.py", line 459, in analyze_text_v3
                                'phonetic_repetitions': self._detect_phonetic_re
petitions(text),
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main
\src\utils\repetition_suppre
ssor_v3.py", line 800, in _detect_phonetic_repetitions
                                katakana_map = str.maketrans(
                   ^^^^^^^^^^^^^^
ValueError: the first two maketrans arguments must have equal length

```

---

maketransã®ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã—ã¾ã—ã‚‡ã†ï¼š

---



---

ã‚‚ã†ä¸€åº¦ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

---

```bash
py -3 demo_repetition_v3_quick.py --sim 0.50 --max_dist 50 --ngram 4 --min_compress 0.05 --drp-base 1.05 --drp-alpha 0.4 --verbose
```

```
py --sim 0.50 --max_dist 50 --ngram 4 --min_compress 0.05 --drp-base 1.05 --drp-alpha 0.4 --verbose
âœ… v3ã‚·ã‚¹ãƒ†ãƒ èª­ã¿è¾¼ã¿æˆåŠŸ
ğŸ”¥ åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3 - ã€Œãƒ©ã‚¹ãƒˆ21.7ptã€å¼·åŒ–ç‰ˆ
   58.3% â†’ 80% é”æˆã‚’ç›®æŒ‡ã—ã¾ã™ï¼
============================================================
âš™ï¸ v3è¨­å®š:
   é¡ä¼¼åº¦é–¾å€¤: 0.5
   æœ€å¤§è·é›¢: 50
   æœ€å°åœ§ç¸®ç‡: 5.0%
   4-gramãƒ–ãƒ­ãƒƒã‚¯: True (ã‚µã‚¤ã‚º: 4)
   DRP: base=1.05, alpha=0.4
   MeCab: False
   ä¿®è¾çš„ä¿è­·: True
ğŸš€ åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3 ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹
   ç›®æ¨™æˆåŠŸç‡: 80.0%
   ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹: 10ä»¶
============================================================
ğŸš€ åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3 åˆæœŸåŒ–å®Œäº†
  File "C:\Users\downl\AppData\Local\Programs\Python\Python312\Lib\re\__init__.py", line 307, in _compile

    p = _compiler.compile(pattern, flags)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\downl\AppData\Local\Programs\Python\Python312\Lib\re\_parser.py", line 544, in _parse

    code = _escape(source, this, state)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\downl\AppData\Local\Programs\Python\Python312\Lib\re\_parser.py", line 440, in _escape

    raise source.error("invalid group reference %d" % group, len(escape) - 1)
re.error: invalid group reference 1 at position 9

```

---

æ­£è¦è¡¨ç¾ã®ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Šã¾ã™ã­ã€‚ä¿®æ­£ã—ã¾ã—ã‚‡ã†ï¼š

---



---

ä¿®è¾çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å•é¡ŒãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ä¿®æ­£ã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

---



---

ã‚¨ãƒ©ãƒ¼ã®åŸå› ã‚’ç‰¹å®šã™ã‚‹ãŸã‚ã«ã€å•é¡Œã®ã‚ã‚‹éƒ¨åˆ†ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ã€åŸºæœ¬çš„ãªãƒ†ã‚¹ãƒˆã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ï¼š

---



---

ã¾ãŸã€æ„Ÿå˜†è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ä¿®æ­£ã—ã¾ã—ã‚‡ã†ï¼š

---



---

ã‚‚ã†ä¸€åº¦ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

---



---



---

_**User**_



---

_**Assistant**_



---



---



---



---



---



---

