<!-- Generated by SpecStory -->

# 検出精度向上のための改善策 (2025-06-03 07:31:07)

_**User**_

ボブにゃん、おつかれ！
**58.3 % → 80 % へのラスト 21.7pt** を詰め切るための “あとひと押し” 作戦をまとめたよ。いまの改善は「**検出感度アップ**」でかなり効いたけど、残りは **誤検知を避けつつ検出漏れを刈り取る** 層を追加する感じだね。

---

## 1️⃣ サンプリング側でブロックして “未然防止”

| 手筋                         | 実装                                | 効果                   | 想定コスト                |
| -------------------------- | --------------------------------- | -------------------- | -------------------- |
| **4-gram block**           | llama.cpp `--ngram 4`             | そのままの語順コピーをほぼ消す      | VRAM +0、遅延 +1 %      |
| **Dynamic repeat-penalty** | `--drp-base 1.05 --drp-alpha 0.4` | 長文後半での「うわぁぁ…」連打を早期抑制 | logits 演算 +5 µs/step |

> **ポイント**：抑制後処理より“出さない”ほうが速くて崩れない。

---

## 2️⃣ 検出ロジックの「２段目フィルタ」で漏れを拾う

### a. MeCab + UniDic で語基底形に正規化

```python
import fugashi, itertools
tagger = fugashi.Tagger()
words = [tok.feature.lemma or tok.surface for tok in tagger(text)]
```

* **語形変化**（食べた→食べる）を同語とみなせる
* 失敗 32 % のうち ≈10 % 減らせる見込み。

### b. ラテン文字 or 数字の連番検知

```python
re.findall(r'([A-Za-z0-9])\1{3,}', text)
```

* ログで見えた “777777” “wwww” 系を一括カット。

### c. 句読点レベルの重複を抑える

* `、、` → `、` / `……` を 3 個制限など。

---

## 3️⃣ 誤抑制（過剰削り）の最小化

| 症状       | 判定ルール          | 例           | 対処           |
| -------- | -------------- | ----------- | ------------ |
| 修辞的反復    | 句点を挟む + 文字数 ≤5 | 「ねえ、ねえ、ねえ！」 | スルー          |
| 音象徴語     | カタカナ2文字×n      | 「ドキドキ」      | 1回残す         |
| 歌詞風リフレイン | 行末重複 (改行毎同語)   | ♪ラララ        | 行単位で 1 度だけ許可 |

→ `_suppress_word_repetition()` に正規表現例外リストを追加。

---

## 4️⃣ 「検出=圧縮率10 %」の基準を下げる

* いまは **10 %短縮＞成功** 判定。
  実運用では “同語 3 コ以上→1 つ” でも読者体感 OK。
* **推奨**: `min_compress_rate = 0.05` (5 %) + パターン数評価。
  → テスト成功率が＋8〜10 pt 上がる余地あり。

---

## 5️⃣ unit-test & CI をズラし防止に合わせる

```yaml
matrix:
  sim_thresh: [0.50]
  ngram_block: [3,4]      # ← 2 パターン
  min_compress: [0.05,0.10]
```

* PR で“基準上げ”が破壊しない限り fail しないよう、CI 側を調整。

---

## 6️⃣ ワンライナーで検証

```bash
python demo_repetition_v3.py \
  --sim 0.50 --max_dist 50 --ngram 4 \
  --min_compress 0.05 --drp-base 1.05 --drp-alpha 0.4
```

目標：**成功率 ≥ 75 %**（手動検証含め 80 % 刻みへ）。

---

### 📌 まとめスプリント

1. **サンプリング n-gram ブロック** → 未然防止
2. **MeCab 正規化 + 異常連番検知** → 漏れ削り
3. **修辞的例外リスト** → 過剰抑制を緩和
4. **min\_compress 5 %** → 評価基準を実用寄りに
5. **CI 行列更新** → 安定継続テスト

これで 80 %ラインが見えてくるはず！
ログ＆新テスト結果が出たらまた投げてね。さらに磨き込もう、ボブにゃん🔥


---

_**Assistant**_



---

おつかれさま！素晴らしい分析と改善案ですね🔥　58.3%から80%への「ラスト21.7pt」を詰めるための戦略、とても実践的で効果的だと思います。

まずは現在のシステム状況を確認して、v3アップグレードを実装していきましょう！

---

<details>
            <summary>Listed directory src/utils • **9** results</summary>
        
| Name |
|-------|
| 📁 `__pycache__` |
| 📄 `repetition_suppressor.py` |
| 📄 `quality_guard.py` |
| 📄 `README.md` |
| 📄 `context.py` |
| 📄 `path.py` |
| 📄 `movie_maker.py` |
| 📄 `job_queue.py` |
| 📄 `const.py` |

</details>

---

<details>
            <summary>Searched codebase "repetition_suppressor AdvancedRepetitionSuppressor" • **25** results</summary>
        
| File | Lines |
|------|-------|
| `src/utils/repetition_suppressor.py` | L773-809 |
| `src/utils/repetition_suppressor.py` | L185-218 |
| `src/utils/repetition_suppressor.py` | L977-1017 |
| `src/utils/repetition_suppressor.py` | L731-773 |
| `src/utils/repetition_suppressor.py` | L809-844 |
| `src/utils/repetition_suppressor.py` | L671-705 |
| `src/utils/repetition_suppressor.py` | L365-407 |
| `src/utils/repetition_suppressor.py` | L844-889 |
| `src/utils/repetition_suppressor.py` | L1017-1044 |
| `src/utils/repetition_suppressor.py` | L645-671 |
| `src/utils/repetition_suppressor.py` | L218-261 |
| `src/utils/repetition_suppressor.py` | L82-125 |
| `src/utils/repetition_suppressor.py` | L889-913 |
| `src/utils/repetition_suppressor.py` | L40-82 |
| `easy_novel_assistant.py` | L94-122 |
| `src/utils/repetition_suppressor.py` | L601-645 |
| `src/utils/repetition_suppressor.py` | L261-306 |
| `src/utils/repetition_suppressor.py` | L548-601 |
| `src/utils/repetition_suppressor.py` | L705-731 |
| `src/utils/repetition_suppressor.py` | L939-977 |
| `src/utils/repetition_suppressor.py` | L1-27 |
| `src/utils/repetition_suppressor.py` | L505-527 |
| `src/utils/repetition_suppressor.py` | L306-334 |
| `src/utils/repetition_suppressor.py` | L451-485 |
| `src/utils/repetition_suppressor.py` | L334-365 |

</details>

---

Read file: test_repetition_enhancement_v2.py

---

```bash
py -3 demo_repetition_v2_quick.py
```

```
PS C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main>  





























PS C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main> 















```

---

現在のシステム状況を確認したので、提案していただいた「ラスト21.7pt」を詰める v3 システムを実装していきます！

---

```diff
- 
+ # -*- coding: utf-8 -*-
+ """
+ 高度語句反復抑制システム v3 (Advanced Repetition Suppression System v3)
+ 成功率80%+達成のための「ラスト21.7pt」強化版
+ 
+ 新機能 v3:
+ 1. サンプリング側4-gramブロック（未然防止）
+ 2. MeCab + UniDic語基底形正規化
+ 3. 誤抑制最小化（修辞的例外、音象徴語保護）
+ 4. 評価基準調整（min_compress_rate: 10% → 5%）
+ 5. ラテン文字・数字連番検知
+ 6. CI行列テスト対応
+ """
+ 
+ import re
+ import time
+ import json
+ import os
+ from typing import List, Dict, Tuple, Optional, Set
+ from collections import defaultdict, Counter
+ from dataclasses import dataclass
+ import numpy as np
+ from tqdm import tqdm
+ 
+ # MeCab統合（オプション）
+ try:
+     import fugashi
+     MECAB_AVAILABLE = True
+ except ImportError:
+     MECAB_AVAILABLE = False
+     print("⚠️ MeCab（fugashi）が利用できません。基本機能のみで動作します。")
+ 
+ 
+ @dataclass
+ class SuppressionMetricsV3:
+     """v3強化版メトリクス"""
+     input_length: int
+     output_length: int
+     patterns_detected: int
+     patterns_suppressed: int
+     detection_misses: int
+     over_compressions: int
+     processing_time_ms: float
+     success_rate: float
+     
+     # v3新項目
+     ngram_blocks_applied: int = 0
+     mecab_normalizations: int = 0
+     rhetorical_exceptions: int = 0
+     latin_number_blocks: int = 0
+     min_compress_rate: float = 0.05  # 5%基準
+ 
+ 
+ @dataclass
+ class RepetitionPatternV3:
+     """v3版反復パターン（例外フラグ付き）"""
+     pattern: str
+     count: int
+     positions: List[int]
+     pattern_type: str
+     severity: float
+     similarity_score: float = 0.0
+     suppression_result: str = ""  # MISS/OVER/OK/SKIP
+     
+     # v3新フィールド
+     is_rhetorical: bool = False  # 修辞的反復
+     is_onomatopoeia: bool = False  # 音象徴語
+     is_lyrical: bool = False  # 歌詞風リフレイン
+     normalized_form: str = ""  # MeCab正規化形
+ 
+ 
+ class AdvancedRepetitionSuppressorV3:
+     """
+     高度語句反復抑制システム v3
+     58.3% → 80% への「ラスト21.7pt」強化版
+     """
+     
+     def __init__(self, config: Dict = None):
+         self.config = config or {}
+         
+         # 基本設定（v3チューニング済み）
+         self.min_repeat_threshold = self.config.get('min_repeat_threshold', 1)
+         self.max_distance = self.config.get('max_distance', 50)
+         self.similarity_threshold = self.config.get('similarity_threshold', 0.50)  # v3: 0.50に調整
+         self.phonetic_threshold = self.config.get('phonetic_threshold', 0.7)
+         
+         # v3新機能設定
+         self.enable_4gram_blocking = self.config.get('enable_4gram_blocking', True)
+         self.enable_mecab_normalization = self.config.get('enable_mecab_normalization', MECAB_AVAILABLE)
+         self.enable_rhetorical_protection = self.config.get('enable_rhetorical_protection', True)
+         self.min_compress_rate = self.config.get('min_compress_rate', 0.05)  # 5%基準
+         self.enable_latin_number_detection = self.config.get('enable_latin_number_detection', True)
+         
+         # 従来設定
+         self.enable_aggressive_mode = self.config.get('enable_aggressive_mode', True)
+         self.debug_mode = self.config.get('debug_mode', True)
+         self.enable_drp = self.config.get('enable_drp', True)
+         self.drp_base = self.config.get('drp_base', 1.05)
+         self.drp_alpha = self.config.get('drp_alpha', 0.4)
+         
+         # MeCab初期化
+         self.tagger = None
+         if self.enable_mecab_normalization and MECAB_AVAILABLE:
+             try:
+                 self.tagger = fugashi.Tagger()
+                 print("✅ MeCab初期化成功（語基底形正規化有効）")
+             except Exception as e:
+                 print(f"⚠️ MeCab初期化失敗: {e}")
+                 self.enable_mecab_normalization = False
+         
+         # 修辞的例外パターン
+         self.rhetorical_patterns = [
+             r'([^。！？]{1,5})、\1、\1[！？。]',  # 「ねえ、ねえ、ねえ！」
+             r'♪.*',  # 歌詞風
+             r'.*ら.*ら.*',  # ♪ラララ系
+         ]
+         
+         # 音象徴語パターン
+         self.onomatopoeia_patterns = [
+             r'[ァ-ン]{2}\1*',  # ドキドキ、ワクワク等
+             r'([ぁ-ん]{2})\1*',  # わくわく等
+         ]
+         
+         # ラテン文字・数字連番パターン
+         self.latin_number_patterns = [
+             r'([A-Za-z0-9])\1{3,}',  # aaaa, 7777等
+             r'([A-Za-z])\1{2,}',  # www, aaa等
+         ]
+         
+         # メトリクス
+         self.session_metrics = []
+         self.total_success_count = 0
+         self.total_attempts = 0
+         
+         # 処理履歴
+         self.debug_log = []
+         self.replacement_cache = {}
+         self.character_patterns = defaultdict(list)
+         
+         # 代替表現辞書の読み込み
+         self.load_replacement_dictionary_v3()
+         
+         print(f"🚀 反復抑制システム v3 初期化完了")
+         print(f"   ├─ 4-gramブロック: {'有効' if self.enable_4gram_blocking else '無効'}")
+         print(f"   ├─ MeCab正規化: {'有効' if self.enable_mecab_normalization else '無効'}")
+         print(f"   ├─ 修辞的保護: {'有効' if self.enable_rhetorical_protection else '無効'}")
+         print(f"   ├─ 成功判定基準: {self.min_compress_rate:.1%}")
+         print(f"   └─ 連番検知: {'有効' if self.enable_latin_number_detection else '無効'}")
+ 
+     def load_replacement_dictionary_v3(self):
+         """v3版代替表現辞書の読み込み"""
+         self.replacement_dict = {
+             # 基本的な代替表現
+             'そうです': ['はい', 'ええ', 'その通りです'],
+             'そうですね': ['ですね', 'そうですよね', 'おっしゃる通りです'],
+             'とても': ['すごく', 'かなり', 'とっても'],
+             'でしょう': ['だと思います', 'かもしれません', 'ですよね'],
+             'ですよね': ['ですものね', 'ですからね', 'そうですよね'],
+             
+             # 感嘆詞系
+             'わあ': ['うわあ', 'きゃあ', 'あら'],
+             'うわあ': ['わあ', 'うひゃあ', 'おおお'],
+             
+             # 関西弁
+             'そや': ['そうや', 'せやな', 'そやな'],
+             'あかん': ['だめ', 'いかん', 'あきまへん'],
+             'やな': ['いや', 'だめ', 'あかん'],
+             
+             # 語尾
+             'です': ['ます', 'である', 'だ'],
+             'ます': ['です', 'る', 'だ'],
+             
+             # 修飾語
+             'いい': ['良い', '素晴らしい', '素敵な'],
+             '良い': ['いい', '素晴らしい', '優れた'],
+             '嬉しい': ['楽しい', '幸せ', '喜ばしい'],
+             '楽しい': ['嬉しい', '愉快', '面白い'],
+         }
+ 
+     def suppress_repetitions_with_debug_v3(self, text: str, character_name: str = None) -> Tuple[str, SuppressionMetricsV3]:
+         """
+         v3版反復抑制処理（デバッグ強化版）
+         """
+         start_time = time.time()
+         original_text = text
+         
+         # フェーズ1: サンプリング側未然防止
+         text = self._apply_sampling_prevention(text)
+         
+         # フェーズ2: MeCab正規化
+         normalized_info = self._apply_mecab_normalization(text) if self.enable_mecab_normalization else None
+         
+         # フェーズ3: パターン分析（v3強化版）
+         analysis = self.analyze_text_v3(text, character_name, normalized_info)
+         
+         # フェーズ4: 例外判定（修辞的保護）
+         protected_patterns = self._apply_rhetorical_protection(analysis['patterns'])
+         
+         # フェーズ5: 抑制処理実行
+         suppressed_count = 0
+         missed_count = 0
+         over_compressed_count = 0
+         rhetorical_exceptions = 0
+         
+         for pattern_type, pattern_list in analysis['patterns'].items():
+             for pattern in pattern_list:
+                 if pattern.severity > 0.1:
+                     # 例外チェック
+                     if pattern in protected_patterns:
+                         pattern.suppression_result = "SKIP"
+                         rhetorical_exceptions += 1
+                         continue
+                     
+                     old_text = text
+                     text = self._apply_suppression_v3(text, pattern, character_name)
+                     
+                     if text != old_text:
+                         suppressed_count += 1
+                         pattern.suppression_result = "OK"
+                         
+                         # 過剰圧縮チェック（v3強化）
+                         if self._is_over_compressed_v3(old_text, text):
+                             over_compressed_count += 1
+                             pattern.suppression_result = "OVER"
+                     else:
+                         missed_count += 1
+                         pattern.suppression_result = "MISS"
+         
+         # フェーズ6: 最終クリーンアップ
+         text = self._final_cleanup_v3(text)
+         
+         # メトリクス計算（v3版）
+         processing_time = (time.time() - start_time) * 1000
+         total_patterns = sum(len(patterns) for patterns in analysis['patterns'].values())
+         
+         # 成功率計算（v3基準）
+         compression_rate = (len(original_text) - len(text)) / len(original_text) if len(original_text) > 0 else 0
+         meets_compression_target = compression_rate >= self.min_compress_rate
+         success_rate = (suppressed_count / total_patterns) if total_patterns > 0 else 1.0
+         
+         # v3では圧縮率も成功判定に含める
+         if total_patterns > 0 and not meets_compression_target:
+             success_rate *= 0.8  # ペナルティ
+         
+         metrics = SuppressionMetricsV3(
+             input_length=len(original_text),
+             output_length=len(text),
+             patterns_detected=total_patterns,
+             patterns_suppressed=suppressed_count,
+             detection_misses=missed_count,
+             over_compressions=over_compressed_count,
+             processing_time_ms=processing_time,
+             success_rate=success_rate,
+             ngram_blocks_applied=getattr(self, '_ngram_blocks_applied', 0),
+             mecab_normalizations=getattr(self, '_mecab_normalizations', 0),
+             rhetorical_exceptions=rhetorical_exceptions,
+             latin_number_blocks=getattr(self, '_latin_blocks_applied', 0),
+             min_compress_rate=self.min_compress_rate
+         )
+         
+         # セッション記録
+         self.session_metrics.append(metrics)
+         self.total_attempts += 1
+         if success_rate >= 0.75:  # v3では75%以上を成功とみなす
+             self.total_success_count += 1
+         
+         # デバッグ出力
+         if self.debug_mode:
+             print(f"🔧 v3処理結果: 成功率 {success_rate:.1%}, 圧縮率 {compression_rate:.1%}")
+             if missed_count > 0 or over_compressed_count > 0:
+                 print(f"   ├─ 検知漏れ: {missed_count}, 過剰圧縮: {over_compressed_count}")
+             if rhetorical_exceptions > 0:
+                 print(f"   └─ 修辞的例外: {rhetorical_exceptions}")
+         
+         return text, metrics
+ 
+     def _apply_sampling_prevention(self, text: str) -> str:
+         """フェーズ1: サンプリング側未然防止"""
+         result = text
+         self._ngram_blocks_applied = 0
+         self._latin_blocks_applied = 0
+         
+         # 4-gramブロック
+         if self.enable_4gram_blocking:
+             result = self._apply_4gram_blocking(result)
+         
+         # ラテン文字・数字連番除去
+         if self.enable_latin_number_detection:
+             result = self._remove_latin_number_sequences(result)
+         
+         # 動的Repeat-Penalty（改良版）
+         if self.enable_drp:
+             result = self._apply_enhanced_drp(result)
+         
+         return result
+ 
+     def _apply_4gram_blocking(self, text: str) -> str:
+         """4-gramレベルでの語順コピー除去"""
+         if len(text) < 4:
+             return text
+         
+         # 4-gramを抽出
+         ngrams = []
+         for i in range(len(text) - 3):
+             ngram = text[i:i+4]
+             if re.search(r'[ひらがなカタカナ漢字]', ngram):
+                 ngrams.append((ngram, i))
+         
+         # 重複4-gramを検出
+         ngram_counts = Counter([ngram for ngram, _ in ngrams])
+         repeated_ngrams = {ngram for ngram, count in ngram_counts.items() if count > 1}
+         
+         if not repeated_ngrams:
+             return text
+         
+         # 2回目以降の出現を削除
+         result = text
+         for ngram in repeated_ngrams:
+             positions = [m.start() for m in re.finditer(re.escape(ngram), result)]
+             if len(positions) > 1:
+                 # 後ろから削除（位置ずれ回避）
+                 for pos in reversed(positions[1:]):
+                     result = result[:pos] + result[pos+4:]
+                 self._ngram_blocks_applied += 1
+         
+         if result != text and self.debug_mode:
+             self.debug_log.append(f"4-gramブロック: {len(repeated_ngrams)}パターン削除")
+         
+         return result
+ 
+     def _remove_latin_number_sequences(self, text: str) -> str:
+         """ラテン文字・数字連番の除去"""
+         result = text
+         
+         for pattern in self.latin_number_patterns:
+             matches = list(re.finditer(pattern, result))
+             for match in reversed(matches):  # 後ろから処理
+                 original = match.group()
+                 if len(original) > 3:
+                     # 3文字以下に短縮
+                     replacement = original[0] * 2
+                     result = result[:match.start()] + replacement + result[match.end():]
+                     self._latin_blocks_applied += 1
+         
+         if result != text and self.debug_mode:
+             self.debug_log.append(f"連番除去: {self._latin_blocks_applied}箇所")
+         
+         return result
+ 
+     def _apply_enhanced_drp(self, text: str) -> str:
+         """強化版動的Repeat-Penalty"""
+         if len(text) < 10:
+             return text
+         
+         window_size = 8
+         result = ""
+         
+         for i, char in enumerate(text):
+             if i < window_size:
+                 result += char
+                 continue
+             
+             window = text[i-window_size:i]
+             char_freq = window.count(char) / window_size
+             
+             # DRP適用判定
+             penalty_threshold = self.drp_base - (self.drp_alpha * char_freq)
+             
+             if char_freq < 0.3 or char in "。、！？\n「」":
+                 result += char
+             # else: 文字をスキップ
+         
+         return result
+ 
+     def _apply_mecab_normalization(self, text: str) -> Dict:
+         """MeCab + UniDic語基底形正規化"""
+         if not self.tagger:
+             return {}
+         
+         try:
+             tokens = []
+             for token in self.tagger(text):
+                 lemma = token.feature.lemma if hasattr(token.feature, 'lemma') else token.surface
+                 tokens.append({
+                     'surface': token.surface,
+                     'lemma': lemma or token.surface,
+                     'pos': token.feature.pos1 if hasattr(token.feature, 'pos1') else 'Unknown'
+                 })
+                 self._mecab_normalizations += 1
+             
+             return {
+                 'tokens': tokens,
+                 'normalized_text': ''.join(token['lemma'] for token in tokens)
+             }
+         except Exception as e:
+             if self.debug_mode:
+                 print(f"MeCab処理エラー: {e}")
+             return {}
+ 
+     def _apply_rhetorical_protection(self, patterns: Dict) -> Set:
+         """修辞的表現の保護判定"""
+         protected = set()
+         
+         if not self.enable_rhetorical_protection:
+             return protected
+         
+         for pattern_type, pattern_list in patterns.items():
+             for pattern in pattern_list:
+                 # 修辞的反復の判定
+                 if self._is_rhetorical_repetition(pattern):
+                     pattern.is_rhetorical = True
+                     protected.add(pattern)
+                 
+                 # 音象徴語の判定
+                 if self._is_onomatopoeia(pattern):
+                     pattern.is_onomatopoeia = True
+                     protected.add(pattern)
+                 
+                 # 歌詞風リフレインの判定
+                 if self._is_lyrical_refrain(pattern):
+                     pattern.is_lyrical = True
+                     protected.add(pattern)
+         
+         return protected
+ 
+     def _is_rhetorical_repetition(self, pattern: RepetitionPatternV3) -> bool:
+         """修辞的反復の判定"""
+         for rhetorical_pattern in self.rhetorical_patterns:
+             if re.search(rhetorical_pattern, pattern.pattern):
+                 return True
+         
+         # 句点を挟む短い反復（5文字以下）
+         if len(pattern.pattern) <= 5 and '、' in pattern.pattern:
+             return True
+         
+         return False
+ 
+     def _is_onomatopoeia(self, pattern: RepetitionPatternV3) -> bool:
+         """音象徴語の判定"""
+         for ono_pattern in self.onomatopoeia_patterns:
+             if re.match(ono_pattern, pattern.pattern):
+                 return True
+         return False
+ 
+     def _is_lyrical_refrain(self, pattern: RepetitionPatternV3) -> bool:
+         """歌詞風リフレインの判定"""
+         # 改行ごとの同語、♪記号の存在
+         if '♪' in pattern.pattern or '\n' in pattern.pattern:
+             return True
+         return False
+ 
+     def analyze_text_v3(self, text: str, character_name: str = None, normalized_info: Dict = None) -> Dict:
+         """v3版テキスト分析（MeCab統合版）"""
+         
+         # 基本パターン検出
+         patterns = {
+             'exact_repetitions': self._detect_exact_repetitions_v3(text, normalized_info),
+             'character_repetitions': self._detect_character_repetitions(text),
+             'phonetic_repetitions': self._detect_phonetic_repetitions(text),
+             'interjection_overuse': self._detect_interjection_overuse(text),
+             'latin_number_repetitions': self._detect_latin_number_repetitions(text)
+         }
+         
+         # アグレッシブモードでは語レベル検出も追加
+         if self.enable_aggressive_mode:
+             patterns['word_repetitions'] = self._detect_word_repetitions_v3(text, normalized_info)
+         
+         # 総合重要度計算
+         total_severity = sum(
+             sum(p.severity for p in pattern_list)
+             for pattern_list in patterns.values()
+         )
+         
+         return {
+             'patterns': patterns,
+             'total_severity': total_severity,
+             'character': character_name,
+             'text_length': len(text),
+             'normalized_info': normalized_info,
+             'timestamp': time.time()
+         }
+ 
+     def _detect_exact_repetitions_v3(self, text: str, normalized_info: Dict = None) -> List[RepetitionPatternV3]:
+         """v3版完全一致検出（MeCab統合）"""
+         patterns = []
+         
+         # 従来の文字ベース検出
+         for length in range(2, min(20, len(text) // 2)):
+             for i in range(len(text) - length):
+                 phrase = text[i:i+length]
+                 
+                 if not re.search(r'[ひらがなカタカナ漢字]', phrase):
+                     continue
+                 
+                 positions = []
+                 for j in range(i + length, len(text) - length + 1):
+                     if text[j:j+length] == phrase:
+                         positions.append(j)
+                 
+                 if len(positions) >= self.min_repeat_threshold - 1:
+                     severity = min(1.0, (len(positions) * length) / 10.0)
+                     pattern = RepetitionPatternV3(
+                         pattern=phrase,
+                         count=len(positions) + 1,
+                         positions=[i] + positions,
+                         pattern_type='exact',
+                         severity=severity
+                     )
+                     
+                     # MeCab正規化情報を追加
+                     if normalized_info:
+                         pattern.normalized_form = self._get_normalized_form(phrase, normalized_info)
+                     
+                     patterns.append(pattern)
+         
+         # MeCabベースの語基底形検出
+         if normalized_info and self.enable_mecab_normalization:
+             patterns.extend(self._detect_lemma_repetitions(normalized_info))
+         
+         return self._filter_overlapping_patterns(patterns)
+ 
+     def _detect_lemma_repetitions(self, normalized_info: Dict) -> List[RepetitionPatternV3]:
+         """語基底形ベースの反復検出"""
+         patterns = []
+         tokens = normalized_info.get('tokens', [])
+         
+         # 語基底形での重複チェック
+         lemma_positions = defaultdict(list)
+         for i, token in enumerate(tokens):
+             if len(token['lemma']) >= 2 and token['pos'] in ['名詞', '動詞', '形容詞']:
+                 lemma_positions[token['lemma']].append(i)
+         
+         for lemma, positions in lemma_positions.items():
+             if len(positions) >= self.min_repeat_threshold:
+                 severity = min(1.0, len(positions) / 5.0)
+                 pattern = RepetitionPatternV3(
+                     pattern=f"語基底形: {lemma}",
+                     count=len(positions),
+                     positions=positions,
+                     pattern_type='lemma',
+                     severity=severity,
+                     normalized_form=lemma
+                 )
+                 patterns.append(pattern)
+         
+         return patterns
+ 
+     def _detect_latin_number_repetitions(self, text: str) -> List[RepetitionPatternV3]:
+         """ラテン文字・数字反復の検出"""
+         patterns = []
+         
+         for pattern_str in self.latin_number_patterns:
+             matches = list(re.finditer(pattern_str, text))
+             
+             if matches:
+                 for match in matches:
+                     severity = min(1.0, len(match.group()) / 5.0)
+                     pattern = RepetitionPatternV3(
+                         pattern=match.group(),
+                         count=1,
+                         positions=[match.start()],
+                         pattern_type='latin_number',
+                         severity=severity
+                     )
+                     patterns.append(pattern)
+         
+         return patterns
+ 
+     def _get_normalized_form(self, phrase: str, normalized_info: Dict) -> str:
+         """語句の正規化形を取得"""
+         tokens = normalized_info.get('tokens', [])
+         
+         # phraseに含まれるトークンの語基底形を結合
+         normalized_parts = []
+         for token in tokens:
+             if token['surface'] in phrase:
+                 normalized_parts.append(token['lemma'])
+         
+         return ''.join(normalized_parts) if normalized_parts else phrase
+ 
+     def _filter_overlapping_patterns(self, patterns: List[RepetitionPatternV3]) -> List[RepetitionPatternV3]:
+         """重複パターンのフィルタリング（v3強化版）"""
+         patterns.sort(key=lambda p: (p.severity, len(p.pattern)), reverse=True)
+         filtered = []
+         used_positions = set()
+         
+         for pattern in patterns:
+             if not any(pos in used_positions for pos in pattern.positions):
+                 filtered.append(pattern)
+                 used_positions.update(pattern.positions)
+         
+         return filtered
+ 
+     def _apply_suppression_v3(self, text: str, pattern: RepetitionPatternV3, character_name: str = None) -> str:
+         """v3版抑制処理適用"""
+         if pattern.pattern_type == 'exact':
+             return self._suppress_exact_repetition_v3(text, pattern)
+         elif pattern.pattern_type == 'lemma':
+             return self._suppress_lemma_repetition(text, pattern)
+         elif pattern.pattern_type == 'character':
+             return self._suppress_character_repetition(text, pattern)
+         elif pattern.pattern_type == 'latin_number':
+             return self._suppress_latin_number_repetition(text, pattern)
+         elif pattern.pattern_type == 'interjection':
+             return self._suppress_interjection_overuse(text, pattern)
+         else:
+             return text
+ 
+     def _suppress_exact_repetition_v3(self, text: str, pattern: RepetitionPatternV3) -> str:
+         """v3版完全一致抑制（スマート置換）"""
+         original_phrase = pattern.pattern
+         
+         # 代替表現の取得（v3強化版）
+         alternatives = self._get_smart_alternatives_v3(original_phrase, pattern)
+         
+         if not alternatives:
+             # 代替表現がない場合は削除
+             return self._smart_removal(text, pattern)
+         
+         # スマート置換実行
+         result = text
+         positions = sorted(pattern.positions, reverse=True)
+         
+         for i, pos in enumerate(positions[1:], 1):
+             if i < len(alternatives):
+                 replacement = alternatives[i - 1]
+                 end_pos = pos + len(original_phrase)
+                 result = result[:pos] + replacement + result[end_pos:]
+         
+         return result
+ 
+     def _get_smart_alternatives_v3(self, phrase: str, pattern: RepetitionPatternV3) -> List[str]:
+         """v3版スマート代替表現生成"""
+         alternatives = []
+         
+         # 基本辞書から検索
+         if phrase in self.replacement_dict:
+             alternatives.extend(self.replacement_dict[phrase])
+         
+         # MeCab情報を活用した代替表現生成
+         if pattern.normalized_form and pattern.normalized_form != phrase:
+             if pattern.normalized_form in self.replacement_dict:
+                 alternatives.extend(self.replacement_dict[pattern.normalized_form])
+         
+         # 動的生成
+         if not alternatives:
+             alternatives = self._generate_dynamic_alternatives(phrase)
+         
+         return alternatives[:3]  # 最大3つまで
+ 
+     def _generate_dynamic_alternatives(self, phrase: str) -> List[str]:
+         """動的代替表現生成"""
+         alternatives = []
+         
+         # 語尾変化
+         if phrase.endswith('です'):
+             alternatives.extend(['ます', 'である'])
+         elif phrase.endswith('ます'):
+             alternatives.extend(['です', 'る'])
+         elif phrase.endswith('だ'):
+             alternatives.extend(['である', 'です'])
+         
+         # 程度副詞の変換
+         if 'とても' in phrase:
+             alternatives.append(phrase.replace('とても', 'すごく'))
+         if 'すごく' in phrase:
+             alternatives.append(phrase.replace('すごく', 'かなり'))
+         
+         return alternatives
+ 
+     def _smart_removal(self, text: str, pattern: RepetitionPatternV3) -> str:
+         """スマート削除（文脈保持）"""
+         result = text
+         positions = sorted(pattern.positions, reverse=True)
+         
+         # 2回目以降の出現を削除（文脈を考慮）
+         for pos in positions[1:]:
+             # 周辺文脈チェック
+             before = text[max(0, pos-3):pos]
+             after = text[pos+len(pattern.pattern):pos+len(pattern.pattern)+3]
+             
+             # 句読点がある場合は削除しやすい
+             if any(char in before + after for char in "。、！？"):
+                 end_pos = pos + len(pattern.pattern)
+                 result = result[:pos] + result[end_pos:]
+         
+         return result
+ 
+     def _is_over_compressed_v3(self, original: str, compressed: str) -> bool:
+         """v3版過剰圧縮判定（強化版）"""
+         compression_rate = (len(original) - len(compressed)) / len(original) if len(original) > 0 else 0
+         
+         # 50%以上の圧縮は過剰
+         if compression_rate > 0.5:
+             return True
+         
+         # 重要情報の消失チェック（v3強化版）
+         important_elements = [
+             r'[。！？]',  # 句読点
+             r'[「」]',    # 括弧
+             r'[ァ-ン]{3,}',  # 長いカタカナ
+             r'[漢字]{2,}',   # 漢字語
+         ]
+         
+         for pattern in important_elements:
+             orig_count = len(re.findall(pattern, original))
+             comp_count = len(re.findall(pattern, compressed))
+             
+             if orig_count > 0 and comp_count < orig_count * 0.6:  # 60%基準
+                 return True
+         
+         return False
+ 
+     def _final_cleanup_v3(self, text: str) -> str:
+         """v3版最終クリーンアップ"""
+         result = text
+         
+         # 句読点レベルの重複除去
+         result = re.sub(r'、{2,}', '、', result)  # 、、 → 、
+         result = re.sub(r'…{4,}', '……', result)  # ……… → ……
+         result = re.sub(r'！{3,}', '！！', result)  # ！！！ → ！！
+         result = re.sub(r'？{3,}', '？？', result)  # ？？？ → ？？
+         
+         # 空白の正規化
+         result = re.sub(r'\s{3,}', '  ', result)  # 過度な空白
+         
+         return result
+ 
+     def get_v3_performance_report(self) -> Dict:
+         """v3版パフォーマンスレポート"""
+         if not self.session_metrics:
+             return {"message": "セッション データがありません"}
+         
+         recent_metrics = self.session_metrics[-10:]  # 最新10件
+         
+         avg_success_rate = sum(m.success_rate for m in recent_metrics) / len(recent_metrics)
+         avg_compression_rate = sum((m.input_length - m.output_length) / m.input_length for m in recent_metrics) / len(recent_metrics)
+         avg_processing_time = sum(m.processing_time_ms for m in recent_metrics) / len(recent_metrics)
+         
+         total_ngram_blocks = sum(m.ngram_blocks_applied for m in recent_metrics)
+         total_mecab_normalizations = sum(m.mecab_normalizations for m in recent_metrics)
+         total_rhetorical_exceptions = sum(m.rhetorical_exceptions for m in recent_metrics)
+         
+         return {
+             "v3_performance": {
+                 "average_success_rate": avg_success_rate,
+                 "target_achievement": avg_success_rate >= 0.8,
+                 "average_compression_rate": avg_compression_rate,
+                 "average_processing_time_ms": avg_processing_time
+             },
+             "v3_features": {
+                 "ngram_blocks_applied": total_ngram_blocks,
+                 "mecab_normalizations": total_mecab_normalizations,
+                 "rhetorical_exceptions": total_rhetorical_exceptions,
+                 "min_compress_rate": self.min_compress_rate
+             },
+             "improvement_from_v2": {
+                 "success_rate_target": "80%+",
+                 "compression_target": f"{self.min_compress_rate:.1%}+",
+                 "new_features": [
+                     "4-gramブロック",
+                     "MeCab語基底形正規化",
+                     "修辞的例外保護",
+                     "連番検知除去"
+                 ]
+             }
+         }
+ 
+     # その他のメソッド（v2からの継承・改良版）
+     def _detect_character_repetitions(self, text: str) -> List[RepetitionPatternV3]:
+         """文字反復検出（v2からの継承）"""
+         patterns = []
+         
+         for char in set(text):
+             if not re.match(r'[あ-んア-ンー]', char):
+                 continue
+                 
+             consecutive_pattern = f'{char}{{3,}}'
+             matches = list(re.finditer(consecutive_pattern, text))
+             
+             if matches:
+                 for match in matches:
+                     length = match.end() - match.start()
+                     severity = min(1.0, length / 5.0)
+                     patterns.append(RepetitionPatternV3(
+                         pattern=match.group(),
+                         count=1,
+                         positions=[match.start()],
+                         pattern_type='character',
+                         severity=severity
+                     ))
+         
+         return patterns
+ 
+     def _detect_phonetic_repetitions(self, text: str) -> List[RepetitionPatternV3]:
+         """音韻反復検出（v2からの継承）"""
+         patterns = []
+         
+         # 基本的な音韻変換マップ
+         katakana_map = str.maketrans(
+             'アイウエオカキクケコサシスセソタチツテトナニヌネノハヒフヘホマミムメモヤユヨラリルレロワヲン',
+             'あいうえおかきくけこさしすせそたちつてとはひふへほまみむめもやゆよらりるれろわをん'
+         )
+         
+         hiragana_text = text.translate(katakana_map)
+         
+         # 音韻グループ化
+         phonetic_groups = defaultdict(list)
+         
+         for length in range(2, 8):
+             for i in range(len(hiragana_text) - length):
+                 phrase = hiragana_text[i:i+length]
+                 if re.match(r'^[あ-ん]+$', phrase):
+                     normalized = self._normalize_phonetic(phrase)
+                     phonetic_groups[normalized].append((phrase, i))
+         
+         # 音韻類似パターンの検出
+         for normalized, occurrences in phonetic_groups.items():
+             if len(occurrences) >= self.min_repeat_threshold:
+                 unique_phrases = list(set(occ[0] for occ in occurrences))
+                 if len(unique_phrases) > 1:
+                     severity = min(1.0, len(occurrences) / 8.0)
+                     patterns.append(RepetitionPatternV3(
+                         pattern=f"音韻類似: {'/'.join(unique_phrases[:3])}",
+                         count=len(occurrences),
+                         positions=[occ[1] for occ in occurrences],
+                         pattern_type='phonetic',
+                         severity=severity
+                     ))
+         
+         return patterns
+ 
+     def _normalize_phonetic(self, text: str) -> str:
+         """音韻正規化"""
+         phonetic_map = str.maketrans(
+             'がぎぐげござじずぜぞだぢづでどばびぶべぼぱぴぷぺぽ',
+             'かきくけこさしすせそたちつてとはひふへほはひふへほ'
+         )
+         
+         normalized = text.translate(phonetic_map)
+         normalized = re.sub(r'[ゃゅょっ]', '', normalized)
+         
+         return normalized
+ 
+     def _detect_interjection_overuse(self, text: str) -> List[RepetitionPatternV3]:
+         """感嘆詞過多検出（v2からの継承）"""
+         patterns = []
+         
+         interjection_patterns = [
+             r'([あおうえい])\1{2,}',
+             r'([きひんしち])\1{1,}っ*',
+             r'([っー〜]){3,}',
+             r'([！？]){2,}',
+             r'(そや|だめ|いや){2,}',
+             r'([あ-ん])\1{2,}[っ〜…！？]*'
+         ]
+         
+         for pattern_str in interjection_patterns:
+             matches = list(re.finditer(pattern_str, text))
+             
+             if len(matches) >= 3:
+                 severity = min(1.0, len(matches) / 5.0)
+                 patterns.append(RepetitionPatternV3(
+                     pattern=f"感嘆詞過多: {pattern_str}",
+                     count=len(matches),
+                     positions=[m.start() for m in matches],
+                     pattern_type='interjection',
+                     severity=severity
+                 ))
+         
+         return patterns
+ 
+     def suppress_repetitions(self, text: str, character_name: str = None) -> str:
+         """互換性維持用のメソッド（v3処理を呼び出し）"""
+         result, _ = self.suppress_repetitions_with_debug_v3(text, character_name)
+         return result
+ 
+ # 互換性エイリアス
+ AdvancedRepetitionSuppressor = AdvancedRepetitionSuppressorV3
+ SuppressionMetrics = SuppressionMetricsV3
+ RepetitionPattern = RepetitionPatternV3 
```

---

次に、v3システムのテスト・デモスクリプトを作成します：

---

```diff
- 
+ # -*- coding: utf-8 -*-
+ """
+ 反復抑制システム v3 クイックデモ
+ 成功率80%達成のための「ラスト21.7pt」改善版
+ 
+ コマンドライン実行例:
+ python demo_repetition_v3_quick.py --sim 0.50 --max_dist 50 --ngram 4 --min_compress 0.05 --drp-base 1.05 --drp-alpha 0.4
+ """
+ 
+ import argparse
+ import json
+ import os
+ import sys
+ import time
+ from typing import Dict, List
+ 
+ # パス設定
+ current_dir = os.path.dirname(os.path.abspath(__file__))
+ src_dir = os.path.join(current_dir, "src")
+ sys.path.insert(0, src_dir)
+ 
+ try:
+     from src.utils.repetition_suppressor_v3 import AdvancedRepetitionSuppressorV3, SuppressionMetricsV3
+     print("✅ v3システム読み込み成功")
+ except ImportError as e:
+     print(f"❌ v3システム読み込み失敗: {e}")
+     print("   v2システムでフォールバックします")
+     from src.utils.repetition_suppressor import AdvancedRepetitionSuppressor, SuppressionMetrics
+     
+     # v3互換レイヤー
+     class AdvancedRepetitionSuppressorV3(AdvancedRepetitionSuppressor):
+         def suppress_repetitions_with_debug_v3(self, text, character_name=None):
+             result, metrics = self.suppress_repetitions_with_debug(text, character_name)
+             # v3メトリクスへの変換
+             v3_metrics = type('SuppressionMetricsV3', (), {
+                 'input_length': metrics.input_length,
+                 'output_length': metrics.output_length,
+                 'patterns_detected': metrics.patterns_detected,
+                 'patterns_suppressed': metrics.patterns_suppressed,
+                 'detection_misses': metrics.detection_misses,
+                 'over_compressions': metrics.over_compressions,
+                 'processing_time_ms': metrics.processing_time_ms,
+                 'success_rate': metrics.success_rate,
+                 'ngram_blocks_applied': 0,
+                 'mecab_normalizations': 0,
+                 'rhetorical_exceptions': 0,
+                 'latin_number_blocks': 0,
+                 'min_compress_rate': 0.05
+             })()
+             return result, v3_metrics
+     
+     SuppressionMetricsV3 = SuppressionMetrics
+ 
+ 
+ def create_v3_test_cases() -> List[Dict]:
+     """v3強化版テストケース（80%達成用）"""
+     return [
+         # === 基本同語反復（v3重点対象） ===
+         {
+             "name": "基本同語反復",
+             "input": "お兄ちゃんお兄ちゃん、どこに行くのですかお兄ちゃん？",
+             "character": "妹",
+             "expected_patterns": ["お兄ちゃん"],
+             "category": "same_word",
+             "target_compression": 0.15
+         },
+         {
+             "name": "関西弁同語反復",
+             "input": "そやそやそや、あかんあかんあかん、やなやなそれは。",
+             "character": "関西弁キャラ",
+             "expected_patterns": ["そや", "あかん", "やな"],
+             "category": "dialect",
+             "target_compression": 0.20
+         },
+         {
+             "name": "語尾反復複合",
+             "input": "ですですね、ますますよ、でしょでしょう。",
+             "character": "丁寧語キャラ",
+             "expected_patterns": ["です", "ます", "でしょ"],
+             "category": "ending",
+             "target_compression": 0.10
+         },
+         
+         # === 4-gramブロック対象 ===
+         {
+             "name": "4-gram反復",
+             "input": "今日は良い天気ですね。今日は良い天気だから散歩しましょう。",
+             "character": "普通の人",
+             "expected_patterns": ["今日は良い"],
+             "category": "ngram",
+             "target_compression": 0.08
+         },
+         
+         # === ラテン文字・数字連番 ===
+         {
+             "name": "連番反復",
+             "input": "wwwwww、そうですね。777777って数字ですか？",
+             "character": "ネットユーザー",
+             "expected_patterns": ["wwwwww", "777777"],
+             "category": "latin_number",
+             "target_compression": 0.12
+         },
+         
+         # === 修辞的例外保護テスト ===
+         {
+             "name": "修辞的反復保護",
+             "input": "ねえ、ねえ、ねえ！聞いてよ。",
+             "character": "感情的キャラ",
+             "expected_patterns": [],  # 保護されるべき
+             "category": "rhetorical",
+             "target_compression": 0.0  # 圧縮されない
+         },
+         {
+             "name": "音象徴語保護",
+             "input": "ドキドキしちゃう。ワクワクするね！",
+             "character": "元気キャラ",
+             "expected_patterns": [],  # 保護されるべき
+             "category": "onomatopoeia",
+             "target_compression": 0.0
+         },
+         
+         # === 複雑ケース ===
+         {
+             "name": "複合反復",
+             "input": "嬉しい嬉しい、楽しい楽しい、幸せ幸せという感じですです。",
+             "character": "ポジティブキャラ",
+             "expected_patterns": ["嬉しい", "楽しい", "幸せ", "です"],
+             "category": "complex",
+             "target_compression": 0.15
+         },
+         {
+             "name": "感嘆詞過多",
+             "input": "あああああああ！うわああああああ！きゃああああああ！",
+             "character": "感情豊かキャラ",
+             "expected_patterns": ["あああああああ", "うわああああああ", "きゃああああああ"],
+             "category": "interjection",
+             "target_compression": 0.25
+         },
+         
+         # === 境界ケース ===
+         {
+             "name": "短文反復",
+             "input": "はい、はい、はい。",
+             "character": "相槌キャラ",
+             "expected_patterns": ["はい"],
+             "category": "short",
+             "target_compression": 0.20
+         }
+     ]
+ 
+ 
+ def parse_args():
+     """コマンドライン引数の解析"""
+     parser = argparse.ArgumentParser(description='反復抑制システム v3 クイックデモ')
+     
+     # 基本パラメータ
+     parser.add_argument('--sim', type=float, default=0.50, help='類似度閾値 (default: 0.50)')
+     parser.add_argument('--max_dist', type=int, default=50, help='最大検出距離 (default: 50)')
+     parser.add_argument('--min_compress', type=float, default=0.05, help='最小圧縮率 (default: 0.05)')
+     
+     # v3新機能
+     parser.add_argument('--ngram', type=int, default=4, help='n-gramブロックサイズ (default: 4)')
+     parser.add_argument('--drp-base', type=float, default=1.05, help='DRP基準値 (default: 1.05)')
+     parser.add_argument('--drp-alpha', type=float, default=0.4, help='DRPアルファ値 (default: 0.4)')
+     parser.add_argument('--enable-mecab', action='store_true', help='MeCab正規化を有効化')
+     parser.add_argument('--disable-rhetorical', action='store_true', help='修辞的保護を無効化')
+     
+     # 実行オプション
+     parser.add_argument('--target-rate', type=float, default=0.80, help='目標成功率 (default: 0.80)')
+     parser.add_argument('--verbose', action='store_true', help='詳細出力')
+     parser.add_argument('--save-report', type=str, help='レポート保存ファイル名')
+     
+     return parser.parse_args()
+ 
+ 
+ def create_v3_config(args) -> Dict:
+     """引数からv3設定を構築"""
+     return {
+         'similarity_threshold': args.sim,
+         'max_distance': args.max_dist,
+         'min_compress_rate': args.min_compress,
+         'enable_4gram_blocking': args.ngram > 0,
+         'ngram_block_size': args.ngram,
+         'enable_drp': True,
+         'drp_base': args.drp_base,
+         'drp_alpha': args.drp_alpha,
+         'enable_mecab_normalization': args.enable_mecab,
+         'enable_rhetorical_protection': not args.disable_rhetorical,
+         'enable_aggressive_mode': True,
+         'debug_mode': args.verbose,
+         'min_repeat_threshold': 1,
+         'enable_latin_number_detection': True
+     }
+ 
+ 
+ def run_v3_quick_test(config: Dict, test_cases: List[Dict], target_success_rate: float = 0.8) -> Dict:
+     """v3クイックテスト実行"""
+     print(f"🚀 反復抑制システム v3 クイックテスト開始")
+     print(f"   目標成功率: {target_success_rate:.1%}")
+     print(f"   テストケース: {len(test_cases)}件")
+     print("=" * 60)
+     
+     # システム初期化
+     suppressor = AdvancedRepetitionSuppressorV3(config)
+     
+     results = []
+     category_stats = {}
+     
+     start_time = time.time()
+     
+     for i, test_case in enumerate(test_cases, 1):
+         print(f"\n📝 テスト {i}/{len(test_cases)}: {test_case['name']}")
+         
+         # 処理実行
+         result_text, metrics = suppressor.suppress_repetitions_with_debug_v3(
+             test_case['input'], 
+             test_case['character']
+         )
+         
+         # 成功判定（v3基準）
+         compression_rate = (metrics.input_length - metrics.output_length) / metrics.input_length
+         meets_compression_target = compression_rate >= test_case.get('target_compression', config['min_compress_rate'])
+         success = metrics.success_rate >= 0.7 and meets_compression_target  # v3では70%+圧縮達成を成功とする
+         
+         # 結果記録
+         test_result = {
+             'test_case': test_case['name'],
+             'category': test_case['category'],
+             'success': success,
+             'success_rate': metrics.success_rate,
+             'compression_rate': compression_rate,
+             'target_compression': test_case.get('target_compression', config['min_compress_rate']),
+             'input_length': metrics.input_length,
+             'output_length': metrics.output_length,
+             'patterns_detected': metrics.patterns_detected,
+             'patterns_suppressed': metrics.patterns_suppressed,
+             'processing_time_ms': metrics.processing_time_ms,
+             'input_text': test_case['input'][:50] + "..." if len(test_case['input']) > 50 else test_case['input'],
+             'output_text': result_text[:50] + "..." if len(result_text) > 50 else result_text,
+             'v3_features': {
+                 'ngram_blocks': getattr(metrics, 'ngram_blocks_applied', 0),
+                 'mecab_normalizations': getattr(metrics, 'mecab_normalizations', 0),
+                 'rhetorical_exceptions': getattr(metrics, 'rhetorical_exceptions', 0),
+                 'latin_blocks': getattr(metrics, 'latin_number_blocks', 0)
+             }
+         }
+         
+         results.append(test_result)
+         
+         # カテゴリ別統計
+         category = test_case['category']
+         if category not in category_stats:
+             category_stats[category] = {'total': 0, 'success': 0}
+         category_stats[category]['total'] += 1
+         if success:
+             category_stats[category]['success'] += 1
+         
+         # 結果表示
+         status_icon = "✅" if success else "❌"
+         print(f"   {status_icon} 成功率: {metrics.success_rate:.1%}, 圧縮率: {compression_rate:.1%}")
+         
+         if config.get('debug_mode', False):
+             print(f"      入力: {test_case['input'][:40]}...")
+             print(f"      出力: {result_text[:40]}...")
+             print(f"      処理時間: {metrics.processing_time_ms:.1f}ms")
+             
+             # v3機能の使用状況
+             v3_features = test_result['v3_features']
+             if any(v3_features.values()):
+                 print(f"      v3機能: 4-gram={v3_features['ngram_blocks']}, MeCab={v3_features['mecab_normalizations']}, 修辞={v3_features['rhetorical_exceptions']}, 連番={v3_features['latin_blocks']}")
+     
+     total_time = time.time() - start_time
+     
+     # 総合統計
+     total_tests = len(results)
+     total_success = sum(1 for r in results if r['success'])
+     overall_success_rate = total_success / total_tests if total_tests > 0 else 0
+     
+     avg_compression = sum(r['compression_rate'] for r in results) / total_tests if total_tests > 0 else 0
+     avg_processing_time = sum(r['processing_time_ms'] for r in results) / total_tests if total_tests > 0 else 0
+     
+     # v3機能統計
+     total_ngram_blocks = sum(r['v3_features']['ngram_blocks'] for r in results)
+     total_mecab_normalizations = sum(r['v3_features']['mecab_normalizations'] for r in results)
+     total_rhetorical_exceptions = sum(r['v3_features']['rhetorical_exceptions'] for r in results)
+     total_latin_blocks = sum(r['v3_features']['latin_blocks'] for r in results)
+     
+     print("\n" + "=" * 60)
+     print(f"📊 v3システム総合結果")
+     print(f"   成功率: {overall_success_rate:.1%} ({total_success}/{total_tests})")
+     print(f"   目標達成: {'🎉 達成！' if overall_success_rate >= target_success_rate else '❌ 未達'}")
+     print(f"   平均圧縮率: {avg_compression:.1%}")
+     print(f"   平均処理時間: {avg_processing_time:.1f}ms")
+     print(f"   総処理時間: {total_time:.2f}秒")
+     
+     print(f"\n🔧 v3機能使用状況:")
+     print(f"   4-gramブロック: {total_ngram_blocks}回")
+     print(f"   MeCab正規化: {total_mecab_normalizations}回")
+     print(f"   修辞的例外: {total_rhetorical_exceptions}回")
+     print(f"   連番除去: {total_latin_blocks}回")
+     
+     print(f"\n📈 カテゴリ別成功率:")
+     for category, stats in category_stats.items():
+         success_rate = stats['success'] / stats['total']
+         print(f"   {category}: {success_rate:.1%} ({stats['success']}/{stats['total']})")
+     
+     # 失敗ケースの分析
+     failed_cases = [r for r in results if not r['success']]
+     if failed_cases:
+         print(f"\n❌ 失敗ケース分析 ({len(failed_cases)}件):")
+         for case in failed_cases:
+             print(f"   - {case['test_case']}: 成功率{case['success_rate']:.1%}, 圧縮率{case['compression_rate']:.1%}")
+     
+     # 改善提案
+     if overall_success_rate < target_success_rate:
+         gap = target_success_rate - overall_success_rate
+         print(f"\n🔍 改善提案（残り{gap:.1%}pt）:")
+         
+         if avg_compression < config['min_compress_rate']:
+             print(f"   - 圧縮率向上: 現在{avg_compression:.1%} → 目標{config['min_compress_rate']:.1%}")
+             print(f"     → similarity_threshold を {config['similarity_threshold']:.2f} → {max(0.3, config['similarity_threshold'] - 0.1):.2f} に下げる")
+         
+         worst_category = min(category_stats.items(), key=lambda x: x[1]['success'] / x[1]['total'])
+         print(f"   - 最弱カテゴリ: {worst_category[0]} ({worst_category[1]['success']}/{worst_category[1]['total']})")
+         
+         if total_ngram_blocks == 0:
+             print(f"   - 4-gramブロック未使用 → ngram_block_size を3に下げる")
+         
+         if total_rhetorical_exceptions > total_success // 2:
+             print(f"   - 修辞的例外が多すぎ → enable_rhetorical_protection=False を検討")
+     
+     # 設定最適化提案
+     print(f"\n⚙️ 次回推奨設定:")
+     optimized_config = dict(config)
+     if overall_success_rate < target_success_rate:
+         optimized_config['similarity_threshold'] = max(0.3, config['similarity_threshold'] - 0.05)
+         if avg_compression < config['min_compress_rate']:
+             optimized_config['min_compress_rate'] = max(0.03, config['min_compress_rate'] - 0.01)
+     
+     print(f"   --sim {optimized_config['similarity_threshold']:.2f} --max_dist {optimized_config['max_distance']} --ngram {optimized_config.get('ngram_block_size', 4)} --min_compress {optimized_config['min_compress_rate']:.2f} --drp-base {optimized_config['drp_base']:.2f} --drp-alpha {optimized_config['drp_alpha']:.1f}")
+     
+     return {
+         'overall_success_rate': overall_success_rate,
+         'target_achievement': overall_success_rate >= target_success_rate,
+         'total_tests': total_tests,
+         'total_success': total_success,
+         'average_compression_rate': avg_compression,
+         'average_processing_time_ms': avg_processing_time,
+         'total_processing_time_s': total_time,
+         'v3_feature_usage': {
+             'ngram_blocks': total_ngram_blocks,
+             'mecab_normalizations': total_mecab_normalizations,
+             'rhetorical_exceptions': total_rhetorical_exceptions,
+             'latin_blocks': total_latin_blocks
+         },
+         'category_stats': category_stats,
+         'failed_cases': failed_cases,
+         'optimized_config': optimized_config,
+         'config_used': config,
+         'detailed_results': results
+     }
+ 
+ 
+ def save_report(report: Dict, filename: str):
+     """レポートをJSONファイルに保存"""
+     try:
+         os.makedirs('logs/v3_tests', exist_ok=True)
+         filepath = f'logs/v3_tests/{filename}'
+         
+         with open(filepath, 'w', encoding='utf-8') as f:
+             json.dump(report, f, indent=2, ensure_ascii=False, default=str)
+         
+         print(f"\n💾 レポート保存: {filepath}")
+     except Exception as e:
+         print(f"\n❌ レポート保存失敗: {e}")
+ 
+ 
+ def main():
+     """メイン実行関数"""
+     args = parse_args()
+     
+     print("🔥 反復抑制システム v3 - 「ラスト21.7pt」強化版")
+     print("   58.3% → 80% 達成を目指します！")
+     print("=" * 60)
+     
+     # 設定構築
+     config = create_v3_config(args)
+     
+     print("⚙️ v3設定:")
+     print(f"   類似度閾値: {config['similarity_threshold']}")
+     print(f"   最大距離: {config['max_distance']}")
+     print(f"   最小圧縮率: {config['min_compress_rate']:.1%}")
+     print(f"   4-gramブロック: {config['enable_4gram_blocking']} (サイズ: {config.get('ngram_block_size', 0)})")
+     print(f"   DRP: base={config['drp_base']}, alpha={config['drp_alpha']}")
+     print(f"   MeCab: {config['enable_mecab_normalization']}")
+     print(f"   修辞的保護: {config['enable_rhetorical_protection']}")
+     
+     # テストケース準備
+     test_cases = create_v3_test_cases()
+     
+     # テスト実行
+     report = run_v3_quick_test(config, test_cases, args.target_rate)
+     
+     # レポート保存
+     if args.save_report:
+         save_report(report, args.save_report)
+     
+     # 最終判定
+     if report['target_achievement']:
+         print(f"\n🎉 目標達成！ 成功率 {report['overall_success_rate']:.1%} ≥ {args.target_rate:.1%}")
+         print("   80%ラインに到達しました！🔥")
+     else:
+         gap = args.target_rate - report['overall_success_rate']
+         print(f"\n📈 あと{gap:.1%}pt で目標達成です")
+         print("   次回推奨設定で再チャレンジしてください")
+     
+     return 0 if report['target_achievement'] else 1
+ 
+ 
+ if __name__ == "__main__":
+     exit_code = main()
+     sys.exit(exit_code) 
```

---

最後に、統合テストスクリプトを作成して、提案された改善案の効果を検証しましょう：

---

Cancelled

---



---

_**User**_

続けて

---

_**Assistant**_



---

了解です！v3システムの実装を完了させて、実際にテストを実行してみましょう。

まず、v3システムで不足しているメソッドを追加します：

---

Error calling tool 'edit_file'.

---



---

続いて、easy_novel_assistant.py にv3システムを統合しましょう：

---



---

では、v3システムの実際のテストを実行してみましょう：

---

```bash
py -3 demo_repetition_v3_quick.py --sim 0.50 --max_dist 50 --ngram 4 --min_compress 0.05 --drp-base 1.05 --drp-alpha 0.4 --verbose
```

```
PS C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main> py
-3 demo_repetition_v3_quick.
py --sim 0.50 --max_dist 50 --ngram 4 --min_compress 0.05 --drp-base 1.05 --drp-
alpha 0.4 --verbose         PS C:\Users\downl\Downloads\EasyNovelAssistant-main\
EasyNovelAssistant-main> py -3 demo_repetition_v3_quick.
py --sim 0.50 --max_dist 50 --ngram 4 --min_compress 0.05 --drp-base 1.05 --drp-
alpha 0.4 --verbose         























✅ v3システム読み込み成功
🔥 反復抑制システム v3 - 「ラスト21.7pt」強化版
   58.3% → 80% 達成を目指します！
============================================================
⚙️ v3設定:
   類似度閾値: 0.5
   最大距離: 50
   最小圧縮率: 5.0%
   4-gramブロック: True (サイズ: 4)
   DRP: base=1.05, alpha=0.4
   MeCab: False
   修辞的保護: True
🚀 反復抑制システム v3 クイックテスト開始
   目標成功率: 80.0%
   テストケース: 10件
============================================================
🚀 反復抑制システム v3 初期化完了
   ├─ 4-gramブロック: 有効
   ├─ MeCab正規化: 無効
   ├─ 修辞的保護: 有効
   ├─ 成功判定基準: 5.0%
   └─ 連番検知: 有効

📝 テスト 1/10: 基本同語反復
Traceback (most recent call last):
  File "C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main
\demo_repetition_v3_quick.py
", line 421, in <module>
                                exit_code = main()
                ^^^^^^
  File "C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main
\demo_repetition_v3_quick.py
", line 402, in main
                                report = run_v3_quick_test(config, test_cases, a
rgs.target_rate)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main
\demo_repetition_v3_quick.py
", line 215, in run_v3_quick_test
                                result_text, metrics = suppressor.suppress_repet
itions_with_debug_v3(
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main
\src\utils\repetition_suppre
ssor_v3.py", line 194, in suppress_repetitions_with_debug_v3
                                analysis = self.analyze_text_v3(text, character_
name, normalized_info)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main
\src\utils\repetition_suppre
ssor_v3.py", line 459, in analyze_text_v3
                                'phonetic_repetitions': self._detect_phonetic_re
petitions(text),
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\downl\Downloads\EasyNovelAssistant-main\EasyNovelAssistant-main
\src\utils\repetition_suppre
ssor_v3.py", line 800, in _detect_phonetic_repetitions
                                katakana_map = str.maketrans(
                   ^^^^^^^^^^^^^^
ValueError: the first two maketrans arguments must have equal length

```

---

maketransのエラーを修正しましょう：

---



---

もう一度テストを実行してみましょう：

---

```bash
py -3 demo_repetition_v3_quick.py --sim 0.50 --max_dist 50 --ngram 4 --min_compress 0.05 --drp-base 1.05 --drp-alpha 0.4 --verbose
```

```
py --sim 0.50 --max_dist 50 --ngram 4 --min_compress 0.05 --drp-base 1.05 --drp-alpha 0.4 --verbose
✅ v3システム読み込み成功
🔥 反復抑制システム v3 - 「ラスト21.7pt」強化版
   58.3% → 80% 達成を目指します！
============================================================
⚙️ v3設定:
   類似度閾値: 0.5
   最大距離: 50
   最小圧縮率: 5.0%
   4-gramブロック: True (サイズ: 4)
   DRP: base=1.05, alpha=0.4
   MeCab: False
   修辞的保護: True
🚀 反復抑制システム v3 クイックテスト開始
   目標成功率: 80.0%
   テストケース: 10件
============================================================
🚀 反復抑制システム v3 初期化完了
  File "C:\Users\downl\AppData\Local\Programs\Python\Python312\Lib\re\__init__.py", line 307, in _compile

    p = _compiler.compile(pattern, flags)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\downl\AppData\Local\Programs\Python\Python312\Lib\re\_parser.py", line 544, in _parse

    code = _escape(source, this, state)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\downl\AppData\Local\Programs\Python\Python312\Lib\re\_parser.py", line 440, in _escape

    raise source.error("invalid group reference %d" % group, len(escape) - 1)
re.error: invalid group reference 1 at position 9

```

---

正規表現のエラーがありますね。修正しましょう：

---



---

修辞的パターンに問題があるかもしれません。修正してみましょう：

---



---

エラーの原因を特定するために、問題のある部分をコメントアウトして、基本的なテストから始めましょう：

---



---

また、感嘆詞パターンも修正しましょう：

---



---

もう一度テストを実行してみましょう：

---



---



---

_**User**_



---

_**Assistant**_



---



---



---



---



---



---

