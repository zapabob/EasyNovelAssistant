# -*- coding: utf-8 -*-
"""
LoRA Ã— NKAT å”èª¿ã‚·ã‚¹ãƒ†ãƒ  v2.0
Î¸ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿæ§‹ã«ã‚ˆã‚‹æ–‡ä½“åˆ¶å¾¡ã®æ¥µã¿

NKATéå¯æ›ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®— + LoRAæ–‡ä½“é‡ã¿ â†’ å®Œå…¨æ–‡ä½“åˆ¶å¾¡
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Tuple, Optional, Callable
from dataclasses import dataclass
from tqdm import tqdm
import json
import time
import os
import re

# NKATçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from nkat.nkat_integration_manager import NKATIntegrationManager
    from nkat.advanced_tensor_processor import AdvancedTensorProcessor
    NKAT_AVAILABLE = True
except ImportError:
    print("WARNING: NKAT ã‚·ã‚¹ãƒ†ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œ")
    NKAT_AVAILABLE = False

# Quality Guardã®çµ±åˆ
import sys
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(current_dir, "..")
sys.path.insert(0, src_dir)

try:
    from utils.quality_guard import QualityGuard, QualityMetrics
    print("OK: Quality Guardçµ±åˆæˆåŠŸ")
except ImportError as e:
    print(f"WARNING: Quality Guardèª­ã¿è¾¼ã¿è­¦å‘Š: {e}")
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒ€ãƒŸãƒ¼ã‚¯ãƒ©ã‚¹
    class QualityGuard:
        def evaluate_quality(self, text, context=""):
            return type('QualityMetrics', (), {
                'grammar_score': 0.85,
                'sense_score': 0.80,
                'coherence_score': 0.82,
                'diversity_score': 0.75,
                'overall_score': 0.80
            })()


@dataclass
class StyleFeedbackConfig:
    """æ–‡ä½“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è¨­å®š"""
    theta_learning_rate: float = 0.001
    style_weight_sensitivity: float = 0.8
    feedback_momentum: float = 0.9
    bleurt_target: float = 0.87
    character_consistency_threshold: float = 0.95
    max_feedback_iterations: int = 100
    convergence_threshold: float = 1e-5


@dataclass
class StyleMetrics:
    """æ–‡ä½“å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    bleurt_score: float
    character_consistency: float
    style_coherence: float
    readability_score: float
    emotional_stability: float
    theta_convergence: float
    feedback_efficiency: float


class ThetaFeedbackMechanism:
    """Î¸ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿæ§‹ - æ–‡ä½“åˆ¶å¾¡ã®æ ¸å¿ƒ"""
    
    def __init__(self, config: StyleFeedbackConfig):
        self.config = config
        self.theta_history: List[torch.Tensor] = []
        self.style_weight_history: List[Dict[str, float]] = []
        self.metrics_history: List[StyleMetrics] = []
        
        # Î¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çŠ¶æ…‹
        self.current_theta = None
        self.target_theta = None
        self.theta_velocity = None
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ¶å¾¡
        self.feedback_controller = AdaptiveFeedbackController()
        
        print("ğŸ¯ Î¸ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿæ§‹åˆæœŸåŒ–å®Œäº†")
    
    def initialize_theta_space(self, model_dim: int, style_dim: int) -> torch.Tensor:
        """Î¸ ç©ºé–“ã®åˆæœŸåŒ–"""
        # éå¯æ›ãƒ†ãƒ³ã‚½ãƒ«ç©ºé–“ã§ã® Î¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        theta = torch.randn(model_dim, style_dim, dtype=torch.float32) * 0.01
        
        # éå¯æ›æ€§ã‚’ä¿è¨¼ã™ã‚‹åˆæœŸåŒ–
        theta = theta @ theta.T - theta.T @ theta  # åå¯¾ç§°åŒ–
        
        self.current_theta = theta.clone()
        self.theta_velocity = torch.zeros_like(theta)
        
        print(f"ğŸ“ Î¸ ç©ºé–“åˆæœŸåŒ–: {theta.shape} (éå¯æ›)")
        return theta
    
    def compute_style_gradient(self, 
                             style_weights: Dict[str, float],
                             current_metrics: StyleMetrics) -> torch.Tensor:
        """æ–‡ä½“é‡ã¿ã‹ã‚‰Î¸å‹¾é…ã‚’è¨ˆç®—"""
        
        # æ–‡ä½“é‡ã¿ â†’ ãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›
        weight_tensor = torch.tensor([
            style_weights.get('formality', 0.5),
            style_weights.get('emotion', 0.5), 
            style_weights.get('complexity', 0.5),
            style_weights.get('character_voice', 0.5)
        ], dtype=torch.float32)
        
        # ç›®æ¨™ã¨ã®å·®åˆ†
        target_bleurt = self.config.bleurt_target
        bleurt_error = current_metrics.bleurt_score - target_bleurt
        
        target_consistency = self.config.character_consistency_threshold
        consistency_error = current_metrics.character_consistency - target_consistency
        
        # ã‚¨ãƒ©ãƒ¼ä¿¡å·ã‚’Î¸ç©ºé–“ã«å°„å½±
        error_vector = torch.tensor([bleurt_error, consistency_error, 0.0, 0.0])
        
        # éå¯æ›æ¼”ç®—ã§å‹¾é…è¨ˆç®—
        if self.current_theta is not None:
            # Î¸ Ã— weight - weight Ã— Î¸ (éå¯æ›æ€§æ´»ç”¨)
            gradient = torch.outer(error_vector, weight_tensor)
            gradient = gradient @ self.current_theta - self.current_theta @ gradient
        else:
            gradient = torch.outer(error_vector, weight_tensor)
        
        return gradient * self.config.style_weight_sensitivity
    
    def update_theta(self, style_gradient: torch.Tensor) -> torch.Tensor:
        """Î¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°"""
        
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æ›´æ–°
        self.theta_velocity = (self.config.feedback_momentum * self.theta_velocity + 
                              self.config.theta_learning_rate * style_gradient)
        
        # Î¸ æ›´æ–°
        new_theta = self.current_theta + self.theta_velocity
        
        # éå¯æ›æ€§ä¿æŒ (åå¯¾ç§°æ€§ã®å¼·åˆ¶)
        new_theta = 0.5 * (new_theta - new_theta.T)
        
        # åæŸåˆ¤å®š
        theta_change = torch.norm(new_theta - self.current_theta)
        
        self.theta_history.append(self.current_theta.clone())
        self.current_theta = new_theta
        
        return theta_change
    
    def project_theta_to_lora(self, theta: torch.Tensor) -> Dict[str, float]:
        """Î¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’LoRAé‡ã¿ã«å°„å½±"""
        
        # Î¸ ã®ç‰¹ç•°å€¤åˆ†è§£
        U, S, V = torch.svd(theta)
        
        # ä¸»æˆåˆ†ã‚’æ–‡ä½“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¤‰æ›
        style_params = {
            'formality': float(S[0] * U[0, 0]) if len(S) > 0 else 0.5,
            'emotion': float(S[1] * U[1, 0]) if len(S) > 1 else 0.5,
            'complexity': float(S[2] * U[2, 0]) if len(S) > 2 else 0.5,
            'character_voice': float(S[3] * U[3, 0]) if len(S) > 3 else 0.5
        }
        
        # æ­£è¦åŒ– [0, 1]
        for key in style_params:
            style_params[key] = max(0.0, min(1.0, style_params[key] + 0.5))
        
        return style_params


class AdaptiveFeedbackController:
    """é©å¿œçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ¶å¾¡å™¨"""
    
    def __init__(self):
        self.error_integral = 0.0
        self.last_error = 0.0
        self.kp = 1.0  # æ¯”ä¾‹ã‚²ã‚¤ãƒ³
        self.ki = 0.1  # ç©åˆ†ã‚²ã‚¤ãƒ³
        self.kd = 0.05  # å¾®åˆ†ã‚²ã‚¤ãƒ³
    
    def compute_control_signal(self, current_error: float, dt: float = 1.0) -> float:
        """PIDåˆ¶å¾¡ä¿¡å·ã®è¨ˆç®—"""
        
        # ç©åˆ†é …
        self.error_integral += current_error * dt
        
        # å¾®åˆ†é …
        error_derivative = (current_error - self.last_error) / dt
        
        # PIDåˆ¶å¾¡ä¿¡å·
        control_signal = (self.kp * current_error + 
                         self.ki * self.error_integral + 
                         self.kd * error_derivative)
        
        self.last_error = current_error
        
        return control_signal


class LoRANKATCoordinator:
    """LoRA Ã— NKAT å”èª¿ã‚·ã‚¹ãƒ†ãƒ  - æ–‡ä½“åˆ¶å¾¡ã®æ¥µã¿"""
    
    def __init__(self, config: StyleFeedbackConfig):
        self.config = config
        self.theta_feedback = ThetaFeedbackMechanism(config)
        
        # NKATçµ±åˆï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if NKAT_AVAILABLE:
            self.nkat_manager = NKATIntegrationManager()
            self.tensor_processor = AdvancedTensorProcessor()
        else:
            self.nkat_manager = None
            self.tensor_processor = None
        
        # LoRAé‡ã¿ç®¡ç†
        self.current_lora_weights = {}
        self.optimal_weights_cache = {}
        
        # Quality Guardã®çµ±åˆ
        self.quality_guard = QualityGuard()
        
        print("OK: LoRA Ã— NKAT å”èª¿ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def initialize_style_space(self, model_dim: int = 768) -> Dict[str, float]:
        """æ–‡ä½“åˆ¶å¾¡ç©ºé–“ã®åˆæœŸåŒ–"""
        
        # Î¸ ç©ºé–“åˆæœŸåŒ–
        style_dim = 4  # formality, emotion, complexity, character_voice
        theta = self.theta_feedback.initialize_theta_space(model_dim, style_dim)
        
        # åˆæœŸLoRAé‡ã¿
        initial_weights = {
            'formality': 0.5,
            'emotion': 0.5,
            'complexity': 0.5,
            'character_voice': 0.5
        }
        
        self.current_lora_weights = initial_weights.copy()
        
        print(f"åˆæœŸåŒ–: æ–‡ä½“åˆ¶å¾¡ç©ºé–“ theta{theta.shape}, LoRA{len(initial_weights)}æ¬¡å…ƒ")
        return initial_weights
    
    def compute_style_metrics(self, text: str, character: str = "default") -> StyleMetrics:
        """å®Ÿéš›ã®æ–‡ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®— - Quality Guardçµ±åˆç‰ˆ"""
        
        # Quality Guardã«ã‚ˆã‚‹åŸºæœ¬å“è³ªè©•ä¾¡
        quality_metrics = self.quality_guard.evaluate_quality(text, context=character)
        
        # BLEURTä»£æ›¿ã‚¹ã‚³ã‚¢ï¼ˆGrammar + Sense + Diversityçµ±åˆï¼‰
        bleurt_alternative = (
            quality_metrics.grammar_score * 0.4 +
            quality_metrics.sense_score * 0.35 +
            quality_metrics.diversity_score * 0.25
        )
        
        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ä¸€è²«æ€§ã®è¨ˆç®—
        character_consistency = self._compute_character_consistency(text, character, quality_metrics)
        
        # æ–‡ä½“çµæŸæ€§ã®è¨ˆç®—
        style_coherence = self._compute_style_coherence(text, quality_metrics)
        
        # å¯èª­æ€§ã‚¹ã‚³ã‚¢ï¼ˆQuality Guardã®ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ + ç‹¬è‡ªè¨ˆç®—ï¼‰
        readability_score = self._compute_readability_score(text, quality_metrics)
        
        # æ„Ÿæƒ…å®‰å®šæ€§ã®è¨ˆç®—
        emotional_stability = self._compute_emotional_stability(text)
        
        # Î¸åæŸåº¦ã®è¨ˆç®—
        theta_convergence = self._compute_theta_convergence()
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åŠ¹ç‡ã®è¨ˆç®—
        feedback_efficiency = self._compute_feedback_efficiency()
        
        return StyleMetrics(
            bleurt_score=bleurt_alternative,
            character_consistency=character_consistency,
            style_coherence=style_coherence,
            readability_score=readability_score,
            emotional_stability=emotional_stability,
            theta_convergence=theta_convergence,
            feedback_efficiency=feedback_efficiency
        )
    
    def _compute_character_consistency(self, text: str, character: str, quality_metrics) -> float:
        """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ–‡ä½“ä¸€è²«æ€§ã®è¨ˆç®—"""
        
        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç‰¹æ€§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        character_patterns = {
            'default': {'formality': 0.5, 'emotion': 0.5, 'complexity': 0.5},
            'èŠ±å­': {'formality': 0.8, 'emotion': 0.6, 'complexity': 0.7},
            'å¤ªéƒ': {'formality': 0.3, 'emotion': 0.4, 'complexity': 0.5},
            'å…ˆç”Ÿ': {'formality': 0.9, 'emotion': 0.3, 'complexity': 0.8},
            'å­ä¾›': {'formality': 0.2, 'emotion': 0.8, 'complexity': 0.3}
        }
        
        target_profile = character_patterns.get(character, character_patterns['default'])
        
        # ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã«ã‚ˆã‚‹å®Ÿéš›ã®ç‰¹æ€§æŠ½å‡º
        actual_formality = self._extract_formality_level(text)
        actual_emotion = self._extract_emotion_level(text)
        actual_complexity = self._extract_complexity_level(text)
        
        # æœŸå¾…å€¤ã¨ã®å·®åˆ†è¨ˆç®—
        formality_match = 1.0 - abs(target_profile['formality'] - actual_formality)
        emotion_match = 1.0 - abs(target_profile['emotion'] - actual_emotion)
        complexity_match = 1.0 - abs(target_profile['complexity'] - actual_complexity)
        
        # æ–‡ä½“ä¸€è²«æ€§ã‚¹ã‚³ã‚¢
        consistency_score = (formality_match * 0.4 + 
                           emotion_match * 0.3 + 
                           complexity_match * 0.3)
        
        # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ã‚ˆã‚‹èª¿æ•´
        quality_adjustment = quality_metrics.coherence_score * 0.2
        
        return max(0.0, min(1.0, consistency_score + quality_adjustment))
    
    def _extract_formality_level(self, text: str) -> float:
        """æ–‡ä½“ã®ä¸å¯§åº¦ãƒ¬ãƒ™ãƒ«æŠ½å‡º"""
        formal_patterns = ['ã§ã™', 'ã¾ã™', 'ã”ã–ã„ã¾ã™', 'ã§ã—ã‚‡ã†', 'ã„ã‚‰ã£ã—ã‚ƒã„']
        informal_patterns = ['ã ', 'ã§ã‚ã‚‹', 'ã˜ã‚ƒã‚“', 'ã£ã—ã‚‡', 'ã¡ã‚ƒã†']
        
        formal_count = sum(text.count(pattern) for pattern in formal_patterns)
        informal_count = sum(text.count(pattern) for pattern in informal_patterns)
        
        total_patterns = formal_count + informal_count
        if total_patterns == 0:
            return 0.5  # ä¸­æ€§
        
        return formal_count / total_patterns
    
    def _extract_emotion_level(self, text: str) -> float:
        """æ„Ÿæƒ…ãƒ¬ãƒ™ãƒ«æŠ½å‡º"""
        emotional_patterns = ['ï¼', 'ï¼Ÿ', 'ã‚ã‚', 'ãã‚ƒãƒ¼', 'ã†ã‚ãƒ¼', 'ãˆãƒ¼', 'ã‚ãƒ¼']
        emotional_count = sum(text.count(pattern) for pattern in emotional_patterns)
        
        # æ–‡å­—æ•°ã«å¯¾ã™ã‚‹æ„Ÿæƒ…è¡¨ç¾ã®å¯†åº¦
        emotion_density = emotional_count / max(len(text), 1) * 100
        
        return min(1.0, emotion_density * 0.1)
    
    def _extract_complexity_level(self, text: str) -> float:
        """æ–‡ç« è¤‡é›‘åº¦ãƒ¬ãƒ™ãƒ«æŠ½å‡º"""
        # èªå½™ã®è¤‡é›‘æ€§ï¼ˆã²ã‚‰ãŒãªã€æ¼¢å­—ã€ã‚«ã‚¿ã‚«ãƒŠã®æ¯”ç‡ï¼‰
        hiragana_count = len(re.findall(r'[ã²-ã‚“]', text))
        kanji_count = len(re.findall(r'[ä¸€-é¾¯]', text))
        katakana_count = len(re.findall(r'[ã‚¢-ãƒ³]', text))
        
        total_chars = len(text)
        if total_chars == 0:
            return 0.5
        
        # æ¼¢å­—æ¯”ç‡ãŒé«˜ã„ã»ã©è¤‡é›‘
        complexity = (kanji_count * 0.6 + katakana_count * 0.3 + hiragana_count * 0.1) / total_chars
        
        return min(1.0, complexity * 2.0)  # æ­£è¦åŒ–
    
    def _compute_style_coherence(self, text: str, quality_metrics) -> float:
        """æ–‡ä½“çµæŸæ€§ã®è¨ˆç®—"""
        # åŸºæœ¬çš„ãªçµæŸæ€§ã¯Quality Guardã‹ã‚‰
        base_coherence = quality_metrics.coherence_score
        
        # æ–‡ã®é•·ã•ã®ä¸€è²«æ€§
        sentences = [s.strip() for s in text.split('ã€‚') if s.strip()]
        if len(sentences) < 2:
            length_consistency = 1.0
        else:
            lengths = [len(s) for s in sentences]
            avg_length = sum(lengths) / len(lengths)
            variance = sum((l - avg_length) ** 2 for l in lengths) / len(lengths)
            length_consistency = 1.0 / (1.0 + variance / 100.0)
        
        # èªå½™ä¸€è²«æ€§
        words = text.split()
        unique_words = set(words)
        vocab_consistency = len(unique_words) / max(len(words), 1)
        
        # ç·åˆçµæŸæ€§
        coherence = (base_coherence * 0.5 + 
                    length_consistency * 0.3 + 
                    vocab_consistency * 0.2)
        
        return max(0.0, min(1.0, coherence))
    
    def _compute_readability_score(self, text: str, quality_metrics) -> float:
        """å¯èª­æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        # åŸºæœ¬ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹
        base_score = quality_metrics.coherence_score * 0.6
        
        # æ–‡ã®é•·ã•é©æ­£æ€§
        sentences = [s.strip() for s in text.split('ã€‚') if s.strip()]
        if sentences:
            avg_sentence_length = sum(len(s) for s in sentences) / len(sentences)
            # ç†æƒ³çš„ãªæ–‡é•·ã¯20-40æ–‡å­—
            length_score = 1.0 - abs(avg_sentence_length - 30) / 30.0
            length_score = max(0.0, min(1.0, length_score))
        else:
            length_score = 0.5
        
        # èªå½™é›£æ˜“åº¦ï¼ˆã²ã‚‰ãŒãªæ¯”ç‡ã§ç°¡æ˜“è¨ˆç®—ï¼‰
        hiragana_chars = sum(1 for c in text if '\u3040' <= c <= '\u309F')
        total_chars = len([c for c in text if c.strip()])
        hiragana_ratio = hiragana_chars / max(total_chars, 1)
        # é©åº¦ãªã²ã‚‰ãŒãªæ¯”ç‡ï¼ˆ0.3-0.6ï¼‰ãŒç†æƒ³çš„
        vocab_score = 1.0 - abs(hiragana_ratio - 0.45) / 0.45
        vocab_score = max(0.0, min(1.0, vocab_score))
        
        return base_score + length_score * 0.25 + vocab_score * 0.15
        
    def _compute_emotional_stability(self, text: str) -> float:
        """æ„Ÿæƒ…å®‰å®šæ€§ã®è¨ˆç®—"""
        # æ„Ÿå˜†ç¬¦ã‚„ç–‘å•ç¬¦ã®ä½¿ç”¨é »åº¦
        exclamation_count = text.count('ï¼') + text.count('!') + text.count('ï¼Ÿ') + text.count('?')
        text_length = len(text)
        punctuation_density = exclamation_count / max(text_length, 1) * 100
        
        # é©åº¦ãªæ„Ÿæƒ…è¡¨ç¾ï¼ˆ2-8%ï¼‰ãŒå®‰å®šçš„
        if punctuation_density <= 2:
            emotion_score = 0.6 + punctuation_density / 2 * 0.2  # 0.6-0.8
        elif punctuation_density <= 8:
            emotion_score = 0.8 + (8 - punctuation_density) / 6 * 0.2  # 0.8-1.0
        else:
            emotion_score = max(0.2, 1.0 - (punctuation_density - 8) / 20)  # æ„Ÿæƒ…éå¤šã§æ¸›ç‚¹
        
        # èªèª¿ã®ä¸€è²«æ€§
        formal_indicators = ['ã§ã™', 'ã¾ã™', 'ã§ã‚ã‚‹', 'ã§ã‚ã‚Š']
        informal_indicators = ['ã ã‚ˆ', 'ã ã­', 'ã˜ã‚ƒã‚“', 'ã£ã½ã„', 'ã‚„ã‚“', 'ã‚„ã§']
        
        formal_count = sum(text.count(indicator) for indicator in formal_indicators)
        informal_count = sum(text.count(indicator) for indicator in informal_indicators)
        
        if formal_count + informal_count == 0:
            tone_consistency = 0.7  # ä¸­æ€§
        else:
            tone_ratio = abs(formal_count - informal_count) / (formal_count + informal_count)
            tone_consistency = 0.5 + tone_ratio * 0.5  # çµ±ä¸€æ€§ãŒé«˜ã„ã»ã©å®‰å®š
        
        return (emotion_score * 0.6 + tone_consistency * 0.4)
    
    def _compute_theta_convergence(self) -> float:
        """Î¸åæŸåº¦ã®è¨ˆç®—"""
        if not hasattr(self, 'theta_feedback') or not self.theta_feedback.theta_history:
            return 0.0
        
        history = self.theta_feedback.theta_history
        if len(history) < 2:
            return 0.0
        
        # æœ€è¿‘ã®å¤‰åŒ–é‡ã‚’è¨ˆç®—
        recent_changes = []
        for i in range(1, min(len(history), 6)):  # æœ€è¿‘5å›ã®å¤‰åŒ–
            if history[-i-1] is not None and history[-i] is not None:
                change = float(torch.norm(history[-i] - history[-i-1]))
                recent_changes.append(change)
        
        if not recent_changes:
            return 0.0
        
        # å¤‰åŒ–é‡ã®å¹³å‡ãŒåæŸé–¾å€¤ä»¥ä¸‹ãªã‚‰é«˜åæŸåº¦
        avg_change = sum(recent_changes) / len(recent_changes)
        convergence_threshold = self.config.convergence_threshold
        
        if avg_change <= convergence_threshold:
            return 1.0
        elif avg_change <= convergence_threshold * 10:
            return 1.0 - (avg_change - convergence_threshold) / (convergence_threshold * 9)
        else:
            return 0.1  # ç™ºæ•£çŠ¶æ…‹
    
    def _compute_feedback_efficiency(self) -> float:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åŠ¹ç‡ã®è¨ˆç®—"""
        if not hasattr(self, 'theta_feedback'):
            return 0.0
        
        total_iterations = len(self.theta_feedback.theta_history)
        max_iterations = self.config.max_feedback_iterations
        
        if total_iterations == 0:
            return 0.0
        
        # æ—©æœŸåæŸã»ã©åŠ¹ç‡ãŒè‰¯ã„
        efficiency_ratio = 1.0 - (total_iterations / max_iterations)
        
        # åæŸåº¦ã‚‚è€ƒæ…®
        convergence_bonus = self._compute_theta_convergence() * 0.3
        
        return min(1.0, max(0.0, efficiency_ratio + convergence_bonus))
    
    def optimize_style_coordination(self, 
                                  target_text: str,
                                  character_profile: Dict[str, any],
                                  max_iterations: int = None) -> Dict[str, float]:
        """æ–‡ä½“å”èª¿ã®æœ€é©åŒ– - ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ """
        
        max_iter = max_iterations or self.config.max_feedback_iterations
        
        print(f"ğŸ¯ æ–‡ä½“å”èª¿æœ€é©åŒ–é–‹å§‹ (æœ€å¤§{max_iter}å›)")
        
        best_metrics = None
        best_weights = self.current_lora_weights.copy()
        iteration_data = []
        
        with tqdm(range(max_iter), desc="Î¸ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æœ€é©åŒ–") as pbar:
            for iteration in pbar:
                
                # ç¾åœ¨ã®æ–‡ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
                current_metrics = self.compute_style_metrics(target_text, 
                                                           character_profile.get('name', 'default'))
                
                # Î¸ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿæ§‹
                style_gradient = self.theta_feedback.compute_style_gradient(
                    self.current_lora_weights, current_metrics)
                
                theta_change = self.theta_feedback.update_theta(style_gradient)
                
                # Î¸ â†’ LoRA å°„å½±
                new_lora_weights = self.theta_feedback.project_theta_to_lora(
                    self.theta_feedback.current_theta)
                
                # NKATéå¯æ›æ¼”ç®—ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
                if self.nkat_manager:
                    enhanced_weights = self._apply_nkat_enhancement(new_lora_weights)
                    new_lora_weights.update(enhanced_weights)
                
                self.current_lora_weights = new_lora_weights
                
                # æœ€è‰¯è§£æ›´æ–°
                if (best_metrics is None or 
                    current_metrics.bleurt_score > best_metrics.bleurt_score):
                    best_metrics = current_metrics
                    best_weights = new_lora_weights.copy()
                
                # é€²æ—è¡¨ç¤º
                pbar.set_postfix({
                    'BLEURT': f"{current_metrics.bleurt_score:.3f}",
                    'Consist': f"{current_metrics.character_consistency:.3f}",
                    'Î¸_change': f"{theta_change:.6f}"
                })
                
                # åæŸåˆ¤å®š
                if theta_change < self.config.convergence_threshold:
                    print(f"âœ… åæŸé”æˆ (iteration {iteration})")
                    break
                
                # ãƒ‡ãƒ¼ã‚¿è¨˜éŒ²
                iteration_data.append({
                    'iteration': iteration,
                    'metrics': current_metrics,
                    'weights': new_lora_weights.copy(),
                    'theta_change': float(theta_change)
                })
        
        # æœ€é©åŒ–çµæœ
        final_improvement = {
            'bleurt_improvement': best_metrics.bleurt_score - 0.82,
            'consistency_improvement': best_metrics.character_consistency - 0.88,
            'theta_convergence': best_metrics.theta_convergence,
            'total_iterations': len(iteration_data)
        }
        
        print(f"\nğŸ‰ æ–‡ä½“å”èª¿æœ€é©åŒ–å®Œäº†ï¼")
        print(f"   BLEURTæ”¹å–„: +{final_improvement['bleurt_improvement']:.3f}")
        print(f"   ä¸€è²«æ€§æ”¹å–„: +{final_improvement['consistency_improvement']:.3f}")
        print(f"   Î¸åæŸåº¦: {final_improvement['theta_convergence']:.3f}")
        
        # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        cache_key = character_profile.get('name', 'default')
        self.optimal_weights_cache[cache_key] = best_weights
        
        return best_weights
    
    def _apply_nkat_enhancement(self, lora_weights: Dict[str, float]) -> Dict[str, float]:
        """NKATéå¯æ›æ¼”ç®—ã«ã‚ˆã‚‹é‡ã¿å¼·åŒ–"""
        
        if not self.nkat_manager:
            return {}
        
        # LoRAé‡ã¿ â†’ ãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›
        weight_tensor = torch.tensor(list(lora_weights.values()))
        
        try:
            # NKATéå¯æ›æ¼”ç®—
            enhanced_tensor = self.tensor_processor.apply_noncommutative_ops(
                weight_tensor.unsqueeze(0), weight_tensor.unsqueeze(0)
            )[0]
            
            # å¼·åŒ–ã•ã‚ŒãŸé‡ã¿
            enhanced_values = enhanced_tensor.squeeze().tolist()
            enhanced_weights = {
                'nkat_formality_boost': enhanced_values[0] if len(enhanced_values) > 0 else 0.0,
                'nkat_emotion_boost': enhanced_values[1] if len(enhanced_values) > 1 else 0.0,
                'nkat_complexity_boost': enhanced_values[2] if len(enhanced_values) > 2 else 0.0,
                'nkat_character_boost': enhanced_values[3] if len(enhanced_values) > 3 else 0.0
            }
            
            return enhanced_weights
            
        except Exception as e:
            print(f"âš ï¸ NKATå¼·åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def get_optimization_report(self) -> Dict[str, any]:
        """æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        
        report = {
            'theta_feedback_stats': {
                'total_updates': len(self.theta_feedback.theta_history),
                'current_theta_norm': float(torch.norm(self.theta_feedback.current_theta)) if self.theta_feedback.current_theta is not None else 0.0,
                'convergence_achieved': len(self.theta_feedback.theta_history) < self.config.max_feedback_iterations
            },
            'lora_coordination_stats': {
                'current_weights': self.current_lora_weights.copy(),
                'cached_optima': len(self.optimal_weights_cache),
                'nkat_enhanced': self.nkat_manager is not None
            },
            'style_quality_summary': {
                'target_bleurt': self.config.bleurt_target,
                'target_consistency': self.config.character_consistency_threshold,
                'optimization_efficiency': 'High' if self.nkat_manager else 'Standard'
            }
        }
        
        return report


def create_demo_character_profile() -> Dict[str, any]:
    """ãƒ‡ãƒ¢ç”¨ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«"""
    return {
        'name': 'èŠ±å­',
        'personality': 'å„ªã—ãã¦çŸ¥çš„',
        'speech_style': 'ä¸å¯§èª',
        'emotional_range': 'medium',
        'complexity_preference': 'high',
        'formality_level': 0.8
    }


def main_coordination_demo():
    """ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¢: LoRA Ã— NKAT å”èª¿ã‚·ã‚¹ãƒ†ãƒ """
    
    print("ğŸš€ LoRA Ã— NKAT å”èª¿ã‚·ã‚¹ãƒ†ãƒ  - Î¸ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿæ§‹ãƒ‡ãƒ¢")
    print("=" * 60)
    
    # è¨­å®š
    config = StyleFeedbackConfig(
        theta_learning_rate=0.002,
        style_weight_sensitivity=0.85,
        bleurt_target=0.90,
        character_consistency_threshold=0.95,
        max_feedback_iterations=50
    )
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    coordinator = LoRANKATCoordinator(config)
    
    # æ–‡ä½“åˆ¶å¾¡ç©ºé–“åˆæœŸåŒ–
    initial_weights = coordinator.initialize_style_space(model_dim=512)
    print(f"ğŸ“Š åˆæœŸLoRAé‡ã¿: {initial_weights}")
    
    # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
    character = create_demo_character_profile()
    print(f"ğŸ‘¤ å¯¾è±¡ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼: {character['name']} ({character['personality']})")
    
    # å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
    target_text = "ãŠå…„ã¡ã‚ƒã‚“ã€ä»Šæ—¥ã¯ã¨ã¦ã‚‚è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚æ•£æ­©ã§ã‚‚ã„ã‹ãŒã§ã™ã‹ï¼Ÿ"
    
    print(f"\nğŸ“ å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ: ã€Œ{target_text}ã€")
    print(f"ğŸ¯ ç›®æ¨™: BLEURT {config.bleurt_target:.1%}, ä¸€è²«æ€§ {config.character_consistency_threshold:.1%}")
    
    # æ–‡ä½“å”èª¿æœ€é©åŒ–å®Ÿè¡Œ
    start_time = time.time()
    
    optimal_weights = coordinator.optimize_style_coordination(
        target_text=target_text,
        character_profile=character,
        max_iterations=30  # ãƒ‡ãƒ¢ç”¨ã«çŸ­ç¸®
    )
    
    optimization_time = time.time() - start_time
    
    # çµæœè¡¨ç¤º
    print(f"\nğŸ‰ æœ€é©åŒ–å®Œäº† (å‡¦ç†æ™‚é–“: {optimization_time:.2f}ç§’)")
    print(f"ğŸ“Š æœ€é©LoRAé‡ã¿:")
    for key, value in optimal_weights.items():
        print(f"   {key}: {value:.4f}")
    
    # æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ
    report = coordinator.get_optimization_report()
    print(f"\nğŸ“ˆ æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ:")
    print(f"   Î¸æ›´æ–°å›æ•°: {report['theta_feedback_stats']['total_updates']}")
    print(f"   Î¸ãƒãƒ«ãƒ : {report['theta_feedback_stats']['current_theta_norm']:.4f}")
    print(f"   åæŸé”æˆ: {report['theta_feedback_stats']['convergence_achieved']}")
    print(f"   NKATå¼·åŒ–: {report['lora_coordination_stats']['nkat_enhanced']}")
    
    print(f"\nğŸ† æ–‡ä½“åˆ¶å¾¡ã®æ¥µã¿é”æˆï¼Î¸ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿæ§‹ã«ã‚ˆã‚‹å®Œå…¨å”èª¿ãŒå®Œæˆã—ã¾ã—ãŸï¼")


if __name__ == "__main__":
    main_coordination_demo() 