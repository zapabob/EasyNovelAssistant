# -*- coding: utf-8 -*-
"""
ã‚¯ãƒ­ã‚¹æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  + ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ•ã‚¡ã‚¨ãƒ³ã‚¸ãƒ³ v1.0

ã€æ¦‚è¦ã€‘
è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–“ã§åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å…±æœ‰ã—ã€
æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸåå¾©æŠ‘åˆ¶ã‚’è¡Œã†ã‚¯ãƒ­ã‚¹æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ ã€‚

ã€ä¸»è¦æ©Ÿèƒ½ã€‘
1. ã‚»ãƒƒã‚·ãƒ§ãƒ³æ¨ªæ–­ã®åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³è¨˜æ†¶
2. ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åˆ¥åå¾©å±¥æ­´ç®¡ç†
3. æ–‡è„ˆè€ƒæ…®å‹æŠ‘åˆ¶é‡ã¿èª¿æ•´
4. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ãƒ»é©å¿œ

Author: EasyNovelAssistant Development Team
Date: 2025-01-06
"""

import os
import sys
import json
import time
import threading
from typing import Dict, List, Optional, Tuple, Set, Any
from collections import deque, defaultdict
from dataclasses import dataclass, asdict
import numpy as np
import logging

# ãƒ‘ã‚¹è¨­å®š
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
src_dir = os.path.join(project_root, "src")
sys.path.insert(0, project_root)
sys.path.insert(0, src_dir)

try:
    from utils.repetition_suppressor_v3 import AdvancedRepetitionSuppressorV3
    from integration.lora_style_coordinator import LoRAStyleCoordinator
    V3_AVAILABLE = True
except ImportError:
    V3_AVAILABLE = False
    logging.warning("v3ã‚·ã‚¹ãƒ†ãƒ ã¾ãŸã¯LoRAå”èª¿ã‚·ã‚¹ãƒ†ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")


@dataclass
class CrossSuppressionPattern:
    """ã‚¯ãƒ­ã‚¹æŠ‘åˆ¶ãƒ‘ã‚¿ãƒ¼ãƒ³"""
    pattern: str
    frequency: int
    last_seen: float
    contexts: List[str]
    suppression_weight: float
    character_sources: Set[str]
    session_ids: Set[str]


@dataclass
class MemoryContext:
    """ãƒ¡ãƒ¢ãƒªæ–‡è„ˆæƒ…å ±"""
    session_id: str
    timestamp: float
    character: str
    text_snippet: str
    patterns_found: List[str]
    suppression_applied: List[str]


class SessionMemoryBuffer:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ•ã‚¡"""
    
    def __init__(self, max_size: int = 1000, context_window: int = 10):
        self.max_size = max_size
        self.context_window = context_window
        self.contexts = deque(maxlen=max_size)
        self.pattern_index = defaultdict(list)  # ãƒ‘ã‚¿ãƒ¼ãƒ³ -> ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        self.character_index = defaultdict(list)  # ã‚­ãƒ£ãƒ© -> ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        self.session_index = defaultdict(list)  # ã‚»ãƒƒã‚·ãƒ§ãƒ³ -> ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        self.lock = threading.RLock()
    
    def add_context(self, context: MemoryContext):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è¿½åŠ """
        with self.lock:
            context_idx = len(self.contexts)
            self.contexts.append(context)
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°
            for pattern in context.patterns_found:
                self.pattern_index[pattern].append(context_idx)
            
            self.character_index[context.character].append(context_idx)
            self.session_index[context.session_id].append(context_idx)
            
            # å¤ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if len(self.contexts) > self.max_size * 0.9:
                self._cleanup_old_indices()
    
    def _cleanup_old_indices(self):
        """å¤ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        cutoff = len(self.contexts) - self.max_size
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        for pattern in list(self.pattern_index.keys()):
            self.pattern_index[pattern] = [
                idx for idx in self.pattern_index[pattern] if idx >= cutoff
            ]
            if not self.pattern_index[pattern]:
                del self.pattern_index[pattern]
        
        # åŒæ§˜ã«ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        for char in list(self.character_index.keys()):
            self.character_index[char] = [
                idx for idx in self.character_index[char] if idx >= cutoff
            ]
            if not self.character_index[char]:
                del self.character_index[char]
        
        for session in list(self.session_index.keys()):
            self.session_index[session] = [
                idx for idx in self.session_index[session] if idx >= cutoff
            ]
            if not self.session_index[session]:
                del self.session_index[session]
    
    def get_relevant_contexts(self, pattern: str = None, character: str = None, 
                            session_id: str = None, limit: int = None) -> List[MemoryContext]:
        """é–¢é€£ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å–å¾—"""
        with self.lock:
            indices = set()
            
            if pattern:
                indices.update(self.pattern_index.get(pattern, []))
            if character:
                indices.update(self.character_index.get(character, []))
            if session_id:
                indices.update(self.session_index.get(session_id, []))
            
            if not indices:
                # æœ€æ–°ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
                indices = range(max(0, len(self.contexts) - self.context_window), len(self.contexts))
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ™‚é–“é †ã§ã‚½ãƒ¼ãƒˆ
            sorted_indices = sorted(indices, reverse=True)
            if limit:
                sorted_indices = sorted_indices[:limit]
            
            return [self.contexts[idx] for idx in sorted_indices if idx < len(self.contexts)]


class CrossSuppressionEngine:
    """ã‚¯ãƒ­ã‚¹æŠ‘åˆ¶ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.config_path = config_path or "config/cross_suppression.json"
        self.logger = logging.getLogger(__name__)
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.memory_buffer = SessionMemoryBuffer()
        self.repetition_suppressor: Optional[AdvancedRepetitionSuppressorV3] = None
        self.lora_coordinator: Optional[LoRAStyleCoordinator] = None
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ç®¡ç†
        self.cross_patterns: Dict[str, CrossSuppressionPattern] = {}
        self.pattern_lock = threading.RLock()
        
        # è¨­å®š
        self.config = {
            'cross_suppression_threshold': 0.3,
            'pattern_decay_rate': 0.95,  # æ™‚é–“çµŒéã«ã‚ˆã‚‹é‡ã¿æ¸›è¡°
            'min_pattern_frequency': 2,  # æœ€å°å‡ºç¾å›æ•°
            'context_influence_weight': 0.4,  # æ–‡è„ˆå½±éŸ¿ã®é‡ã¿
            'character_isolation': True,  # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åˆ¥åˆ†é›¢
            'session_memory_hours': 24,  # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ¡ãƒ¢ãƒªä¿æŒæ™‚é–“
            'adaptive_learning': True,   # é©å¿œå­¦ç¿’
            'realtime_updates': True     # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°
        }
        
        # çµ±è¨ˆ
        self.stats = {
            'patterns_learned': 0,
            'cross_suppressions_applied': 0,
            'sessions_tracked': 0,
            'contexts_remembered': 0
        }
        
        self._load_configuration()
        self.logger.info("ğŸ§  ã‚¯ãƒ­ã‚¹æŠ‘åˆ¶ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
    
    def _load_configuration(self):
        """è¨­å®šã®èª­ã¿è¾¼ã¿"""
        try:
            if os.path.exists(self.config_path):
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    loaded_config = json.load(f)
                
                self.config.update(loaded_config.get('engine_config', {}))
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¾©å…ƒ
                patterns_data = loaded_config.get('cross_patterns', {})
                for pattern_str, data in patterns_data.items():
                    pattern = CrossSuppressionPattern(
                        pattern=data['pattern'],
                        frequency=data['frequency'],
                        last_seen=data['last_seen'],
                        contexts=data['contexts'],
                        suppression_weight=data['suppression_weight'],
                        character_sources=set(data['character_sources']),
                        session_ids=set(data['session_ids'])
                    )
                    self.cross_patterns[pattern_str] = pattern
                
                self.logger.info(f"ğŸ“– è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†: {len(self.cross_patterns)}ãƒ‘ã‚¿ãƒ¼ãƒ³")
        except Exception as e:
            self.logger.warning(f"è¨­å®šèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    
    def _save_configuration(self):
        """è¨­å®šã®ä¿å­˜"""
        try:
            os.makedirs(os.path.dirname(self.config_path), exist_ok=True)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¾æ›¸åŒ–
            patterns_data = {}
            for pattern_str, pattern in self.cross_patterns.items():
                patterns_data[pattern_str] = {
                    'pattern': pattern.pattern,
                    'frequency': pattern.frequency,
                    'last_seen': pattern.last_seen,
                    'contexts': pattern.contexts,
                    'suppression_weight': pattern.suppression_weight,
                    'character_sources': list(pattern.character_sources),
                    'session_ids': list(pattern.session_ids)
                }
            
            config_data = {
                'engine_config': self.config,
                'cross_patterns': patterns_data,
                'stats': self.stats
            }
            
            with open(self.config_path, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, indent=2, ensure_ascii=False, default=str)
            
            self.logger.info("ğŸ’¾ ã‚¯ãƒ­ã‚¹æŠ‘åˆ¶è¨­å®šä¿å­˜å®Œäº†")
        except Exception as e:
            self.logger.error(f"è¨­å®šä¿å­˜å¤±æ•—: {e}")
    
    def initialize_systems(self, repetition_config: Optional[Dict] = None,
                          lora_config: Optional[Dict] = None):
        """ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–"""
        if V3_AVAILABLE:
            # åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            config = repetition_config or {
                'similarity_threshold': 0.30,
                'max_distance': 60,
                'min_compress_rate': 0.02,
                'enable_4gram_blocking': True,
                'ngram_block_size': 3,
                'enable_drp': True,
                'drp_base': 1.15,
                'drp_alpha': 0.6,
                'enable_mecab_normalization': False,
                'enable_rhetorical_protection': True,
                'enable_latin_number_detection': True,
                'debug_mode': True
            }
            self.repetition_suppressor = AdvancedRepetitionSuppressorV3(config)
            
            # LoRAå”èª¿ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            self.lora_coordinator = LoRAStyleCoordinator(lora_config)
            self.lora_coordinator.initialize_systems(config)
            
            self.logger.info("âœ… ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
            return True
        
        return False
    
    def learn_patterns_from_text(self, text: str, character: str, session_id: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’"""
        patterns_found = []
        current_time = time.time()
        
        # åŸºæœ¬çš„ãªåå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        words = text.split()
        pattern_counts = {}
        
        # å˜èªãƒ¬ãƒ™ãƒ«ã§ã®åå¾©æ¤œå‡º
        word_counts = {}
        for word in words:
            if len(word) > 1:  # 1æ–‡å­—ä»¥ä¸Šã®å˜èª
                word_counts[word] = word_counts.get(word, 0) + 1
        
        # 2-4èªã®n-gramã§ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        for n in range(2, 5):  # 2, 3, 4èªã®çµ„ã¿åˆã‚ã›
            for i in range(len(words) - n + 1):
                pattern = ' '.join(words[i:i+n])
                if len(pattern) > 2:  # æœ€å°é•·åˆ¶é™ç·©å’Œ
                    pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
        
        # åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’
        # å˜èªãƒ¬ãƒ™ãƒ«ï¼ˆ2å›ä»¥ä¸Šå‡ºç¾ï¼‰
        for word, count in word_counts.items():
            if count >= 2 and len(word) > 1:
                patterns_found.append(word)
        
        # ãƒ•ãƒ¬ãƒ¼ã‚ºãƒ¬ãƒ™ãƒ«ï¼ˆ1å›ã§ã‚‚å‡ºç¾ã™ã‚Œã°è¨˜éŒ²ï¼‰
        for pattern, count in pattern_counts.items():
            if count >= 1:
                patterns_found.append(pattern)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ›´æ–°
        with self.pattern_lock:
            for pattern in patterns_found:
                if pattern in self.cross_patterns:
                    # æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ›´æ–°
                    existing = self.cross_patterns[pattern]
                    existing.frequency += 1
                    existing.last_seen = current_time
                    existing.character_sources.add(character)
                    existing.session_ids.add(session_id)
                    
                    # æ–‡è„ˆã®è¿½åŠ ï¼ˆæœ€æ–°5ä»¶ã‚’ä¿æŒï¼‰
                    context_snippet = text[:100] + "..." if len(text) > 100 else text
                    existing.contexts.append(context_snippet)
                    if len(existing.contexts) > 5:
                        existing.contexts.pop(0)
                    
                    # é‡ã¿ã®èª¿æ•´
                    existing.suppression_weight = min(2.0, 
                        existing.suppression_weight + 0.1)
                else:
                    # æ–°è¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¿½åŠ 
                    new_pattern = CrossSuppressionPattern(
                        pattern=pattern,
                        frequency=1,
                        last_seen=current_time,
                        contexts=[text[:100] + "..." if len(text) > 100 else text],
                        suppression_weight=0.5,
                        character_sources={character},
                        session_ids={session_id}
                    )
                    self.cross_patterns[pattern] = new_pattern
                    self.stats['patterns_learned'] += 1
        
        return patterns_found
    
    def process_with_cross_suppression(self, text: str, character: str = None, 
                                     session_id: str = None) -> Tuple[str, Dict[str, Any]]:
        """ã‚¯ãƒ­ã‚¹æŠ‘åˆ¶ã‚’é©ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†"""
        if not session_id:
            session_id = f"session_{int(time.time())}"
        
        start_time = time.time()
        
        # å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆå…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å­¦ç¿’ï¼‰
        learned_patterns = self.learn_patterns_from_text(text, character, session_id)
        
        # ãƒ‡ãƒãƒƒã‚°ï¼šå­¦ç¿’çµæœã‚’ç¢ºèª
        if self.logger.isEnabledFor(logging.DEBUG):
            self.logger.debug(f"ğŸ“– å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³: {learned_patterns}")
            self.logger.debug(f"ğŸ“Š ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(learned_patterns)}")
        
        # é–¢é€£ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å–å¾—
        relevant_contexts = self.memory_buffer.get_relevant_contexts(
            character=character, session_id=session_id, limit=10
        )
        
        # ã‚¯ãƒ­ã‚¹æŠ‘åˆ¶é‡ã¿ã®è¨ˆç®—
        cross_suppression_weights = self._calculate_cross_weights(
            text, character, relevant_contexts
        )
        
        # åŸºæœ¬åå¾©æŠ‘åˆ¶å‡¦ç†
        if self.repetition_suppressor:
            # ã‚¯ãƒ­ã‚¹æŠ‘åˆ¶é‡ã¿ã‚’åæ˜ ã—ãŸè¨­å®šèª¿æ•´
            if cross_suppression_weights:
                adjusted_config = self._adjust_config_with_cross_weights(
                    cross_suppression_weights
                )
                self.repetition_suppressor.update_config(adjusted_config)
            
            result_text, metrics = self.repetition_suppressor.suppress_repetitions_with_debug_v3(
                text, character
            )
        else:
            result_text = text
            metrics = None
        
        # ã‚¯ãƒ­ã‚¹æŠ‘åˆ¶ã®è¿½åŠ é©ç”¨
        final_text, cross_stats = self._apply_cross_suppression(
            result_text, cross_suppression_weights
        )
        
        # ãƒ¡ãƒ¢ãƒªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è¿½åŠ 
        memory_context = MemoryContext(
            session_id=session_id,
            timestamp=time.time(),
            character=character or "unknown",
            text_snippet=text[:200] + "..." if len(text) > 200 else text,
            patterns_found=learned_patterns,
            suppression_applied=cross_stats.get('patterns_suppressed', [])
        )
        self.memory_buffer.add_context(memory_context)
        
        # çµ±è¨ˆæƒ…å ±ã®æ§‹ç¯‰
        processing_time = (time.time() - start_time) * 1000
        
        stats = {
            'session_id': session_id,
            'character': character,
            'original_length': len(text),
            'final_length': len(final_text),
            'total_compression_rate': (len(text) - len(final_text)) / len(text),
            'patterns_learned': len(learned_patterns),
            'cross_patterns_applied': cross_stats.get('suppressions_applied', 0),
            'relevant_contexts_used': len(relevant_contexts),
            'processing_time_ms': processing_time,
            'v3_metrics': metrics.__dict__ if metrics else {},
            'cross_suppression_weights': cross_suppression_weights,
            'learned_patterns': learned_patterns
        }
        
        self.stats['contexts_remembered'] += 1
        if cross_stats.get('suppressions_applied', 0) > 0:
            self.stats['cross_suppressions_applied'] += 1
        
        return final_text, stats
    
    def _calculate_cross_weights(self, text: str, character: str, 
                               contexts: List[MemoryContext]) -> Dict[str, float]:
        """ã‚¯ãƒ­ã‚¹æŠ‘åˆ¶é‡ã¿ã®è¨ˆç®—"""
        weights = {}
        current_time = time.time()
        
        with self.pattern_lock:
            for pattern_str, pattern in self.cross_patterns.items():
                if pattern.pattern in text:
                    # åŸºæœ¬é‡ã¿
                    base_weight = pattern.suppression_weight
                    
                    # é »åº¦ã«ã‚ˆã‚‹èª¿æ•´
                    frequency_factor = min(2.0, pattern.frequency / 10.0)
                    
                    # æ™‚é–“æ¸›è¡°
                    time_decay = self.config['pattern_decay_rate'] ** (
                        (current_time - pattern.last_seen) / 3600  # æ™‚é–“å˜ä½
                    )
                    
                    # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åˆ†é›¢
                    character_factor = 1.0
                    if self.config['character_isolation'] and character:
                        if character not in pattern.character_sources:
                            character_factor = 0.3  # ä»–ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯æŠ‘åˆ¶ã‚’å¼±ã
                    
                    # æ–‡è„ˆå½±éŸ¿
                    context_factor = 1.0
                    for context in contexts:
                        if pattern.pattern in context.text_snippet:
                            context_factor += self.config['context_influence_weight']
                    
                    final_weight = (base_weight * frequency_factor * time_decay * 
                                  character_factor * context_factor)
                    
                    if final_weight > self.config['cross_suppression_threshold']:
                        weights[pattern.pattern] = final_weight
        
        return weights
    
    def _adjust_config_with_cross_weights(self, weights: Dict[str, float]) -> Dict[str, Any]:
        """ã‚¯ãƒ­ã‚¹é‡ã¿ã‚’åæ˜ ã—ãŸè¨­å®šèª¿æ•´"""
        base_config = self.repetition_suppressor.config.copy()
        
        if weights:
            avg_weight = sum(weights.values()) / len(weights)
            
            # é‡ã¿ãŒé«˜ã„ã»ã©æŠ‘åˆ¶ã‚’å¼·åŒ–
            adjustment_factor = 1.0 + (avg_weight - 1.0) * 0.3
            
            base_config['similarity_threshold'] *= max(0.5, 1.0 / adjustment_factor)
            base_config['min_compress_rate'] *= adjustment_factor
        
        return base_config
    
    def _apply_cross_suppression(self, text: str, 
                               weights: Dict[str, float]) -> Tuple[str, Dict[str, Any]]:
        """ã‚¯ãƒ­ã‚¹æŠ‘åˆ¶ã®è¿½åŠ é©ç”¨"""
        if not weights:
            return text, {'suppressions_applied': 0, 'patterns_suppressed': []}
        
        result_text = text
        suppressions_applied = 0
        patterns_suppressed = []
        
        # é‡ã¿ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_patterns = sorted(weights.items(), key=lambda x: x[1], reverse=True)
        
        for pattern, weight in sorted_patterns:
            if pattern in result_text and weight > self.config['cross_suppression_threshold']:
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                count = result_text.count(pattern)
                if count > 1:
                    # é‡ã¿ã«åŸºã¥ã„ã¦æŠ‘åˆ¶å¼·åº¦ã‚’æ±ºå®š
                    suppress_ratio = min(0.8, weight / 2.0)
                    target_count = max(1, int(count * (1 - suppress_ratio)))
                    
                    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‰Šæ¸›
                    for _ in range(count - target_count):
                        # æœ€å¾Œã®å‡ºç¾ã‚’å‰Šé™¤
                        last_index = result_text.rfind(pattern)
                        if last_index != -1:
                            result_text = (result_text[:last_index] + 
                                         result_text[last_index + len(pattern):])
                            suppressions_applied += 1
                    
                    if suppressions_applied > 0:
                        patterns_suppressed.append(pattern)
        
        return result_text, {
            'suppressions_applied': suppressions_applied,
            'patterns_suppressed': patterns_suppressed
        }
    
    def get_session_statistics(self, session_id: str) -> Dict[str, Any]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆã®å–å¾—"""
        contexts = self.memory_buffer.get_relevant_contexts(session_id=session_id)
        
        if not contexts:
            return {'error': 'ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}
        
        total_patterns = sum(len(ctx.patterns_found) for ctx in contexts)
        total_suppressions = sum(len(ctx.suppression_applied) for ctx in contexts)
        
        return {
            'session_id': session_id,
            'context_count': len(contexts),
            'total_patterns_learned': total_patterns,
            'total_suppressions_applied': total_suppressions,
            'time_span': contexts[-1].timestamp - contexts[0].timestamp if len(contexts) > 1 else 0,
            'characters_involved': list(set(ctx.character for ctx in contexts))
        }
    
    def get_cross_pattern_stats(self) -> Dict[str, Any]:
        """ã‚¯ãƒ­ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±è¨ˆã®å–å¾—"""
        with self.pattern_lock:
            total_patterns = len(self.cross_patterns)
            active_patterns = sum(1 for p in self.cross_patterns.values() 
                                if p.frequency >= self.config['min_pattern_frequency'])
            
            top_patterns = sorted(
                self.cross_patterns.items(),
                key=lambda x: x[1].frequency,
                reverse=True
            )[:10]
            
            return {
                'total_patterns': total_patterns,
                'active_patterns': active_patterns,
                'top_patterns': [
                    {
                        'pattern': pattern.pattern,
                        'frequency': pattern.frequency,
                        'weight': pattern.suppression_weight,
                        'characters': list(pattern.character_sources)
                    }
                    for _, pattern in top_patterns
                ],
                'engine_stats': self.stats
            }
    
    def cleanup_old_patterns(self, max_age_hours: float = 168):  # 1é€±é–“
        """å¤ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        current_time = time.time()
        cutoff_time = current_time - (max_age_hours * 3600)
        
        with self.pattern_lock:
            patterns_to_remove = [
                pattern_str for pattern_str, pattern in self.cross_patterns.items()
                if pattern.last_seen < cutoff_time and 
                   pattern.frequency < self.config['min_pattern_frequency']
            ]
            
            for pattern_str in patterns_to_remove:
                del self.cross_patterns[pattern_str]
            
            self.logger.info(f"ğŸ§¹ å¤ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: {len(patterns_to_remove)}ä»¶å‰Šé™¤")
    
    def save_state(self):
        """çŠ¶æ…‹ã®ä¿å­˜"""
        self._save_configuration()
    
    def get_engine_status(self) -> Dict[str, Any]:
        """ã‚¨ãƒ³ã‚¸ãƒ³çŠ¶æ…‹ã®å–å¾—"""
        return {
            'patterns_count': len(self.cross_patterns),
            'memory_contexts': len(self.memory_buffer.contexts),
            'stats': self.stats,
            'config': self.config,
            'systems_available': {
                'repetition_suppressor': self.repetition_suppressor is not None,
                'lora_coordinator': self.lora_coordinator is not None
            }
        }


def create_default_cross_engine() -> CrossSuppressionEngine:
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¯ãƒ­ã‚¹æŠ‘åˆ¶ã‚¨ãƒ³ã‚¸ãƒ³ã®ä½œæˆ"""
    engine = CrossSuppressionEngine()
    engine.initialize_systems()
    return engine


if __name__ == "__main__":
    # ãƒ‡ãƒ¢å®Ÿè¡Œ
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸ§  ã‚¯ãƒ­ã‚¹æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  ãƒ‡ãƒ¢")
    print("=" * 50)
    
    engine = create_default_cross_engine()
    
    # ãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³
    session_id = "demo_session_001"
    character = "ãƒ†ã‚¹ãƒˆã‚­ãƒ£ãƒ©"
    
    try:
        # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹1
        text1 = "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã ã‹ã‚‰æ•£æ­©ã—ã¾ã—ã‚‡ã†ã€‚"
        result1, stats1 = engine.process_with_cross_suppression(text1, character, session_id)
        
        print(f"ãƒ†ã‚¹ãƒˆ1:")
        print(f"  å…¥åŠ›: {text1}")
        print(f"  å‡ºåŠ›: {result1}")
        print(f"  å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³: {stats1['patterns_learned']}")
        print(f"  å­¦ç¿’è©³ç´°: {stats1.get('learned_patterns', [])}")
        print(f"  åœ§ç¸®ç‡: {stats1['total_compression_rate']:.1%}")
        
        # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹2ï¼ˆåŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€ï¼‰
        text2 = "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã ã—ã€ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ãªã®ã§å¤–ã«å‡ºã¾ã™ã€‚"
        result2, stats2 = engine.process_with_cross_suppression(text2, character, session_id)
        
        print(f"\nãƒ†ã‚¹ãƒˆ2ï¼ˆã‚¯ãƒ­ã‚¹æŠ‘åˆ¶é©ç”¨ï¼‰:")
        print(f"  å…¥åŠ›: {text2}")
        print(f"  å‡ºåŠ›: {result2}")
        print(f"  åœ§ç¸®ç‡: {stats2['total_compression_rate']:.1%}")
        print(f"  ã‚¯ãƒ­ã‚¹æŠ‘åˆ¶: {stats2['cross_patterns_applied']}ä»¶")
    except Exception as e:
        print(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    # çµ±è¨ˆè¡¨ç¤º
    pattern_stats = engine.get_cross_pattern_stats()
    print(f"\nğŸ“Š ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±è¨ˆ:")
    print(f"  ç·ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {pattern_stats['total_patterns']}")
    print(f"  ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ‘ã‚¿ãƒ¼ãƒ³: {pattern_stats['active_patterns']}")
    
    if pattern_stats['top_patterns']:
        print(f"  ä¸Šä½ãƒ‘ã‚¿ãƒ¼ãƒ³:")
        for i, pattern_info in enumerate(pattern_stats['top_patterns'][:3], 1):
            print(f"    {i}. '{pattern_info['pattern']}' é »åº¦:{pattern_info['frequency']} é‡ã¿:{pattern_info['weight']:.2f}")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ
    session_stats = engine.get_session_statistics(session_id)
    print(f"\nğŸ“ˆ ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ:")
    print(f"  ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ•°: {session_stats.get('context_count', 0)}")
    print(f"  å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {session_stats.get('total_patterns_learned', 0)}")
    print(f"  é©ç”¨æŠ‘åˆ¶æ•°: {session_stats.get('total_suppressions_applied', 0)}")
    
    engine.save_state()
    print("\nâœ… ãƒ‡ãƒ¢å®Œäº†") 