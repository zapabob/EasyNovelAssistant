# -*- coding: utf-8 -*-
"""
RTX 3080ç‰¹åŒ–æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
GPUåˆ©ç”¨ç‡æœ€å¤§åŒ–ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹èª¿æ•´
"""

import os
import sys
import time
import json
import logging
import platform
import subprocess
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
import numpy as np

try:
    import torch
    import torch.cuda as cuda
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False


class RTX3080Optimizer:
    """
    RTX 3080ç‰¹åŒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = self._setup_logging()
        
        # GPUæƒ…å ±
        self.gpu_info = self._detect_gpu()
        self.is_rtx3080 = self._verify_rtx3080()
        
        # æœ€é©åŒ–è¨­å®š
        self.memory_target_usage = config.get("memory_target_usage", 0.85)  # 85%ã®VRAMä½¿ç”¨ç‡ã‚’ç›®æ¨™
        self.batch_size_auto = config.get("batch_size_auto", True)
        self.quantization_enabled = config.get("quantization", True)
        self.cuda_optimization = config.get("cuda_optimization", True)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        self.performance_stats = {
            "tokens_per_second": 0.0,
            "memory_usage": 0.0,
            "gpu_utilization": 0.0,
            "temperature": 0.0,
            "power_draw": 0.0
        }
        
        self.logger.info(f"RTX 3080æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆæœŸåŒ–å®Œäº† (GPUæ¤œå‡º: {self.is_rtx3080})")
    
    def _setup_logging(self) -> logging.Logger:
        """ãƒ­ã‚°è¨­å®š"""
        logger = logging.getLogger("RTX3080Optimizer")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
    
    def _detect_gpu(self) -> Dict[str, Any]:
        """GPUæƒ…å ±ã®æ¤œå‡º"""
        gpu_info = {
            "detected": False,
            "name": "Unknown",
            "memory_total": 0,
            "memory_free": 0,
            "compute_capability": "0.0",
            "driver_version": "Unknown",
            "cuda_version": "Unknown"
        }
        
        if not TORCH_AVAILABLE:
            self.logger.warning("PyTorchãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚GPUæ¤œå‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return gpu_info
        
        if not torch.cuda.is_available():
            self.logger.warning("CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
            return gpu_info
        
        try:
            gpu_info["detected"] = True
            gpu_info["name"] = torch.cuda.get_device_name(0)
            
            # ãƒ¡ãƒ¢ãƒªæƒ…å ±
            memory_info = torch.cuda.mem_get_info(0)
            gpu_info["memory_free"] = memory_info[0] // (1024**3)  # GB
            gpu_info["memory_total"] = memory_info[1] // (1024**3)  # GB
            
            # Compute Capability
            capability = torch.cuda.get_device_capability(0)
            gpu_info["compute_capability"] = f"{capability[0]}.{capability[1]}"
            
            # CUDAæƒ…å ±
            gpu_info["cuda_version"] = torch.version.cuda
            
            # ãƒ‰ãƒ©ã‚¤ãƒæƒ…å ±ï¼ˆnvidia-smiã‚’ä½¿ç”¨ï¼‰
            try:
                result = subprocess.run(
                    ["nvidia-smi", "--query-gpu=driver_version", "--format=csv,noheader,nounits"],
                    capture_output=True, text=True, timeout=5
                )
                if result.returncode == 0:
                    gpu_info["driver_version"] = result.stdout.strip()
            except (subprocess.TimeoutExpired, FileNotFoundError):
                pass
            
            self.logger.info(f"GPUæ¤œå‡º: {gpu_info['name']} ({gpu_info['memory_total']}GB VRAM)")
            
        except Exception as e:
            self.logger.error(f"GPUæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        return gpu_info
    
    def _verify_rtx3080(self) -> bool:
        """RTX 3080ã®æ¤œè¨¼"""
        if not self.gpu_info["detected"]:
            return False
        
        gpu_name = self.gpu_info["name"].lower()
        rtx3080_identifiers = ["rtx 3080", "geforce rtx 3080", "3080"]
        
        is_rtx3080 = any(identifier in gpu_name for identifier in rtx3080_identifiers)
        
        if is_rtx3080:
            self.logger.info("RTX 3080ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å°‚ç”¨æœ€é©åŒ–ã‚’é©ç”¨ã—ã¾ã™ã€‚")
        else:
            self.logger.warning(f"RTX 3080ä»¥å¤–ã®GPUæ¤œå‡º: {self.gpu_info['name']}")
        
        return is_rtx3080
    
    def optimize_for_text_generation(self, model_size: str = "8B") -> Dict[str, Any]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆç”¨ã®æœ€é©åŒ–è¨­å®š
        """
        try:
            optimization_config = {
                "gpu_optimization": self._get_gpu_optimization(),
                "memory_optimization": self._get_memory_optimization(),
                "model_optimization": self._get_model_optimization(model_size),
                "cuda_optimization": self._get_cuda_optimization(),
                "quantization": self._get_quantization_settings(),
                "performance_prediction": self._predict_performance(model_size)
            }
            
            self.logger.info("ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆæœ€é©åŒ–è¨­å®šã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
            return optimization_config
            
        except Exception as e:
            self.logger.error(f"æœ€é©åŒ–è¨­å®šç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _get_gpu_optimization(self) -> Dict[str, Any]:
        """GPUæœ€é©åŒ–è¨­å®š"""
        if not self.is_rtx3080:
            return {"enabled": False, "reason": "RTX 3080ä»¥å¤–ã®GPU"}
        
        return {
            "enabled": True,
            "tensor_cores": True,  # RTX 3080ã®Tensor Coreæ´»ç”¨
            "mixed_precision": True,  # FP16è¨ˆç®—
            "cuda_graphs": True,  # CUDA Graphsã§ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰Šæ¸›
            "gpu_layers": self._calculate_optimal_gpu_layers(),
            "max_seq_len": 8192,  # RTX 3080ã«é©ã—ãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
            "threads": 16,  # RTX 3080ã®ä¸¦åˆ—å‡¦ç†èƒ½åŠ›
            "batch_size": self._calculate_optimal_batch_size()
        }
    
    def _get_memory_optimization(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š"""
        if not self.gpu_info["detected"]:
            return {"enabled": False}
        
        total_vram = self.gpu_info["memory_total"]
        target_usage = int(total_vram * self.memory_target_usage)
        
        return {
            "enabled": True,
            "vram_total": total_vram,
            "vram_target": target_usage,
            "memory_growth": True,  # å‹•çš„ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦
            "cache_optimization": True,
            "garbage_collection": True,
            "memory_mapping": True,  # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ”ãƒ³ã‚°æ´»ç”¨
            "swap_prevention": True  # ã‚¹ãƒ¯ãƒƒãƒ—é˜²æ­¢
        }
    
    def _get_model_optimization(self, model_size: str) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–è¨­å®š"""
        size_configs = {
            "7B": {
                "recommended_quantization": "Q4_K_M",
                "max_context": 8192,
                "rope_freq_base": 10000,
                "rope_freq_scale": 1.0
            },
            "8B": {
                "recommended_quantization": "Q4_K_M",
                "max_context": 8192,
                "rope_freq_base": 10000,
                "rope_freq_scale": 1.0
            },
            "13B": {
                "recommended_quantization": "Q4_K_S",  # ã‚ˆã‚Šè»½ã„é‡å­åŒ–
                "max_context": 4096,  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’åˆ¶é™
                "rope_freq_base": 10000,
                "rope_freq_scale": 1.0
            },
            "30B": {
                "recommended_quantization": "Q3_K_M",  # å¤§å¹…ãªé‡å­åŒ–
                "max_context": 2048,
                "rope_freq_base": 10000,
                "rope_freq_scale": 1.0
            }
        }
        
        config = size_configs.get(model_size, size_configs["8B"])
        config["model_size"] = model_size
        config["optimization_target"] = "speed_quality_balance"
        
        return config
    
    def _get_cuda_optimization(self) -> Dict[str, Any]:
        """CUDAæœ€é©åŒ–è¨­å®š"""
        if not self.cuda_optimization or not TORCH_AVAILABLE:
            return {"enabled": False}
        
        return {
            "enabled": True,
            "cuda_version": self.gpu_info.get("cuda_version", "Unknown"),
            "compute_capability": self.gpu_info.get("compute_capability", "0.0"),
            "tensor_cores": True,
            "cudnn_benchmark": True,  # cuDNNãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æœ€é©åŒ–
            "cudnn_deterministic": False,  # é€Ÿåº¦å„ªå…ˆ
            "cuda_streams": 4,  # ä¸¦åˆ—ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°
            "memory_pool": True,  # ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ä½¿ç”¨
            "jit_compilation": True  # JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
        }
    
    def _get_quantization_settings(self) -> Dict[str, Any]:
        """é‡å­åŒ–è¨­å®š"""
        if not self.quantization_enabled:
            return {"enabled": False}
        
        return {
            "enabled": True,
            "default_type": "Q4_K_M",  # RTX 3080ã«é©ã—ãŸé‡å­åŒ–
            "supported_types": [
                "Q4_K_M",   # æ¨å¥¨ï¼šå“è³ªã¨é€Ÿåº¦ã®ãƒãƒ©ãƒ³ã‚¹
                "Q4_K_S",   # è»½é‡ç‰ˆ
                "Q5_K_M",   # é«˜å“è³ªç‰ˆ
                "Q6_K",     # æœ€é«˜å“è³ª
                "Q8_0",     # ç²¾åº¦é‡è¦–
                "Q3_K_M"    # æœ€è»½é‡
            ],
            "adaptive": True,  # å‹•çš„é‡å­åŒ–èª¿æ•´
            "calibration": True,  # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨
            "optimization_target": "inference_speed"
        }
    
    def _calculate_optimal_gpu_layers(self) -> int:
        """æœ€é©ãªGPUãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ã®è¨ˆç®—"""
        if not self.is_rtx3080:
            return 0
        
        vram_gb = self.gpu_info["memory_total"]
        
        # RTX 3080 (10GB) å‘ã‘ã®æ¨å¥¨ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
        if vram_gb >= 10:
            return 35  # 8Bãƒ¢ãƒ‡ãƒ«ã®å¤§éƒ¨åˆ†ã‚’GPUã«
        elif vram_gb >= 8:
            return 28  # 7-8GBã®å ´åˆ
        elif vram_gb >= 6:
            return 20  # 6GBã®å ´åˆ
        else:
            return 10  # ä½VRAMç’°å¢ƒ
    
    def _calculate_optimal_batch_size(self) -> int:
        """æœ€é©ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã®è¨ˆç®—"""
        if not self.batch_size_auto:
            return self.config.get("batch_size", 1)
        
        vram_gb = self.gpu_info["memory_total"]
        
        # RTX 3080å‘ã‘ãƒãƒƒãƒã‚µã‚¤ã‚º
        if vram_gb >= 10:
            return 8  # 10GB VRAMã®å ´åˆ
        elif vram_gb >= 8:
            return 6
        elif vram_gb >= 6:
            return 4
        else:
            return 1
    
    def _predict_performance(self, model_size: str) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬"""
        if not self.is_rtx3080:
            return {"available": False, "reason": "RTX 3080ä»¥å¤–ã®GPU"}
        
        # RTX 3080ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã«åŸºã¥ãäºˆæ¸¬
        performance_estimates = {
            "7B": {
                "tokens_per_second": 42.0,
                "memory_usage_gb": 6.5,
                "power_draw_w": 280,
                "temperature_target_c": 78
            },
            "8B": {
                "tokens_per_second": 40.0,
                "memory_usage_gb": 7.2,
                "power_draw_w": 285,
                "temperature_target_c": 79
            },
            "13B": {
                "tokens_per_second": 28.0,
                "memory_usage_gb": 9.8,
                "power_draw_w": 300,
                "temperature_target_c": 82
            },
            "30B": {
                "tokens_per_second": 12.0,
                "memory_usage_gb": 9.9,
                "power_draw_w": 305,
                "temperature_target_c": 84
            }
        }
        
        estimate = performance_estimates.get(model_size, performance_estimates["8B"])
        estimate["model_size"] = model_size
        estimate["optimization_level"] = "RTX3080_optimized"
        estimate["confidence"] = 0.85
        
        return estimate
    
    def monitor_performance(self) -> Dict[str, Any]:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–"""
        try:
            stats = {
                "timestamp": datetime.now().isoformat(),
                "gpu_detected": self.gpu_info["detected"],
                "is_rtx3080": self.is_rtx3080
            }
            
            if TORCH_AVAILABLE and torch.cuda.is_available():
                # GPUä½¿ç”¨ç‡ã¨ãƒ¡ãƒ¢ãƒª
                stats.update(self._get_gpu_stats())
            
            if PSUTIL_AVAILABLE:
                # ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ
                stats.update(self._get_system_stats())
            
            # nvidia-smiæƒ…å ±
            nvidia_stats = self._get_nvidia_smi_stats()
            if nvidia_stats:
                stats.update(nvidia_stats)
            
            # çµ±è¨ˆæ›´æ–°
            self.performance_stats.update(stats)
            
            return stats
            
        except Exception as e:
            self.logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _get_gpu_stats(self) -> Dict[str, Any]:
        """GPUçµ±è¨ˆæƒ…å ±å–å¾—"""
        stats = {}
        
        try:
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            memory_info = torch.cuda.mem_get_info(0)
            memory_used = memory_info[1] - memory_info[0]
            memory_total = memory_info[1]
            
            stats["memory_used_gb"] = memory_used / (1024**3)
            stats["memory_total_gb"] = memory_total / (1024**3)
            stats["memory_usage_percent"] = (memory_used / memory_total) * 100
            
            # GPUåˆ©ç”¨ç‡ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            stats["gpu_utilization_estimate"] = min(stats["memory_usage_percent"] * 1.2, 100)
            
        except Exception as e:
            self.logger.error(f"GPUçµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        return stats
    
    def _get_system_stats(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆæƒ…å ±å–å¾—"""
        stats = {}
        
        try:
            # CPUä½¿ç”¨ç‡
            stats["cpu_percent"] = psutil.cpu_percent(interval=1)
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            memory = psutil.virtual_memory()
            stats["ram_used_gb"] = memory.used / (1024**3)
            stats["ram_total_gb"] = memory.total / (1024**3)
            stats["ram_usage_percent"] = memory.percent
            
            # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡
            disk = psutil.disk_usage('/')
            stats["disk_usage_percent"] = (disk.used / disk.total) * 100
            
        except Exception as e:
            self.logger.error(f"ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        return stats
    
    def _get_nvidia_smi_stats(self) -> Optional[Dict[str, Any]]:
        """nvidia-smiã‹ã‚‰è©³ç´°çµ±è¨ˆã‚’å–å¾—"""
        try:
            result = subprocess.run([
                "nvidia-smi",
                "--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu,power.draw",
                "--format=csv,noheader,nounits"
            ], capture_output=True, text=True, timeout=5)
            
            if result.returncode == 0:
                values = result.stdout.strip().split(", ")
                if len(values) >= 5:
                    return {
                        "gpu_utilization_percent": float(values[0]),
                        "gpu_memory_used_mb": float(values[1]),
                        "gpu_memory_total_mb": float(values[2]),
                        "gpu_temperature_c": float(values[3]),
                        "gpu_power_draw_w": float(values[4])
                    }
        
        except (subprocess.TimeoutExpired, FileNotFoundError, ValueError) as e:
            self.logger.debug(f"nvidia-smiçµ±è¨ˆå–å¾—ä¸å¯: {e}")
        
        return None
    
    def optimize_model_loading(self, model_path: str) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æœ€é©åŒ–"""
        optimization_params = {
            "use_mmap": True,  # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ”ãƒ³ã‚°ä½¿ç”¨
            "use_mlock": False,  # RTX 3080ç’°å¢ƒã§ã¯ç„¡åŠ¹
            "numa": False,  # å˜ä¸€GPUç’°å¢ƒ
            "low_vram": False,  # RTX 3080ã¯ååˆ†ãªVRAM
            "verbose": True,
            "threads": 16,  # RTX 3080ã®CUDAã‚³ã‚¢æ•°ã«æœ€é©åŒ–
            "n_batch": self._calculate_optimal_batch_size(),
            "n_gpu_layers": self._calculate_optimal_gpu_layers(),
            "rope_freq_base": 10000.0,
            "rope_freq_scale": 1.0
        }
        
        # VRAMä½¿ç”¨é‡èª¿æ•´
        available_vram = self.gpu_info["memory_total"]
        if available_vram >= 10:
            optimization_params["n_ctx"] = 8192  # ãƒ•ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        elif available_vram >= 8:
            optimization_params["n_ctx"] = 6144
        else:
            optimization_params["n_ctx"] = 4096
        
        self.logger.info(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æœ€é©åŒ–è¨­å®š: {optimization_params}")
        return optimization_params
    
    def generate_optimization_report(self) -> str:
        """æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = f"""
=== RTX 3080æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ ===
ç”Ÿæˆæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ–¥ï¸ GPUæƒ…å ±:
  âœ“ æ¤œå‡º: {self.gpu_info['name']}
  âœ“ VRAM: {self.gpu_info['memory_total']}GB
  âœ“ CUDA: {self.gpu_info['cuda_version']}
  âœ“ ãƒ‰ãƒ©ã‚¤ãƒ: {self.gpu_info['driver_version']}
  âœ“ RTX 3080èªè­˜: {'Yes' if self.is_rtx3080 else 'No'}

âš¡ æœ€é©åŒ–è¨­å®š:
  âœ“ GPUæœ€é©åŒ–: {'æœ‰åŠ¹' if self.is_rtx3080 else 'ç„¡åŠ¹'}
  âœ“ Tensor Cores: {'æœ‰åŠ¹' if self.is_rtx3080 else 'ç„¡åŠ¹'}
  âœ“ æ··åˆç²¾åº¦: {'æœ‰åŠ¹' if self.is_rtx3080 else 'ç„¡åŠ¹'}
  âœ“ é‡å­åŒ–: {'æœ‰åŠ¹' if self.quantization_enabled else 'ç„¡åŠ¹'}
  âœ“ æ¨å¥¨GPUãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {self._calculate_optimal_gpu_layers()}
  âœ“ æ¨å¥¨ãƒãƒƒãƒã‚µã‚¤ã‚º: {self._calculate_optimal_batch_size()}

ğŸ“Š äºˆæƒ³ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ (8Bãƒ¢ãƒ‡ãƒ«):
  âœ“ ãƒˆãƒ¼ã‚¯ãƒ³/ç§’: {self._predict_performance('8B')['tokens_per_second']}
  âœ“ VRAMä½¿ç”¨é‡: {self._predict_performance('8B')['memory_usage_gb']:.1f}GB
  âœ“ æ¶ˆè²»é›»åŠ›: {self._predict_performance('8B')['power_draw_w']}W
  âœ“ ç›®æ¨™æ¸©åº¦: {self._predict_performance('8B')['temperature_target_c']}Â°C

ğŸ› ï¸ æ¨å¥¨è¨­å®š:
"""
        
        if self.is_rtx3080:
            report += """  â€¢ Q4_K_Mé‡å­åŒ–ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
  â€¢ GPUå±¤æ•°35ã§VRAMã‚’æœ€å¤§æ´»ç”¨
  â€¢ æ··åˆç²¾åº¦(FP16)ã§é«˜é€ŸåŒ–
  â€¢ ãƒãƒƒãƒã‚µã‚¤ã‚º8ã§åŠ¹ç‡åŒ–
  â€¢ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·8192ã§é«˜å“è³ª"""
        else:
            report += """  â€¢ RTX 3080ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ
  â€¢ æ±ç”¨GPUè¨­å®šã‚’é©ç”¨
  â€¢ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒåˆ¶é™ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™"""
        
        return report
    
    def save_optimization_config(self, filepath: str):
        """æœ€é©åŒ–è¨­å®šã®ä¿å­˜"""
        try:
            config = {
                "timestamp": datetime.now().isoformat(),
                "gpu_info": self.gpu_info,
                "is_rtx3080": self.is_rtx3080,
                "optimization_8b": self.optimize_for_text_generation("8B"),
                "optimization_7b": self.optimize_for_text_generation("7B"),
                "optimization_13b": self.optimize_for_text_generation("13B"),
                "model_loading": self.optimize_model_loading(""),
                "performance_stats": self.performance_stats
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"æœ€é©åŒ–è¨­å®šã‚’ä¿å­˜: {filepath}")
            
        except Exception as e:
            self.logger.error(f"è¨­å®šä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    print("RTX 3080æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    config = {
        "memory_target_usage": 0.85,
        "batch_size_auto": True,
        "quantization": True,
        "cuda_optimization": True
    }
    
    optimizer = RTX3080Optimizer(config)
    
    # æœ€é©åŒ–è¨­å®šç”Ÿæˆ
    optimization = optimizer.optimize_for_text_generation("8B")
    print("\n8Bãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–è¨­å®š:")
    print(json.dumps(optimization, indent=2, ensure_ascii=False))
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
    performance = optimizer.monitor_performance()
    print("\nãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ:")
    print(json.dumps(performance, indent=2, ensure_ascii=False))
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = optimizer.generate_optimization_report()
    print(report)
    
    # è¨­å®šä¿å­˜
    optimizer.save_optimization_config("rtx3080_optimization_config.json") 