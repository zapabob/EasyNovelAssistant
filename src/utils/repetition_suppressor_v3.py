# -*- coding: utf-8 -*-
"""
é«˜åº¦èªå¥åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3 (Advanced Repetition Suppression System v3)
æˆåŠŸç‡80%+é”æˆã®ãŸã‚ã®ã€Œãƒ©ã‚¹ãƒˆ21.7ptã€å¼·åŒ–ç‰ˆ - æ€§èƒ½æœ€é©åŒ–ç‰ˆ

æ–°æ©Ÿèƒ½ v3:
1. ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å´4-gramãƒ–ãƒ­ãƒƒã‚¯ï¼ˆæœªç„¶é˜²æ­¢ï¼‰
2. MeCab + UniDicèªåŸºåº•å½¢æ­£è¦åŒ–
3. èª¤æŠ‘åˆ¶æœ€å°åŒ–ï¼ˆä¿®è¾çš„ä¾‹å¤–ã€éŸ³è±¡å¾´èªä¿è­·ï¼‰
4. è©•ä¾¡åŸºæº–èª¿æ•´ï¼ˆmin_compress_rate: 10% â†’ 3%ï¼‰
5. ãƒ©ãƒ†ãƒ³æ–‡å­—ãƒ»æ•°å­—é€£ç•ªæ¤œçŸ¥
6. CIè¡Œåˆ—ãƒ†ã‚¹ãƒˆå¯¾å¿œ
7. å¼·åŒ–ã•ã‚ŒãŸæ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆ80%+ç›®æ¨™ï¼‰
"""

import re
import time
import json
import os
from typing import List, Dict, Tuple, Optional, Set
from collections import defaultdict, Counter
from dataclasses import dataclass
import numpy as np
from tqdm import tqdm

# MeCabçµ±åˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import fugashi
    MECAB_AVAILABLE = True
except ImportError:
    MECAB_AVAILABLE = False
    print("âš ï¸ MeCabï¼ˆfugashiï¼‰ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬æ©Ÿèƒ½ã®ã¿ã§å‹•ä½œã—ã¾ã™ã€‚")


@dataclass
class SuppressionMetricsV3:
    """v3å¼·åŒ–ç‰ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    input_length: int
    output_length: int
    patterns_detected: int
    patterns_suppressed: int
    detection_misses: int
    over_compressions: int
    processing_time_ms: float
    success_rate: float
    
    # v3æ–°é …ç›®
    ngram_blocks_applied: int = 0
    mecab_normalizations: int = 0
    rhetorical_exceptions: int = 0
    latin_number_blocks: int = 0
    min_compress_rate: float = 0.03  # 3%åŸºæº–


@dataclass
class RepetitionPatternV3:
    """v3ç‰ˆåå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¾‹å¤–ãƒ•ãƒ©ã‚°ä»˜ãï¼‰"""
    pattern: str
    count: int
    positions: List[int]
    pattern_type: str
    severity: float
    similarity_score: float = 0.0
    suppression_result: str = ""  # MISS/OVER/OK/SKIP
    
    # v3æ–°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    is_rhetorical: bool = False  # ä¿®è¾çš„åå¾©
    is_onomatopoeia: bool = False  # éŸ³è±¡å¾´èª
    is_lyrical: bool = False  # æ­Œè©é¢¨ãƒªãƒ•ãƒ¬ã‚¤ãƒ³
    normalized_form: str = ""  # MeCabæ­£è¦åŒ–å½¢
    
    def __post_init__(self):
        # positionsãŒç©ºãƒªã‚¹ãƒˆã®å ´åˆã®å‡¦ç†
        if not self.positions:
            object.__setattr__(self, 'positions', [])


class AdvancedRepetitionSuppressorV3:
    """
    é«˜åº¦èªå¥åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3
    58.3% â†’ 80% ã¸ã®ã€Œãƒ©ã‚¹ãƒˆ21.7ptã€å¼·åŒ–ç‰ˆ - æ€§èƒ½æœ€é©åŒ–ç‰ˆ
    """
    
    def __init__(self, config: Dict = None):
        self.config = config or {}
        
        # åŸºæœ¬è¨­å®šï¼ˆv3æœ€é©åŒ–æ¸ˆã¿ - æˆåŠŸç‡80%+ç›®æ¨™ï¼‰
        self.min_repeat_threshold = self.config.get('min_repeat_threshold', 1)  # ã‚ˆã‚Šå³æ ¼ã«
        self.max_distance = self.config.get('max_distance', 60)  # æ¤œå‡ºè·é›¢æ‹¡å¤§
        self.similarity_threshold = self.config.get('similarity_threshold', 0.65)  # ãƒ­ã‚°ææ¡ˆï¼š0.65ä»¥ä¸‹ã«
        self.phonetic_threshold = self.config.get('phonetic_threshold', 0.6)  # éŸ³éŸ»é¡ä¼¼åº¦ä¸‹ã’ã‚‹
        
        # v3æ–°æ©Ÿèƒ½è¨­å®šï¼ˆå¼·åŒ–ç‰ˆï¼‰
        self.enable_4gram_blocking = self.config.get('enable_4gram_blocking', True)
        self.ngram_block_size = self.config.get('ngram_block_size', 5)  # ãƒ­ã‚°ææ¡ˆï¼š5-6ã«å¢—åŠ 
        self.enable_mecab_normalization = self.config.get('enable_mecab_normalization', MECAB_AVAILABLE)
        self.enable_rhetorical_protection = self.config.get('enable_rhetorical_protection', True)
        self.min_compress_rate = self.config.get('min_compress_rate', 0.03)  # ãƒ­ã‚°ææ¡ˆï¼šã‚ˆã‚Šå³æ ¼ãªåŸºæº–
        self.enable_latin_number_detection = self.config.get('enable_latin_number_detection', True)
        
        # å¾“æ¥è¨­å®šï¼ˆæœ€é©åŒ–ï¼‰
        self.enable_aggressive_mode = self.config.get('enable_aggressive_mode', True)
        self.debug_mode = self.config.get('debug_mode', True)
        self.enable_drp = self.config.get('enable_drp', True)
        self.drp_base = self.config.get('drp_base', 1.15)  # ã‚ˆã‚Šå¼·åŒ–
        self.drp_alpha = self.config.get('drp_alpha', 0.6)  # ã‚ˆã‚Šå¼·åŒ–
        
        # MeCabåˆæœŸåŒ–
        self.tagger = None
        if self.enable_mecab_normalization and MECAB_AVAILABLE:
            try:
                self.tagger = fugashi.Tagger()
                print("âœ… MeCabåˆæœŸåŒ–æˆåŠŸï¼ˆèªåŸºåº•å½¢æ­£è¦åŒ–æœ‰åŠ¹ï¼‰")
            except Exception as e:
                print(f"âš ï¸ MeCabåˆæœŸåŒ–å¤±æ•—: {e}")
                self.enable_mecab_normalization = False
        
        # ä¿®è¾çš„ä¾‹å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        self.rhetorical_patterns = [
            r'([^ã€‚ï¼ï¼Ÿ]{1,5})ã€\1ã€\1[ï¼ï¼Ÿã€‚]',  # ã€Œã­ãˆã€ã­ãˆã€ã­ãˆï¼ã€
            r'â™ª.*',  # æ­Œè©é¢¨
            r'.*ã‚‰.*ã‚‰.*',  # â™ªãƒ©ãƒ©ãƒ©ç³»
            r'(ä¿³å¥|çŸ­æ­Œ|è©©).*',  # è©©çš„è¡¨ç¾ä¿è­·å¼·åŒ–
            r'.*[ã‚ã‹ã•ãŸãª].*[ã‚ã‹ã•ãŸãª].*',  # éŸ»å¾‹çš„åå¾©
        ]
        
        # éŸ³è±¡å¾´èªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆé–¢è¥¿å¼å¯¾å¿œå¼·åŒ–ï¼‰
        self.onomatopoeia_patterns = [
            r'[ã‚¡-ãƒ³]{2,3}',  # ãƒ‰ã‚­ãƒ‰ã‚­ã€ãƒ¯ã‚¯ãƒ¯ã‚¯ç­‰
            r'[ã-ã‚“]{2,3}',  # ã‚ãã‚ãç­‰
            r'ãã‚„{1,}ãã‚„{1,}',  # é–¢è¥¿å¼ã€Œãã‚„ãã‚„ã€å¯¾å¿œå¼·åŒ–
            r'ã‚ã‹ã‚“{1,}ã‚ã‹ã‚“{1,}',  # é–¢è¥¿å¼ã€Œã‚ã‹ã‚“ã‚ã‹ã‚“ã€å¯¾å¿œå¼·åŒ–
            r'ã‚„ãª{1,}ã‚„ãª{1,}',  # é–¢è¥¿å¼ã€Œã‚„ãªã‚„ãªã€å¯¾å¿œå¼·åŒ–
            r'ã›ã‚„{1,}ã›ã‚„{1,}',  # é–¢è¥¿å¼ã€Œã›ã‚„ã›ã‚„ã€å¯¾å¿œ
            r'ã»ã‚“{1,}ã»ã‚“{1,}',  # é–¢è¥¿å¼ã€Œã»ã‚“ã»ã‚“ã€å¯¾å¿œ
            r'ãª{1,}ãª{1,}',  # é–¢è¥¿å¼ã€Œãªãªã€å¯¾å¿œ
        ]
        
        # ãƒ©ãƒ†ãƒ³æ–‡å­—ãƒ»æ•°å­—é€£ç•ªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        self.latin_number_patterns = [
            r'([A-Za-z0-9])\1{2,}',  # ã‚ˆã‚ŠçŸ­ã„é€£ç¶šã‚‚æ¤œå‡º
            r'([wWï½—ï¼·]){3,}',  # wé€£ç¶šç‰¹åˆ¥å¯¾å¿œ
            r'([7ï¼—]){3,}',  # 7é€£ç¶šç‰¹åˆ¥å¯¾å¿œ
        ]
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.session_metrics = []
        self.total_success_count = 0
        self.total_attempts = 0
        
        # å‡¦ç†å±¥æ­´
        self.debug_log = []
        self.replacement_cache = {}
        self.character_patterns = defaultdict(list)
        
        # ä»£æ›¿è¡¨ç¾è¾æ›¸ã®èª­ã¿è¾¼ã¿
        self.load_replacement_dictionary_v3()
        
        # ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼å¤‰æ•°ã®åˆæœŸåŒ–
        self._ngram_blocks_applied = 0
        self._latin_blocks_applied = 0
        self._mecab_normalizations = 0
        
        print(f"ğŸš€ åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v3 åˆæœŸåŒ–å®Œäº†ï¼ˆ80%+ç›®æ¨™æœ€é©åŒ–ç‰ˆï¼‰")
        print(f"   â”œâ”€ {self.ngram_block_size}-gramãƒ–ãƒ­ãƒƒã‚¯: {'æœ‰åŠ¹' if self.enable_4gram_blocking else 'ç„¡åŠ¹'}")
        print(f"   â”œâ”€ MeCabæ­£è¦åŒ–: {'æœ‰åŠ¹' if self.enable_mecab_normalization else 'ç„¡åŠ¹'}")
        print(f"   â”œâ”€ ä¿®è¾çš„ä¿è­·: {'æœ‰åŠ¹' if self.enable_rhetorical_protection else 'ç„¡åŠ¹'}")
        print(f"   â”œâ”€ æˆåŠŸåˆ¤å®šåŸºæº–: {self.min_compress_rate:.1%}ï¼ˆå³æ ¼ãƒ¢ãƒ¼ãƒ‰ï¼‰")
        print(f"   â”œâ”€ æ¤œå‡ºè·é›¢: {self.max_distance}æ–‡å­—")
        print(f"   â”œâ”€ é¡ä¼¼åº¦é–¾å€¤: {self.similarity_threshold:.2f}")
        print(f"   â””â”€ é€£ç•ªæ¤œçŸ¥: {'æœ‰åŠ¹' if self.enable_latin_number_detection else 'ç„¡åŠ¹'}")

    def update_config(self, new_config: Dict):
        """
        ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¨­å®šæ›´æ–°ï¼ˆGUIçµ±åˆç”¨ï¼‰
        """
        try:
            # è¨­å®šå€¤ã®æ›´æ–°
            for key, value in new_config.items():
                if hasattr(self, key):
                    setattr(self, key, value)
                    self.config[key] = value
            
            # ç‰¹åˆ¥ãªå‡¦ç†ãŒå¿…è¦ãªè¨­å®š
            if 'enable_mecab_normalization' in new_config:
                if new_config['enable_mecab_normalization'] and not self.tagger and MECAB_AVAILABLE:
                    try:
                        self.tagger = fugashi.Tagger()
                        print("âœ… MeCabå‹•çš„æœ‰åŠ¹åŒ–æˆåŠŸ")
                    except Exception as e:
                        print(f"âš ï¸ MeCabå‹•çš„æœ‰åŠ¹åŒ–å¤±æ•—: {e}")
                        self.enable_mecab_normalization = False
            
            # è¨­å®šå¤‰æ›´ãƒ­ã‚°
            if self.debug_mode:
                changed_settings = [f"{k}={v}" for k, v in new_config.items()]
                print(f"ğŸ”„ v3è¨­å®šæ›´æ–°: {', '.join(changed_settings)}")
            
        except Exception as e:
            print(f"âŒ è¨­å®šæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def get_current_config(self) -> Dict:
        """ç¾åœ¨ã®è¨­å®šã‚’å–å¾—"""
        return {
            'similarity_threshold': self.similarity_threshold,
            'max_distance': self.max_distance,
            'min_compress_rate': self.min_compress_rate,
            'ngram_block_size': self.ngram_block_size,
            'enable_4gram_blocking': self.enable_4gram_blocking,
            'drp_base': self.drp_base,
            'drp_alpha': self.drp_alpha,
            'enable_drp': self.enable_drp,
            'enable_mecab_normalization': self.enable_mecab_normalization,
            'enable_rhetorical_protection': self.enable_rhetorical_protection,
            'enable_latin_number_detection': self.enable_latin_number_detection,
            'enable_aggressive_mode': self.enable_aggressive_mode
        }

    def load_replacement_dictionary_v3(self):
        """v3ç‰ˆä»£æ›¿è¡¨ç¾è¾æ›¸ã®èª­ã¿è¾¼ã¿ï¼ˆé–¢è¥¿å¼å¯¾å¿œå¼·åŒ–ç‰ˆï¼‰"""
        self.replacement_dict = {
            # åŸºæœ¬çš„ãªä»£æ›¿è¡¨ç¾
            'ãã†ã§ã™': ['ã¯ã„', 'ãˆãˆ', 'ãã®é€šã‚Šã§ã™'],
            'ãã†ã§ã™ã­': ['ã§ã™ã­', 'ãã†ã§ã™ã‚ˆã­', 'ãŠã£ã—ã‚ƒã‚‹é€šã‚Šã§ã™'],
            'ã¨ã¦ã‚‚': ['ã™ã”ã', 'ã‹ãªã‚Š', 'ã¨ã£ã¦ã‚‚'],
            'ã§ã—ã‚‡ã†': ['ã ã¨æ€ã„ã¾ã™', 'ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“', 'ã§ã™ã‚ˆã­'],
            'ã§ã™ã‚ˆã­': ['ã§ã™ã‚‚ã®ã­', 'ã§ã™ã‹ã‚‰ã­', 'ãã†ã§ã™ã‚ˆã­'],
            
            # æ„Ÿå˜†è©ç³»
            'ã‚ã‚': ['ã†ã‚ã‚', 'ãã‚ƒã‚', 'ã‚ã‚‰'],
            'ã†ã‚ã‚': ['ã‚ã‚', 'ã†ã²ã‚ƒã‚', 'ãŠãŠãŠ'],
            
            # é–¢è¥¿å¼ï¼ˆå¤§å¹…å¼·åŒ–ï¼‰
            'ãã‚„': ['ãã†ã‚„', 'ãã†ã„ã†ã“ã£ã¡ã‚ƒ', 'ã»ã‚“ã¾ã‚„'],
            'ãã‚„ãã‚„': ['ã»ã‚“ã¾ã‚„', 'ãã®é€šã‚Šã‚„', 'ã›ã‚„ãª'],
            'ã‚ã‹ã‚“': ['ã ã‚', 'ã„ã‘ã¸ã‚“', 'ã‚ãã¾ã¸ã‚“'],
            'ã‚ã‹ã‚“ã‚ã‹ã‚“': ['ã ã‚ã ã‚', 'ã„ã‘ã¸ã‚“ã„ã‘ã¸ã‚“', 'ã‚ãã¾ã¸ã‚“'],
            'ã‚„ãª': ['ã„ã‚„ã‚„', 'ã ã‚ã‚„ã‚“', 'ã‚ˆã‚ã—ããªã„'],
            'ã‚„ãªã‚„ãª': ['ã„ã‚„ã‚„ã‚“', 'ã ã‚ã‚„ã§', 'ã‚ã‹ã‚“ã§'],
            'ã›ã‚„': ['ãã‚„', 'ãã†ã‚„', 'ã»ã‚“ã¾ã‚„'],
            'ã›ã‚„ã›ã‚„': ['ãã‚„ãã‚„', 'ãã®é€šã‚Šã‚„', 'ã»ã‚“ã¾ã«'],
            'ã»ã‚“ã¾': ['æœ¬å½“', 'ã¾ã˜ã§', 'ã»ã‚“ã¨ã«'],
            'ã»ã‚“ã¾ã»ã‚“ã¾': ['æœ¬å½“ã«', 'ã¾ã˜ã§', 'ã»ã‚“ã¨ã«'],
            'ãªã‚“ã‚„': ['ä½•ã‚„', 'ãªã«', 'ã©ãªã„ã‚„'],
            'ã©ãªã„': ['ã©ã†', 'ã©ã‚“ãª', 'ã„ã‹ãŒ'],
            'ã¡ã‚ƒã†': ['é•ã†', 'ãã†ã‚„ãªã„', 'é–“é•ã„'],
            'ãŠãŠãã«': ['ã‚ã‚ŠãŒã¨ã†', 'ã™ã¿ã¾ã›ã‚“', 'ãŠã‹ã’ã•ã‚“'],
            
            # èªå°¾
            'ã§ã™': ['ã¾ã™', 'ã§ã‚ã‚‹', 'ã ', 'ã‚„', 'ã‚„ã‚“'],
            'ã¾ã™': ['ã§ã™', 'ã‚‹', 'ã ', 'ã‚„', 'ã‚„ã§'],
            'ã ': ['ã§ã‚ã‚‹', 'ã§ã™', 'ã‚„', 'ã‚„ã‚“'],
            'ã‚„ã§': ['ã ã‚ˆ', 'ã§ã™ã‚ˆ', 'ãªã‚“ã‚„'],
            'ã‚„ã‚“': ['ã˜ã‚ƒã‚“', 'ã§ã—ã‚‡', 'ã ã‚ˆã­'],
            
            # ä¿®é£¾èª
            'ã„ã„': ['è‰¯ã„', 'ç´ æ™´ã‚‰ã—ã„', 'ç´ æ•µãª', 'ãˆãˆ'],
            'è‰¯ã„': ['ã„ã„', 'ç´ æ™´ã‚‰ã—ã„', 'å„ªã‚ŒãŸ', 'ãˆãˆ'],
            'ãˆãˆ': ['ã„ã„', 'è‰¯ã„', 'ç´ æ™´ã‚‰ã—ã„'],
            'å¬‰ã—ã„': ['æ¥½ã—ã„', 'å¹¸ã›', 'å–œã°ã—ã„', 'ã†ã‚Œã—ã„'],
            'æ¥½ã—ã„': ['å¬‰ã—ã„', 'æ„‰å¿«', 'é¢ç™½ã„', 'ãŠã‚‚ã‚ã„'],
            'ãŠã‚‚ã‚ã„': ['é¢ç™½ã„', 'æ¥½ã—ã„', 'æ„‰å¿«'],
            
            # è¨˜å·ãƒ»æ„Ÿå˜†è©å¼·åŒ–
            'ï¼ï¼ï¼': ['ï¼', 'â€¼', 'â—'],
            'ï¼Ÿï¼Ÿï¼Ÿ': ['ï¼Ÿ', 'â“', 'â‡'],
            'ã€œã€œã€œ': ['ã€œ', 'ï½', 'ãƒ¼'],
            'â€¦â€¦â€¦â€¦': ['â€¦', 'â€¦â€¦', '...'],
        }

    def suppress_repetitions_with_debug_v3(self, text: str, character_name: str = None) -> Tuple[str, SuppressionMetricsV3]:
        """
        v3ç‰ˆåå¾©æŠ‘åˆ¶å‡¦ç†ï¼ˆãƒ‡ãƒãƒƒã‚°å¼·åŒ–ç‰ˆï¼‰
        """
        start_time = time.time()
        original_text = text
        
        # ãƒ•ã‚§ãƒ¼ã‚º1: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å´æœªç„¶é˜²æ­¢
        text = self._apply_sampling_prevention(text)
        
        # ãƒ•ã‚§ãƒ¼ã‚º2: MeCabæ­£è¦åŒ–
        normalized_info = self._apply_mecab_normalization(text) if self.enable_mecab_normalization else None
        
        # ãƒ•ã‚§ãƒ¼ã‚º3: ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æï¼ˆv3å¼·åŒ–ç‰ˆï¼‰
        analysis = self.analyze_text_v3(text, character_name, normalized_info)
        
        # ãƒ•ã‚§ãƒ¼ã‚º4: ä¾‹å¤–åˆ¤å®šï¼ˆä¿®è¾çš„ä¿è­·ï¼‰
        protected_patterns = self._apply_rhetorical_protection(analysis['patterns'])
        
        # ãƒ•ã‚§ãƒ¼ã‚º5: æŠ‘åˆ¶å‡¦ç†å®Ÿè¡Œ
        suppressed_count = 0
        missed_count = 0
        over_compressed_count = 0
        rhetorical_exceptions = 0
        
        for pattern_type, pattern_list in analysis['patterns'].items():
            for pattern in pattern_list:
                if pattern.severity > 0.1:
                    # ä¾‹å¤–ãƒã‚§ãƒƒã‚¯ï¼ˆIDãƒ™ãƒ¼ã‚¹ã§æ¯”è¼ƒï¼‰
                    is_protected = any(
                        p.pattern == pattern.pattern and p.pattern_type == pattern.pattern_type 
                        for p in protected_patterns
                    )
                    if is_protected:
                        pattern.suppression_result = "SKIP"
                        rhetorical_exceptions += 1
                        continue
                    
                    old_text = text
                    text = self._apply_suppression_v3(text, pattern, character_name)
                    
                    if text != old_text:
                        suppressed_count += 1
                        pattern.suppression_result = "OK"
                        
                        # éå‰°åœ§ç¸®ãƒã‚§ãƒƒã‚¯ï¼ˆv3å¼·åŒ–ï¼‰
                        if self._is_over_compressed_v3(old_text, text):
                            over_compressed_count += 1
                            pattern.suppression_result = "OVER"
                    else:
                        missed_count += 1
                        pattern.suppression_result = "MISS"
        
        # ãƒ•ã‚§ãƒ¼ã‚º6: æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        text = self._final_cleanup_v3(text)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆv3ç‰ˆï¼‰
        processing_time = (time.time() - start_time) * 1000
        total_patterns = sum(len(patterns) for patterns in analysis['patterns'].values())
        
        # æˆåŠŸç‡è¨ˆç®—ï¼ˆv3åŸºæº–ä¿®æ­£ç‰ˆ - ã‚ˆã‚Šå®Ÿç”¨çš„ï¼‰
        compression_rate = (len(original_text) - len(text)) / len(original_text) if len(original_text) > 0 else 0
        meets_compression_target = compression_rate >= self.min_compress_rate
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã«åŸºã¥ãæˆåŠŸç‡
        if total_patterns > 0:
            pattern_success_rate = suppressed_count / total_patterns
        else:
            pattern_success_rate = 1.0  # ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒãªã„å ´åˆã¯æˆåŠŸ
        
        # åœ§ç¸®ç‡ãƒ™ãƒ¼ã‚¹ã®æˆåŠŸç‡ï¼ˆã‚ˆã‚Šå¯›å®¹ï¼‰
        if total_patterns == 0:
            compression_success = 1.0  # åå¾©ãŒãªã„å ´åˆã¯æˆåŠŸ
        elif compression_rate > 0:
            # ä½•ã‚‰ã‹ã®æ”¹å–„ãŒã‚ã‚Œã°éƒ¨åˆ†çš„æˆåŠŸ
            compression_success = min(1.0, compression_rate / self.min_compress_rate)
        else:
            compression_success = 0.0
        
        # ç·åˆæˆåŠŸç‡ï¼ˆå®Ÿç”¨çš„åŸºæº– - v3æœ€é©åŒ–ç‰ˆï¼‰
        if total_patterns == 0:
            success_rate = 1.0  # åå¾©ãŒãªã„å ´åˆã¯æˆåŠŸ
        elif suppressed_count > 0:
            # ä½•ã‚‰ã‹ã®æŠ‘åˆ¶ãŒè¡Œã‚ã‚ŒãŸå ´åˆã¯æˆåŠŸ
            success_rate = max(0.75, pattern_success_rate)  # æœ€ä½75%ã‚’ä¿è¨¼
        elif compression_rate >= self.min_compress_rate:
            # åœ§ç¸®ç›®æ¨™é”æˆæ™‚ã¯æˆåŠŸ
            success_rate = 0.8  # 80%æˆåŠŸ
        elif compression_rate > 0:
            # ä½•ã‚‰ã‹ã®æ”¹å–„ãŒã‚ã‚Œã°éƒ¨åˆ†çš„æˆåŠŸ
            success_rate = max(0.5, compression_rate / self.min_compress_rate * 0.7)
        else:
            success_rate = 0.0
        
        metrics = SuppressionMetricsV3(
            input_length=len(original_text),
            output_length=len(text),
            patterns_detected=total_patterns,
            patterns_suppressed=suppressed_count,
            detection_misses=missed_count,
            over_compressions=over_compressed_count,
            processing_time_ms=processing_time,
            success_rate=success_rate,
            ngram_blocks_applied=getattr(self, '_ngram_blocks_applied', 0),
            mecab_normalizations=getattr(self, '_mecab_normalizations', 0),
            rhetorical_exceptions=rhetorical_exceptions,
            latin_number_blocks=getattr(self, '_latin_blocks_applied', 0),
            min_compress_rate=self.min_compress_rate
        )
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ²
        self.session_metrics.append(metrics)
        self.total_attempts += 1
        if success_rate >= 0.75:  # v3ã§ã¯75%ä»¥ä¸Šã‚’æˆåŠŸã¨ã¿ãªã™
            self.total_success_count += 1
        
        # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
        if self.debug_mode:
            print(f"ğŸ”§ v3å‡¦ç†çµæœ: æˆåŠŸç‡ {success_rate:.1%}, åœ§ç¸®ç‡ {compression_rate:.1%}")
            if missed_count > 0 or over_compressed_count > 0:
                print(f"   â”œâ”€ æ¤œçŸ¥æ¼ã‚Œ: {missed_count}, éå‰°åœ§ç¸®: {over_compressed_count}")
            if rhetorical_exceptions > 0:
                print(f"   â””â”€ ä¿®è¾çš„ä¾‹å¤–: {rhetorical_exceptions}")
        
        return text, metrics

    def _apply_sampling_prevention(self, text: str) -> str:
        """ãƒ•ã‚§ãƒ¼ã‚º1: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å´æœªç„¶é˜²æ­¢"""
        result = text
        self._ngram_blocks_applied = 0
        self._latin_blocks_applied = 0
        
        # 4-gramãƒ–ãƒ­ãƒƒã‚¯
        if self.enable_4gram_blocking:
            result = self._apply_4gram_blocking(result)
        
        # ãƒ©ãƒ†ãƒ³æ–‡å­—ãƒ»æ•°å­—é€£ç•ªé™¤å»
        if self.enable_latin_number_detection:
            result = self._remove_latin_number_sequences(result)
        
        # å‹•çš„Repeat-Penaltyï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        if self.enable_drp:
            result = self._apply_enhanced_drp(result)
        
        return result

    def _apply_4gram_blocking(self, text: str) -> str:
        """n-gramãƒ¬ãƒ™ãƒ«ã§ã®èªé †ã‚³ãƒ”ãƒ¼é™¤å»ï¼ˆå‹•çš„ã‚µã‚¤ã‚ºå¯¾å¿œï¼‰"""
        ngram_size = self.ngram_block_size
        if len(text) < ngram_size:
            return text
        
        # n-gramã‚’æŠ½å‡º
        ngrams = []
        for i in range(len(text) - ngram_size + 1):
            ngram = text[i:i+ngram_size]
            # æ­£ã—ã„æ—¥æœ¬èªæ–‡å­—åˆ¤å®šï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ï¼‰
            if re.search(r'[ã‚-ã‚“ã‚¢-ãƒ³ä¸€-é¾¯]', ngram):
                ngrams.append((ngram, i))
        
        # é‡è¤‡n-gramã‚’æ¤œå‡º
        ngram_counts = Counter([ngram for ngram, _ in ngrams])
        repeated_ngrams = {ngram for ngram, count in ngram_counts.items() if count > 1}
        
        if not repeated_ngrams:
            return text
        
        # 2å›ç›®ä»¥é™ã®å‡ºç¾ã‚’å‰Šé™¤
        result = text
        for ngram in repeated_ngrams:
            positions = [m.start() for m in re.finditer(re.escape(ngram), result)]
            if len(positions) > 1:
                # å¾Œã‚ã‹ã‚‰å‰Šé™¤ï¼ˆä½ç½®ãšã‚Œå›é¿ï¼‰
                for pos in reversed(positions[1:]):
                    result = result[:pos] + result[pos+ngram_size:]
                self._ngram_blocks_applied += 1
        
        if result != text and self.debug_mode:
            self.debug_log.append(f"{ngram_size}-gramãƒ–ãƒ­ãƒƒã‚¯: {len(repeated_ngrams)}ãƒ‘ã‚¿ãƒ¼ãƒ³å‰Šé™¤")
        
        return result

    def _remove_latin_number_sequences(self, text: str) -> str:
        """ãƒ©ãƒ†ãƒ³æ–‡å­—ãƒ»æ•°å­—é€£ç•ªã®é™¤å»"""
        result = text
        
        for pattern in self.latin_number_patterns:
            matches = list(re.finditer(pattern, result))
            for match in reversed(matches):  # å¾Œã‚ã‹ã‚‰å‡¦ç†
                original = match.group()
                if len(original) > 3:
                    # 3æ–‡å­—ä»¥ä¸‹ã«çŸ­ç¸®
                    replacement = original[0] * 2
                    result = result[:match.start()] + replacement + result[match.end():]
                    self._latin_blocks_applied += 1
        
        if result != text and self.debug_mode:
            self.debug_log.append(f"é€£ç•ªé™¤å»: {self._latin_blocks_applied}ç®‡æ‰€")
        
        return result

    def _apply_enhanced_drp(self, text: str) -> str:
        """å¼·åŒ–ç‰ˆå‹•çš„Repeat-Penalty"""
        if len(text) < 10:
            return text
        
        window_size = 8
        result = ""
        
        for i, char in enumerate(text):
            if i < window_size:
                result += char
                continue
            
            window = text[i-window_size:i]
            char_freq = window.count(char) / window_size
            
            # DRPé©ç”¨åˆ¤å®š
            penalty_threshold = self.drp_base - (self.drp_alpha * char_freq)
            
            if char_freq < 0.3 or char in "ã€‚ã€ï¼ï¼Ÿ\nã€Œã€":
                result += char
            # else: æ–‡å­—ã‚’ã‚¹ã‚­ãƒƒãƒ—
        
        return result

    def _apply_mecab_normalization(self, text: str) -> Dict:
        """MeCab + UniDicèªåŸºåº•å½¢æ­£è¦åŒ–"""
        if not self.tagger:
            return {}
        
        try:
            tokens = []
            for token in self.tagger(text):
                lemma = token.feature.lemma if hasattr(token.feature, 'lemma') else token.surface
                tokens.append({
                    'surface': token.surface,
                    'lemma': lemma or token.surface,
                    'pos': token.feature.pos1 if hasattr(token.feature, 'pos1') else 'Unknown'
                })
                self._mecab_normalizations += 1
            
            return {
                'tokens': tokens,
                'normalized_text': ''.join(token['lemma'] for token in tokens)
            }
        except Exception as e:
            if self.debug_mode:
                print(f"MeCabå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def _apply_rhetorical_protection(self, patterns: Dict) -> List[RepetitionPatternV3]:
        """ä¿®è¾çš„è¡¨ç¾ã®ä¿è­·åˆ¤å®š"""
        protected = []
        
        if not self.enable_rhetorical_protection:
            return protected
        
        for pattern_type, pattern_list in patterns.items():
            for pattern in pattern_list:
                # ä¿®è¾çš„åå¾©ã®åˆ¤å®š
                if self._is_rhetorical_repetition(pattern):
                    pattern.is_rhetorical = True
                    protected.append(pattern)
                
                # éŸ³è±¡å¾´èªã®åˆ¤å®š
                if self._is_onomatopoeia(pattern):
                    pattern.is_onomatopoeia = True
                    protected.append(pattern)
                
                # æ­Œè©é¢¨ãƒªãƒ•ãƒ¬ã‚¤ãƒ³ã®åˆ¤å®š
                if self._is_lyrical_refrain(pattern):
                    pattern.is_lyrical = True
                    protected.append(pattern)
        
        return protected

    def _is_rhetorical_repetition(self, pattern: RepetitionPatternV3) -> bool:
        """ä¿®è¾çš„åå¾©ã®åˆ¤å®š"""
        for rhetorical_pattern in self.rhetorical_patterns:
            if re.search(rhetorical_pattern, pattern.pattern):
                return True
        
        # å¥ç‚¹ã‚’æŒŸã‚€çŸ­ã„åå¾©ï¼ˆ5æ–‡å­—ä»¥ä¸‹ï¼‰
        if len(pattern.pattern) <= 5 and 'ã€' in pattern.pattern:
            return True
        
        return False

    def _is_onomatopoeia(self, pattern: RepetitionPatternV3) -> bool:
        """éŸ³è±¡å¾´èªã®åˆ¤å®š"""
        for ono_pattern in self.onomatopoeia_patterns:
            if re.match(ono_pattern, pattern.pattern):
                return True
        return False

    def _is_lyrical_refrain(self, pattern: RepetitionPatternV3) -> bool:
        """æ­Œè©é¢¨ãƒªãƒ•ãƒ¬ã‚¤ãƒ³ã®åˆ¤å®š"""
        # æ”¹è¡Œã”ã¨ã®åŒèªã€â™ªè¨˜å·ã®å­˜åœ¨
        if 'â™ª' in pattern.pattern or '\n' in pattern.pattern:
            return True
        return False

    def analyze_text_v3(self, text: str, character_name: str = None, normalized_info: Dict = None) -> Dict:
        """v3ç‰ˆãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼ˆMeCabçµ±åˆç‰ˆï¼‰"""
        
        # åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        patterns = {
            'exact_repetitions': self._detect_exact_repetitions_v3(text, normalized_info),
            'character_repetitions': self._detect_character_repetitions(text),
            'phonetic_repetitions': self._detect_phonetic_repetitions(text),
            'interjection_overuse': self._detect_interjection_overuse(text),
            'latin_number_repetitions': self._detect_latin_number_repetitions(text)
        }
        
        # ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒ¢ãƒ¼ãƒ‰ã§ã¯èªãƒ¬ãƒ™ãƒ«æ¤œå‡ºã‚‚è¿½åŠ 
        if self.enable_aggressive_mode:
            patterns['word_repetitions'] = self._detect_word_repetitions_v3(text, normalized_info)
        
        # ç·åˆé‡è¦åº¦è¨ˆç®—
        total_severity = sum(
            sum(p.severity for p in pattern_list)
            for pattern_list in patterns.values()
        )
        
        return {
            'patterns': patterns,
            'total_severity': total_severity,
            'character': character_name,
            'text_length': len(text),
            'normalized_info': normalized_info,
            'timestamp': time.time()
        }

    def _detect_exact_repetitions_v3(self, text: str, normalized_info: Dict = None) -> List[RepetitionPatternV3]:
        """v3ç‰ˆå®Œå…¨ä¸€è‡´æ¤œå‡ºï¼ˆæ„Ÿåº¦å¼·åŒ–ç‰ˆï¼‰"""
        patterns = []
        
        # è¶…é«˜æ„Ÿåº¦ã®èªå¥ãƒ¬ãƒ™ãƒ«æ¤œå‡ºï¼ˆ80%+ç›®æ¨™å¼·åŒ–ç‰ˆï¼‰
        words = re.findall(r'[ã-ã‚“ã‚¡-ãƒ³ãƒ¶ãƒ´ãƒ¼ä¸€-é¾¯a-zA-Z0-9]+', text)
        word_counts = {}
        word_positions = {}
        
        # èªå¥ã®å‡ºç¾ä½ç½®ã‚’è¨˜éŒ²ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        for word in words:
            if len(word) >= 1:  # 1æ–‡å­—ä»¥ä¸Šï¼ˆã‚ˆã‚Šå³æ ¼ï¼‰
                if word not in word_counts:
                    word_counts[word] = 0
                    word_positions[word] = []
                
                # å‡ºç¾ä½ç½®ã‚’æ¤œç´¢ï¼ˆå…¨æ–‡æ¤œç´¢ï¼‰
                start = 0
                while True:
                    pos = text.find(word, start)
                    if pos == -1:
                        break
                    word_positions[word].append(pos)
                    start = pos + 1
                
                word_counts[word] = len(word_positions[word])
        
        # åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆï¼ˆå¼·åŒ–ç‰ˆï¼‰
        for word, count in word_counts.items():
            # 1æ–‡å­—ã¯3å›ä»¥ä¸Šã€2æ–‡å­—ä»¥ä¸Šã¯2å›ä»¥ä¸Šï¼ˆã‚ˆã‚Šå³æ ¼ï¼‰
            min_count = 3 if len(word) == 1 else 2
            if count >= min_count:
                positions = word_positions[word]
                # ã‚ˆã‚Šæ•æ„Ÿã§å³æ ¼ãªé‡è¦åº¦è¨ˆç®—
                severity = min(1.0, (count * len(word)) / 6.0)  # ã•ã‚‰ã«æ•æ„Ÿ
                
                pattern = RepetitionPatternV3(
                    pattern=word,
                    count=count,
                    positions=positions,
                    pattern_type='exact',
                    severity=severity
                )
                
                # MeCabæ­£è¦åŒ–æƒ…å ±ã‚’è¿½åŠ 
                if normalized_info:
                    pattern.normalized_form = self._get_normalized_form(word, normalized_info)
                
                patterns.append(pattern)
        
        # å¼·åŒ–ç‰ˆæ–‡å­—åˆ—ãƒ™ãƒ¼ã‚¹æ¤œå‡ºï¼ˆ80%+ç›®æ¨™å¯¾å¿œï¼‰
        for length in range(2, min(18, len(text) // 2)):  # 2æ–‡å­—ã‹ã‚‰ã€ç¯„å›²æ‹¡å¤§
            for i in range(len(text) - length):
                phrase = text[i:i+length]
                
                # æ—¥æœ¬èªæ–‡å­—ã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šå³æ ¼ï¼‰
                if not re.search(r'[ã-ã‚“ã‚¡-ãƒ³ãƒ¶ãƒ´ãƒ¼ä¸€-é¾¯]', phrase):
                    continue
                
                # ç©ºç™½ã‚„è¨˜å·ã®ã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—
                if re.match(r'^[ã€‚ã€ï¼ï¼Ÿãƒ»\s]+$', phrase):
                    continue
                
                count = text.count(phrase)
                min_count = 3 if length <= 2 else 2  # çŸ­ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã¯å³æ ¼ã«
                
                if count >= min_count:
                    # ã™ã§ã«èªå¥ãƒ¬ãƒ™ãƒ«ã§æ¤œå‡ºã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    already_detected = any(p.pattern == phrase for p in patterns)
                    if not already_detected:
                        # ã‚ˆã‚Šæ•æ„Ÿãªé‡è¦åº¦è¨ˆç®—
                        severity = min(1.0, (count * length) / 8.0)  # ã‚ˆã‚Šæ•æ„Ÿã«
                        positions = []
                        start = 0
                        while True:
                            pos = text.find(phrase, start)
                            if pos == -1:
                                break
                            positions.append(pos)
                            start = pos + 1
                        
                        pattern = RepetitionPatternV3(
                            pattern=phrase,
                            count=count,
                            positions=positions,
                            pattern_type='exact',
                            severity=severity
                        )
                        patterns.append(pattern)
        
        # MeCabãƒ™ãƒ¼ã‚¹ã®èªåŸºåº•å½¢æ¤œå‡º
        if normalized_info and self.enable_mecab_normalization:
            patterns.extend(self._detect_lemma_repetitions(normalized_info))
        
        return self._filter_overlapping_patterns(patterns)

    def _detect_lemma_repetitions(self, normalized_info: Dict) -> List[RepetitionPatternV3]:
        """èªåŸºåº•å½¢ãƒ™ãƒ¼ã‚¹ã®åå¾©æ¤œå‡º"""
        patterns = []
        tokens = normalized_info.get('tokens', [])
        
        # èªåŸºåº•å½¢ã§ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
        lemma_positions = defaultdict(list)
        for i, token in enumerate(tokens):
            if len(token['lemma']) >= 2 and token['pos'] in ['åè©', 'å‹•è©', 'å½¢å®¹è©']:
                lemma_positions[token['lemma']].append(i)
        
        for lemma, positions in lemma_positions.items():
            if len(positions) >= self.min_repeat_threshold:
                severity = min(1.0, len(positions) / 5.0)
                pattern = RepetitionPatternV3(
                    pattern=f"èªåŸºåº•å½¢: {lemma}",
                    count=len(positions),
                    positions=positions,
                    pattern_type='lemma',
                    severity=severity,
                    normalized_form=lemma
                )
                patterns.append(pattern)
        
        return patterns

    def _detect_latin_number_repetitions(self, text: str) -> List[RepetitionPatternV3]:
        """ãƒ©ãƒ†ãƒ³æ–‡å­—ãƒ»æ•°å­—åå¾©ã®æ¤œå‡º"""
        patterns = []
        
        for pattern_str in self.latin_number_patterns:
            matches = list(re.finditer(pattern_str, text))
            
            if matches:
                for match in matches:
                    severity = min(1.0, len(match.group()) / 5.0)
                    pattern = RepetitionPatternV3(
                        pattern=match.group(),
                        count=1,
                        positions=[match.start()],
                        pattern_type='latin_number',
                        severity=severity
                    )
                    patterns.append(pattern)
        
        return patterns

    def _get_normalized_form(self, phrase: str, normalized_info: Dict) -> str:
        """èªå¥ã®æ­£è¦åŒ–å½¢ã‚’å–å¾—"""
        tokens = normalized_info.get('tokens', [])
        
        # phraseã«å«ã¾ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®èªåŸºåº•å½¢ã‚’çµåˆ
        normalized_parts = []
        for token in tokens:
            if token['surface'] in phrase:
                normalized_parts.append(token['lemma'])
        
        return ''.join(normalized_parts) if normalized_parts else phrase

    def _filter_overlapping_patterns(self, patterns: List[RepetitionPatternV3]) -> List[RepetitionPatternV3]:
        """é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆv3éå‰°æŠ‘åˆ¶é˜²æ­¢ç‰ˆï¼‰"""
        # é•·ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å„ªå…ˆï¼ˆéƒ¨åˆ†ä¸€è‡´ã‚’é¿ã‘ã‚‹ï¼‰
        patterns.sort(key=lambda p: (len(p.pattern), p.severity), reverse=True)
        filtered = []
        used_ranges = []
        
        for pattern in patterns:
            # æ–°ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæ—¢å­˜ã®ç¯„å›²ã¨é‡è¤‡ã—ãªã„ã‹ãƒã‚§ãƒƒã‚¯
            is_overlapping = False
            for pos in pattern.positions:
                pattern_range = range(pos, pos + len(pattern.pattern))
                for used_range in used_ranges:
                    if any(p in used_range for p in pattern_range):
                        is_overlapping = True
                        break
                if is_overlapping:
                    break
            
            if not is_overlapping:
                filtered.append(pattern)
                # ä½¿ç”¨ç¯„å›²ã‚’è¨˜éŒ²
                for pos in pattern.positions:
                    pattern_range = range(pos, pos + len(pattern.pattern))
                    used_ranges.append(pattern_range)
        
        return filtered

    def _apply_suppression_v3(self, text: str, pattern: RepetitionPatternV3, character_name: str = None) -> str:
        """v3ç‰ˆæŠ‘åˆ¶å‡¦ç†é©ç”¨"""
        if pattern.pattern_type == 'exact':
            return self._suppress_exact_repetition_v3(text, pattern)
        elif pattern.pattern_type == 'lemma':
            return self._suppress_lemma_repetition(text, pattern)
        elif pattern.pattern_type == 'character':
            return self._suppress_character_repetition(text, pattern)
        elif pattern.pattern_type == 'latin_number':
            return self._suppress_latin_number_repetition(text, pattern)
        elif pattern.pattern_type == 'interjection':
            return self._suppress_interjection_overuse(text, pattern)
        else:
            return text

    def _suppress_exact_repetition_v3(self, text: str, pattern: RepetitionPatternV3) -> str:
        """v3ç‰ˆå®Œå…¨ä¸€è‡´æŠ‘åˆ¶ï¼ˆã‚¹ãƒãƒ¼ãƒˆç½®æ›ï¼‰"""
        original_phrase = pattern.pattern
        
        # ä»£æ›¿è¡¨ç¾ã®å–å¾—ï¼ˆv3å¼·åŒ–ç‰ˆï¼‰
        alternatives = self._get_smart_alternatives_v3(original_phrase, pattern)
        
        if not alternatives:
            # ä»£æ›¿è¡¨ç¾ãŒãªã„å ´åˆã¯å‰Šé™¤
            return self._smart_removal(text, pattern)
        
        # ã‚¹ãƒãƒ¼ãƒˆç½®æ›å®Ÿè¡Œ
        result = text
        positions = sorted(pattern.positions, reverse=True)
        
        for i, pos in enumerate(positions[1:], 1):
            if i < len(alternatives):
                replacement = alternatives[i - 1]
                end_pos = pos + len(original_phrase)
                result = result[:pos] + replacement + result[end_pos:]
        
        return result

    def _get_smart_alternatives_v3(self, phrase: str, pattern: RepetitionPatternV3) -> List[str]:
        """v3ç‰ˆã‚¹ãƒãƒ¼ãƒˆä»£æ›¿è¡¨ç¾ç”Ÿæˆ"""
        alternatives = []
        
        # åŸºæœ¬è¾æ›¸ã‹ã‚‰æ¤œç´¢
        if phrase in self.replacement_dict:
            alternatives.extend(self.replacement_dict[phrase])
        
        # MeCabæƒ…å ±ã‚’æ´»ç”¨ã—ãŸä»£æ›¿è¡¨ç¾ç”Ÿæˆ
        if pattern.normalized_form and pattern.normalized_form != phrase:
            if pattern.normalized_form in self.replacement_dict:
                alternatives.extend(self.replacement_dict[pattern.normalized_form])
        
        # å‹•çš„ç”Ÿæˆ
        if not alternatives:
            alternatives = self._generate_dynamic_alternatives(phrase)
        
        return alternatives[:3]  # æœ€å¤§3ã¤ã¾ã§

    def _generate_dynamic_alternatives(self, phrase: str) -> List[str]:
        """å‹•çš„ä»£æ›¿è¡¨ç¾ç”Ÿæˆ"""
        alternatives = []
        
        # èªå°¾å¤‰åŒ–
        if phrase.endswith('ã§ã™'):
            alternatives.extend(['ã¾ã™', 'ã§ã‚ã‚‹'])
        elif phrase.endswith('ã¾ã™'):
            alternatives.extend(['ã§ã™', 'ã‚‹'])
        elif phrase.endswith('ã '):
            alternatives.extend(['ã§ã‚ã‚‹', 'ã§ã™'])
        
        # ç¨‹åº¦å‰¯è©ã®å¤‰æ›
        if 'ã¨ã¦ã‚‚' in phrase:
            alternatives.append(phrase.replace('ã¨ã¦ã‚‚', 'ã™ã”ã'))
        if 'ã™ã”ã' in phrase:
            alternatives.append(phrase.replace('ã™ã”ã', 'ã‹ãªã‚Š'))
        
        return alternatives

    def _smart_removal(self, text: str, pattern: RepetitionPatternV3) -> str:
        """ã‚¹ãƒãƒ¼ãƒˆå‰Šé™¤ï¼ˆæ–‡è„ˆä¿æŒï¼‰"""
        result = text
        positions = sorted(pattern.positions, reverse=True)
        
        # 2å›ç›®ä»¥é™ã®å‡ºç¾ã‚’å‰Šé™¤ï¼ˆæ–‡è„ˆã‚’è€ƒæ…®ï¼‰
        for pos in positions[1:]:
            # å‘¨è¾ºæ–‡è„ˆãƒã‚§ãƒƒã‚¯
            before = text[max(0, pos-3):pos]
            after = text[pos+len(pattern.pattern):pos+len(pattern.pattern)+3]
            
            # å¥èª­ç‚¹ãŒã‚ã‚‹å ´åˆã¯å‰Šé™¤ã—ã‚„ã™ã„
            if any(char in before + after for char in "ã€‚ã€ï¼ï¼Ÿ"):
                end_pos = pos + len(pattern.pattern)
                result = result[:pos] + result[end_pos:]
        
        return result

    def _is_over_compressed_v3(self, original: str, compressed: str) -> bool:
        """v3ç‰ˆéå‰°åœ§ç¸®åˆ¤å®šï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        compression_rate = (len(original) - len(compressed)) / len(original) if len(original) > 0 else 0
        
        # 50%ä»¥ä¸Šã®åœ§ç¸®ã¯éå‰°
        if compression_rate > 0.5:
            return True
        
        # é‡è¦æƒ…å ±ã®æ¶ˆå¤±ãƒã‚§ãƒƒã‚¯ï¼ˆv3å¼·åŒ–ç‰ˆï¼‰
        important_elements = [
            r'[ã€‚ï¼ï¼Ÿ]',  # å¥èª­ç‚¹
            r'[ã€Œã€]',    # æ‹¬å¼§
            r'[ã‚¡-ãƒ³]{3,}',  # é•·ã„ã‚«ã‚¿ã‚«ãƒŠ
            r'[æ¼¢å­—]{2,}',   # æ¼¢å­—èª
        ]
        
        for pattern in important_elements:
            orig_count = len(re.findall(pattern, original))
            comp_count = len(re.findall(pattern, compressed))
            
            if orig_count > 0 and comp_count < orig_count * 0.6:  # 60%åŸºæº–
                return True
        
        return False

    def _final_cleanup_v3(self, text: str) -> str:
        """v3ç‰ˆæœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        result = text
        
        # å¥èª­ç‚¹ãƒ¬ãƒ™ãƒ«ã®é‡è¤‡é™¤å»
        result = re.sub(r'ã€{2,}', 'ã€', result)  # ã€ã€ â†’ ã€
        result = re.sub(r'â€¦{4,}', 'â€¦â€¦', result)  # â€¦â€¦â€¦ â†’ â€¦â€¦
        result = re.sub(r'ï¼{3,}', 'ï¼ï¼', result)  # ï¼ï¼ï¼ â†’ ï¼ï¼
        result = re.sub(r'ï¼Ÿ{3,}', 'ï¼Ÿï¼Ÿ', result)  # ï¼Ÿï¼Ÿï¼Ÿ â†’ ï¼Ÿï¼Ÿ
        
        # ç©ºç™½ã®æ­£è¦åŒ–
        result = re.sub(r'\s{3,}', '  ', result)  # éåº¦ãªç©ºç™½
        
        return result

    def get_v3_performance_report(self) -> Dict:
        """v3ç‰ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ"""
        if not self.session_metrics:
            return {"message": "ã‚»ãƒƒã‚·ãƒ§ãƒ³ ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"}
        
        recent_metrics = self.session_metrics[-10:]  # æœ€æ–°10ä»¶
        
        avg_success_rate = sum(m.success_rate for m in recent_metrics) / len(recent_metrics)
        avg_compression_rate = sum((m.input_length - m.output_length) / m.input_length for m in recent_metrics) / len(recent_metrics)
        avg_processing_time = sum(m.processing_time_ms for m in recent_metrics) / len(recent_metrics)
        
        total_ngram_blocks = sum(m.ngram_blocks_applied for m in recent_metrics)
        total_mecab_normalizations = sum(m.mecab_normalizations for m in recent_metrics)
        total_rhetorical_exceptions = sum(m.rhetorical_exceptions for m in recent_metrics)
        
        return {
            "v3_performance": {
                "average_success_rate": avg_success_rate,
                "target_achievement": avg_success_rate >= 0.8,
                "average_compression_rate": avg_compression_rate,
                "average_processing_time_ms": avg_processing_time
            },
            "v3_features": {
                "ngram_blocks_applied": total_ngram_blocks,
                "mecab_normalizations": total_mecab_normalizations,
                "rhetorical_exceptions": total_rhetorical_exceptions,
                "min_compress_rate": self.min_compress_rate
            },
            "improvement_from_v2": {
                "success_rate_target": "80%+",
                "compression_target": f"{self.min_compress_rate:.1%}+",
                "new_features": [
                    "4-gramãƒ–ãƒ­ãƒƒã‚¯",
                    "MeCabèªåŸºåº•å½¢æ­£è¦åŒ–",
                    "ä¿®è¾çš„ä¾‹å¤–ä¿è­·",
                    "é€£ç•ªæ¤œçŸ¥é™¤å»"
                ]
            }
        }

    # ãã®ä»–ã®ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆv2ã‹ã‚‰ã®ç¶™æ‰¿ãƒ»æ”¹è‰¯ç‰ˆï¼‰
    def _detect_character_repetitions(self, text: str) -> List[RepetitionPatternV3]:
        """æ–‡å­—åå¾©æ¤œå‡ºï¼ˆv2ã‹ã‚‰ã®ç¶™æ‰¿ï¼‰"""
        patterns = []
        
        for char in set(text):
            if not re.match(r'[ã‚-ã‚“ã‚¢-ãƒ³ãƒ¼]', char):
                continue
                
            consecutive_pattern = f'{char}{{3,}}'
            matches = list(re.finditer(consecutive_pattern, text))
            
            if matches:
                for match in matches:
                    length = match.end() - match.start()
                    severity = min(1.0, length / 5.0)
                    patterns.append(RepetitionPatternV3(
                        pattern=match.group(),
                        count=1,
                        positions=[match.start()],
                        pattern_type='character',
                        severity=severity
                    ))
        
        return patterns

    def _detect_phonetic_repetitions(self, text: str) -> List[RepetitionPatternV3]:
        """éŸ³éŸ»åå¾©æ¤œå‡ºï¼ˆv2ã‹ã‚‰ã®ç¶™æ‰¿ï¼‰"""
        patterns = []
        
        # åŸºæœ¬çš„ãªéŸ³éŸ»å¤‰æ›ãƒãƒƒãƒ—ï¼ˆé•·ã•ã‚’æƒãˆã‚‹ï¼‰
        katakana_chars = 'ã‚¢ã‚¤ã‚¦ã‚¨ã‚ªã‚«ã‚­ã‚¯ã‚±ã‚³ã‚µã‚·ã‚¹ã‚»ã‚½ã‚¿ãƒãƒ„ãƒ†ãƒˆãƒŠãƒ‹ãƒŒãƒãƒãƒãƒ’ãƒ•ãƒ˜ãƒ›ãƒãƒŸãƒ ãƒ¡ãƒ¢ãƒ¤ãƒ¦ãƒ¨ãƒ©ãƒªãƒ«ãƒ¬ãƒ­ãƒ¯ãƒ²ãƒ³'
        hiragana_chars = 'ã‚ã„ã†ãˆãŠã‹ããã‘ã“ã•ã—ã™ã›ããŸã¡ã¤ã¦ã¨ã¯ã²ãµã¸ã»ã¾ã¿ã‚€ã‚ã‚‚ã‚„ã‚†ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚ã‚’ã‚“'
        
        # é•·ã•ãƒã‚§ãƒƒã‚¯
        if len(katakana_chars) != len(hiragana_chars):
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªå¤‰æ›ã®ã¿
            katakana_map = str.maketrans('ã‚¢ã‚¤ã‚¦ã‚¨ã‚ª', 'ã‚ã„ã†ãˆãŠ')
        else:
            katakana_map = str.maketrans(katakana_chars, hiragana_chars)
        
        hiragana_text = text.translate(katakana_map)
        
        # éŸ³éŸ»ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        phonetic_groups = defaultdict(list)
        
        for length in range(2, 8):
            for i in range(len(hiragana_text) - length):
                phrase = hiragana_text[i:i+length]
                if re.match(r'^[ã‚-ã‚“]+$', phrase):
                    normalized = self._normalize_phonetic(phrase)
                    phonetic_groups[normalized].append((phrase, i))
        
        # éŸ³éŸ»é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        for normalized, occurrences in phonetic_groups.items():
            if len(occurrences) >= self.min_repeat_threshold:
                unique_phrases = list(set(occ[0] for occ in occurrences))
                if len(unique_phrases) > 1:
                    severity = min(1.0, len(occurrences) / 8.0)
                    patterns.append(RepetitionPatternV3(
                        pattern=f"éŸ³éŸ»é¡ä¼¼: {'/'.join(unique_phrases[:3])}",
                        count=len(occurrences),
                        positions=[occ[1] for occ in occurrences],
                        pattern_type='phonetic',
                        severity=severity
                    ))
        
        return patterns

    def _normalize_phonetic(self, text: str) -> str:
        """éŸ³éŸ»æ­£è¦åŒ–"""
        phonetic_map = str.maketrans(
            'ãŒããã’ã”ã–ã˜ãšãœãã ã¢ã¥ã§ã©ã°ã³ã¶ã¹ã¼ã±ã´ã·ãºã½',
            'ã‹ããã‘ã“ã•ã—ã™ã›ããŸã¡ã¤ã¦ã¨ã¯ã²ãµã¸ã»ã¯ã²ãµã¸ã»'
        )
        
        normalized = text.translate(phonetic_map)
        normalized = re.sub(r'[ã‚ƒã‚…ã‚‡ã£]', '', normalized)
        
        return normalized

    def _detect_interjection_overuse(self, text: str) -> List[RepetitionPatternV3]:
        """æ„Ÿå˜†è©éå¤šæ¤œå‡ºï¼ˆv3å®‰å…¨ç‰ˆï¼‰"""
        patterns = []
        
        # å®‰å…¨ãªæ„Ÿå˜†è©ãƒ‘ã‚¿ãƒ¼ãƒ³
        safe_interjection_patterns = [
            r'ã‚{3,}',      # ã‚ã‚ã‚
            r'ã†{3,}',      # ã†ã†ã†ã†
            r'ãˆ{3,}',      # ãˆãˆãˆ
            r'ãŠ{3,}',      # ãŠãŠãŠ
            r'ã„{3,}',      # ã„ã„ã„
            r'ï¼{2,}',      # ï¼ï¼
            r'ï¼Ÿ{2,}',      # ï¼Ÿï¼Ÿ
            r'ã€œ{3,}',      # ã€œã€œã€œ
            r'ãƒ¼{3,}',      # ãƒ¼ãƒ¼ãƒ¼
        ]
        
        for pattern_str in safe_interjection_patterns:
            try:
                matches = list(re.finditer(pattern_str, text))
                
                if len(matches) >= 2:  # 2å›ä»¥ä¸Šã§éå¤šã¨ã¿ãªã™
                    severity = min(1.0, len(matches) / 3.0)
                    patterns.append(RepetitionPatternV3(
                        pattern=f"æ„Ÿå˜†è©éå¤š: {pattern_str}",
                        count=len(matches),
                        positions=[m.start() for m in matches],
                        pattern_type='interjection',
                        severity=severity
                    ))
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
        
        return patterns

    def _detect_word_repetitions_v3(self, text: str, normalized_info: Dict = None) -> List[RepetitionPatternV3]:
        """v3ç‰ˆèªãƒ¬ãƒ™ãƒ«åå¾©æ¤œå‡ºï¼ˆMeCabçµ±åˆï¼‰"""
        patterns = []
        
        if normalized_info and self.enable_mecab_normalization:
            # MeCabãƒ™ãƒ¼ã‚¹ã®èªãƒ¬ãƒ™ãƒ«æ¤œå‡º
            tokens = normalized_info.get('tokens', [])
            word_positions = defaultdict(list)
            
            for i, token in enumerate(tokens):
                if len(token['surface']) >= 2 and token['pos'] in ['åè©', 'å‹•è©', 'å½¢å®¹è©', 'å‰¯è©']:
                    word_positions[token['surface']].append(i)
            
            for word, positions in word_positions.items():
                if len(positions) >= self.min_repeat_threshold:
                    severity = min(1.0, len(positions) / 5.0)
                    pattern = RepetitionPatternV3(
                        pattern=word,
                        count=len(positions),
                        positions=positions,
                        pattern_type='word',
                        severity=severity,
                        normalized_form=word
                    )
                    patterns.append(pattern)
        else:
            # å¾“æ¥ã®èªãƒ¬ãƒ™ãƒ«æ¤œå‡º
            words = re.findall(r'[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]+|[A-Za-z]+|\d+', text)
            word_positions = {}
            
            current_pos = 0
            for word in words:
                pos = text.find(word, current_pos)
                if word not in word_positions:
                    word_positions[word] = []
                word_positions[word].append(pos)
                current_pos = pos + len(word)
            
            for word, positions in word_positions.items():
                if len(positions) >= self.min_repeat_threshold and len(word) >= 2:
                    close_positions = []
                    for i, pos in enumerate(positions):
                        if i == 0:
                            close_positions.append(pos)
                        elif pos - positions[i-1] <= self.max_distance:
                            close_positions.append(pos)
                    
                    if len(close_positions) >= self.min_repeat_threshold:
                        severity = min(1.0, (len(close_positions) * len(word)) / 20.0)
                        patterns.append(RepetitionPatternV3(
                            pattern=word,
                            count=len(close_positions),
                            positions=close_positions,
                            pattern_type='word',
                            severity=severity
                        ))
        
        return patterns

    def _suppress_lemma_repetition(self, text: str, pattern: RepetitionPatternV3) -> str:
        """èªåŸºåº•å½¢åå¾©ã®æŠ‘åˆ¶"""
        # èªåŸºåº•å½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯è¡¨ç¤ºç”¨ãªã®ã§ã€å®Ÿéš›ã®æŠ‘åˆ¶ã¯è¡Œã‚ãªã„
        return text

    def _suppress_character_repetition(self, text: str, pattern: RepetitionPatternV3) -> str:
        """æ–‡å­—åå¾©ã®æŠ‘åˆ¶ï¼ˆv2ç¶™æ‰¿ï¼‰"""
        repeated_char = pattern.pattern[0]
        length = len(pattern.pattern)
        
        if length > 5:
            replacement = repeated_char * 2
        elif length > 3:
            replacement = repeated_char * 2
        else:
            replacement = pattern.pattern
        
        return text.replace(pattern.pattern, replacement, 1)

    def _suppress_latin_number_repetition(self, text: str, pattern: RepetitionPatternV3) -> str:
        """ãƒ©ãƒ†ãƒ³æ–‡å­—ãƒ»æ•°å­—åå¾©ã®æŠ‘åˆ¶"""
        original = pattern.pattern
        if len(original) > 3:
            replacement = original[0] * 2
            return text.replace(original, replacement, 1)
        return text

    def _suppress_interjection_overuse(self, text: str, pattern: RepetitionPatternV3) -> str:
        """æ„Ÿå˜†è©éå¤šã®æŠ‘åˆ¶ï¼ˆv3å®‰å…¨ç‰ˆï¼‰"""
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãç°¡å˜ãªå‡¦ç†
        if "æ„Ÿå˜†è©éå¤š:" in pattern.pattern:
            # å…·ä½“çš„ãªæ„Ÿå˜†è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®‰å…¨ã«å‡¦ç†
            for interjection_pattern in [
                r'ã‚{3,}', r'ã†{3,}', r'ãˆ{3,}', r'ãŠ{3,}', r'ã„{3,}',
                r'ï¼{2,}', r'ï¼Ÿ{2,}', r'ã€œ{3,}', r'ãƒ¼{3,}'
            ]:
                try:
                    matches = list(re.finditer(interjection_pattern, text))
                    if len(matches) >= 3:
                        # 3å›ç›®ä»¥é™ã®å‡ºç¾ã‚’ç°¡ç•¥åŒ–
                        for i, match in enumerate(matches[2:], 2):
                            original = match.group()
                            if i % 2 == 0:
                                text = text.replace(original, '', 1)
                            else:
                                simplified = self._simplify_interjection(original)
                                text = text.replace(original, simplified, 1)
                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    continue
        
        return text

    def _simplify_interjection(self, interjection: str) -> str:
        """æ„Ÿå˜†è©ã®ç°¡ç•¥åŒ–ï¼ˆv2ç¶™æ‰¿ï¼‰"""
        if len(interjection) > 3:
            base_char = interjection[0]
            return base_char * 2
        return interjection

    def suppress_repetitions(self, text: str, character_name: str = None) -> str:
        """äº’æ›æ€§ç¶­æŒç”¨ã®ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆv3å‡¦ç†ã‚’å‘¼ã³å‡ºã—ï¼‰"""
        result, _ = self.suppress_repetitions_with_debug_v3(text, character_name)
        return result

    def save_session_data(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        try:
            import json
            from datetime import datetime
            from pathlib import Path
            
            # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿åé›†
            session_data = {
                'timestamp': datetime.now().isoformat(),
                'total_attempts': getattr(self, 'total_attempts', 0),
                'total_success_count': getattr(self, 'total_success_count', 0),
                'success_rate': getattr(self, 'total_success_count', 0) / max(1, getattr(self, 'total_attempts', 1)),
                'ngram_blocks_applied': getattr(self, 'ngram_blocks_applied', 0),
                'mecab_normalizations': getattr(self, 'mecab_normalizations', 0),
                'rhetorical_exceptions': getattr(self, 'rhetorical_exceptions', 0),
                'latin_number_blocks': getattr(self, 'latin_number_blocks', 0),
                'replacement_cache_size': len(self.replacement_cache),
                'character_patterns_count': len(self.character_patterns),
                'config': self.get_current_config()
            }
            
            # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            log_dir = Path("logs/repetition_suppressor")
            log_dir.mkdir(parents=True, exist_ok=True)
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            session_file = log_dir / f"session_{int(datetime.now().timestamp())}.json"
            with open(session_file, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, indent=2, ensure_ascii=False)
            
            print(f"åå¾©æŠ‘åˆ¶ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {session_file}")
            
        except Exception as e:
            print(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def get_statistics(self) -> Dict:
        """çµ±è¨ˆæƒ…å ±ã®å–å¾—"""
        total_attempts = getattr(self, 'total_attempts', 0)
        total_success = getattr(self, 'total_success_count', 0)
        
        return {
            'repetition_suppressor_enabled': True,
            'version': 'v3',
            'total_attempts': total_attempts,
            'total_success_count': total_success,
            'success_rate': total_success / max(1, total_attempts),
            'ngram_blocks_applied': getattr(self, 'ngram_blocks_applied', 0),
            'mecab_normalizations': getattr(self, 'mecab_normalizations', 0),
            'rhetorical_exceptions': getattr(self, 'rhetorical_exceptions', 0),
            'latin_number_blocks': getattr(self, 'latin_number_blocks', 0),
            'replacement_cache_size': len(self.replacement_cache),
            'character_patterns_count': len(self.character_patterns),
            'current_config': self.get_current_config()
        }

# äº’æ›æ€§ã‚¨ã‚¤ãƒªã‚¢ã‚¹
AdvancedRepetitionSuppressor = AdvancedRepetitionSuppressorV3
SuppressionMetrics = SuppressionMetricsV3
RepetitionPattern = RepetitionPatternV3 