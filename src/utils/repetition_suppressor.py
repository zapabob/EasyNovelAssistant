# -*- coding: utf-8 -*-
"""
é«˜åº¦èªå¥åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v2 (Advanced Repetition Suppression System v2)
åŒã˜èªå¥ã®åå¾©ã‚’æ¤œå‡ºã—ã€è‡ªç„¶ãªè¡¨ç¾ã«ä¿®æ­£ã™ã‚‹ - æˆåŠŸç‡80%+ã‚’ç›®æŒ‡ã™æ”¹è‰¯ç‰ˆ
"""

import re
import time
import json
import os
from typing import List, Dict, Tuple, Optional, Set
from collections import defaultdict, Counter
from dataclasses import dataclass
import numpy as np
from tqdm import tqdm


@dataclass
class RepetitionPattern:
    """åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æçµæœ"""
    pattern: str
    count: int
    positions: List[int]
    pattern_type: str  # 'exact', 'similar', 'phonetic'
    severity: float  # 0.0-1.0ã®é‡è¦åº¦
    similarity_score: float = 0.0  # ãƒ‡ãƒãƒƒã‚°ç”¨é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
    suppression_result: str = ""  # æŠ‘åˆ¶çµæœã‚¿ã‚°ï¼ˆMISS/OVER/OKï¼‰


@dataclass 
class SuppressionMetrics:
    """æŠ‘åˆ¶åŠ¹æœãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    input_length: int
    output_length: int
    patterns_detected: int
    patterns_suppressed: int
    detection_misses: int
    over_compressions: int
    processing_time_ms: float
    success_rate: float


class AdvancedRepetitionSuppressor:
    """
    é«˜åº¦èªå¥åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ  v2
    
    æ–°æ©Ÿèƒ½ v2:
    1. ãƒ‡ãƒãƒƒã‚°å¼·åŒ–ï¼ˆMISS/OVERæ¤œå‡ºï¼‰
    2. n-gramãƒã‚¹ã‚¯ä½µç”¨
    3. å‹•çš„Repeat-Penalty
    4. MeCab/UniDicçµ±åˆæº–å‚™
    5. æˆåŠŸç‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©³ç´°åŒ–
    """
    
    def __init__(self, config: Dict = None):
        self.config = config or {}
        
        # åŸºæœ¬è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.min_repeat_threshold = self.config.get('min_repeat_threshold', 1)  # åŒèªåå¾©å¯¾å¿œã§å³æ ¼åŒ–
        self.max_distance = self.config.get('max_distance', 30)  # æ¤œå‡ºè·é›¢çŸ­ç¸®
        self.similarity_threshold = self.config.get('similarity_threshold', 0.68)  # é–¾å€¤ä¸‹ã’ã‚‹
        self.phonetic_threshold = self.config.get('phonetic_threshold', 0.8)  # éŸ³éŸ»é–¾å€¤ä¸‹ã’ã‚‹
        
        # åŒèªåå¾©å¯¾å¿œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆv2å¼·åŒ–ï¼‰
        self.enable_aggressive_mode = self.config.get('enable_aggressive_mode', True)
        self.interjection_sensitivity = self.config.get('interjection_sensitivity', 0.5)  # æ„Ÿåº¦ä¸Šã’ã‚‹
        self.exact_match_priority = self.config.get('exact_match_priority', True)
        self.character_repetition_limit = self.config.get('character_repetition_limit', 3)  # åˆ¶é™å³æ ¼åŒ–
        
        # v2æ–°æ©Ÿèƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.debug_mode = self.config.get('debug_mode', True)
        self.ngram_block_size = self.config.get('ngram_block_size', 4)  # 4-gramä»¥ä¸Šã‚’ãƒ–ãƒ­ãƒƒã‚¯
        self.enable_drp = self.config.get('enable_drp', True)  # å‹•çš„Repeat-Penalty
        self.drp_alpha = self.config.get('drp_alpha', 0.5)
        self.drp_window = self.config.get('drp_window', 10)
        
        # è¨€èªå‡¦ç†å¼·åŒ–æº–å‚™
        self.use_mecab = self.config.get('use_mecab', False)
        self.use_jaccard_similarity = self.config.get('use_jaccard_similarity', True)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
        self.session_metrics = []
        self.total_success_count = 0
        self.total_attempts = 0
        
        # åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æå±¥æ­´
        self.pattern_history = []
        self.character_patterns = defaultdict(list)
        self.replacement_cache = {}
        
        # æ—¥æœ¬èªå›ºæœ‰ã®å‡¦ç†
        self.katakana_map = str.maketrans(
            'ã‚¢ã‚¤ã‚¦ã‚¨ã‚ªã‚«ã‚­ã‚¯ã‚±ã‚³ã‚µã‚·ã‚¹ã‚»ã‚½ã‚¿ãƒãƒ„ãƒ†ãƒˆãƒŠãƒ‹ãƒŒãƒãƒãƒãƒ’ãƒ•ãƒ˜ãƒ›ãƒãƒŸãƒ ãƒ¡ãƒ¢ãƒ¤ãƒ¦ãƒ¨ãƒ©ãƒªãƒ«ãƒ¬ãƒ­ãƒ¯ãƒ²ãƒ³',
            'ã‚ã„ã†ãˆãŠã‹ããã‘ã“ã•ã—ã™ã›ããŸã¡ã¤ã¦ã¨ãªã«ã¬ã­ã®ã¯ã²ãµã¸ã»ã¾ã¿ã‚€ã‚ã‚‚ã‚„ã‚†ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚ã‚’ã‚“'
        )
        
        # æ„Ÿå˜†è©ãƒ»é–“æŠ•è©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆv2æ‹¡å¼µï¼‰
        self.interjection_patterns = [
            r'([ã‚ãŠã†ãˆã„])\1{2,}',  # ã‚ã‚ã‚, ãŠãŠãŠç­‰
            r'([ãã²ã‚“ã—ã¡])\1{1,}ã£*',  # ãã, ã²ã²ç­‰
            r'([ã£ãƒ¼ã€œ]){3,}',  # ã£ã£ã£, ãƒ¼ãƒ¼ãƒ¼ç­‰
            r'([ï¼ï¼Ÿ]){2,}',  # !!!, ???ç­‰
            r'(ãã‚„|ã ã‚|ã„ã‚„){2,}',  # ãã‚„ãã‚„ç­‰é–¢è¥¿å¼å¯¾å¿œ
            r'([ã‚-ã‚“])\1{2,}[ã£ã€œâ€¦ï¼ï¼Ÿ]*'  # ä¸€èˆ¬çš„ãªé‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³
        ]
        
        # ä»£æ›¿è¡¨ç¾è¾æ›¸ï¼ˆv2æ‹¡å¼µç‰ˆï¼‰
        self.load_replacement_dictionary()
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¨­å®š
        self.debug_log = []
        
        # n-gramãƒ–ãƒ­ãƒƒã‚¯ç”¨è¾æ›¸
        self.ngram_cache = set()
        
        mode_text = "v2ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒ¢ãƒ¼ãƒ‰" if self.enable_aggressive_mode else "v2æ¨™æº–ãƒ¢ãƒ¼ãƒ‰"
        features = []
        if self.debug_mode:
            features.append("ãƒ‡ãƒãƒƒã‚°å¼·åŒ–")
        if self.ngram_block_size > 0:
            features.append(f"{self.ngram_block_size}-gramãƒ–ãƒ­ãƒƒã‚¯")
        if self.enable_drp:
            features.append("DRP")
        
        feature_text = "(" + ", ".join(features) + ")" if features else ""
        print(f"ğŸ”„ é«˜åº¦èªå¥åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ v2ãŒåˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸï¼ˆ{mode_text}ï¼‰{feature_text}")

    def load_replacement_dictionary(self):
        """ä»£æ›¿è¡¨ç¾è¾æ›¸ã®èª­ã¿è¾¼ã¿ï¼ˆv2æ‹¡å¼µç‰ˆï¼‰"""
        self.replacement_dict = {
            # æ„Ÿå˜†è¡¨ç¾ï¼ˆæ‹¡å¼µï¼‰
            'ã‚ã‚ã‚': ['ã‚ã‚', 'ã¯ã‚', 'ã†ã†', 'ãµã†'],
            'ã‚ã‚ã‚ã‚': ['ã‚ã‚', 'ã¯ã‚ã£', 'ãµã†ã†'],
            'ã‚ã‚ã‚ã‚ã‚': ['ã‚ã‚ã£', 'ã¯ã‚ã£'],
            'ã†ã†ã†ã†': ['ã†ã†', 'ã‚“ã‚“', 'ãµã†'],
            'ãŠãŠãŠãŠ': ['ãŠãŠ', 'ã‚ã‚', 'ã»ãŠ'],
            'ãã‚ƒã‚ã‚': ['ãã‚ƒã‚', 'ã„ã‚„', 'ã‚ã‚'],
            'ã²ã„ã„ã„': ['ã²ã„', 'ã„ã‚„', 'ã†ã†'],
            
            # æ¿éŸ³ãƒ»æ‹—éŸ³åå¾©ï¼ˆæ‹¡å¼µï¼‰
            'ã£ã£ã£': ['ã£', 'ã‚“', ''],
            'ã£ã£ã£ã£': ['ã£', ''],
            'ã€œã€œã€œ': ['ã€œ', 'â€¦', ''],
            'ã€œã€œã€œã€œ': ['ã€œ', ''],
            'â€¦â€¦â€¦â€¦': ['â€¦', 'ã€œ', ''],
            'â€¦â€¦â€¦â€¦â€¦â€¦': ['â€¦', ''],
            'ï¼ï¼ï¼ï¼': ['ï¼', 'ï¼ï¼Ÿ', ''],
            'ï¼ï¼ï¼ï¼ï¼': ['ï¼ï¼', ''],
            'ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ': ['ï¼Ÿ', 'ï¼ï¼Ÿ', ''],
            
            # èªå°¾åå¾©ï¼ˆæ‹¡å¼µï¼‰
            'ã§ã™ã§ã™': ['ã§ã™', 'ã§ã™ã­', 'ã§ã™ã‚ˆ'],
            'ã¾ã™ã¾ã™': ['ã¾ã™', 'ã¾ã™ã­', 'ã¾ã™ã‚ˆ'],
            'ã ã£ãŸã ã£ãŸ': ['ã ã£ãŸ', 'ã§ã—ãŸ', 'ã ã£ãŸã­'],
            'ã§ã—ã‚‡ã§ã—ã‚‡': ['ã§ã—ã‚‡ã†', 'ã§ã™ã­', 'ã§ã—ã‚‡ã†ã­'],
            'ã®ã§ã™ã®ã§ã™': ['ã®ã§ã™', 'ã‚“ã§ã™', 'ã®ã§ã™ã‚ˆ'],
            
            # æ¥ç¶šè©åå¾©ï¼ˆæ‹¡å¼µï¼‰
            'ãã—ã¦ãã—ã¦': ['ãã—ã¦', 'ã¾ãŸ', 'ãã‚Œã‹ã‚‰'],
            'ã§ã‚‚ã§ã‚‚': ['ã§ã‚‚', 'ã—ã‹ã—', 'ã ã‘ã©'],
            'ã ã‹ã‚‰ã ã‹ã‚‰': ['ã ã‹ã‚‰', 'ãã‚Œã§', 'ãªã®ã§'],
            'ãã‚Œã§ãã‚Œã§': ['ãã‚Œã§', 'ã ã‹ã‚‰', 'ãã—ã¦'],
            'ã¤ã¾ã‚Šã¤ã¾ã‚Š': ['ã¤ã¾ã‚Š', 'ã™ãªã‚ã¡', 'è¦ã™ã‚‹ã«'],
            
            # é–¢è¥¿å¼å¯¾å¿œ
            'ãã‚„ãã‚„': ['ãã‚„', 'ã»ã‚“ã¾', 'ã›ã‚„ãª'],
            'ã‚„ãªã‚„ãª': ['ã‚„ãª', 'ã„ã‚„ã‚„ãª', 'ã‚ã‹ã‚“ãª'],
            'ã‚ã‹ã‚“ã‚ã‹ã‚“': ['ã‚ã‹ã‚“', 'ã ã‚', 'ã„ã‚„'],
            
            # æ„Ÿæƒ…è¡¨ç¾ï¼ˆæ‹¡å¼µï¼‰
            'å¬‰ã—ã„å¬‰ã—ã„': ['ã¨ã¦ã‚‚å¬‰ã—ã„', 'æœ¬å½“ã«å¬‰ã—ã„', 'ã™ã”ãå¬‰ã—ã„'],
            'æ‚²ã—ã„æ‚²ã—ã„': ['ã¨ã¦ã‚‚æ‚²ã—ã„', 'æœ¬å½“ã«æ‚²ã—ã„', 'ã™ã”ãæ‚²ã—ã„'],
            'æ€–ã„æ€–ã„': ['ã¨ã¦ã‚‚æ€–ã„', 'æœ¬å½“ã«æ€–ã„', 'ã™ã”ãæ€–ã„'],
            'ç—›ã„ç—›ã„': ['ã¨ã¦ã‚‚ç—›ã„', 'æœ¬å½“ã«ç—›ã„', 'ã™ã”ãç—›ã„'],
            
            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å›ºæœ‰è¡¨ç¾ï¼ˆæ‹¡å¼µï¼‰
            'ã‚ªã‚¹æ§˜ã‚ªã‚¹æ§˜': ['ã‚ªã‚¹æ§˜', 'ä¸»äººæ§˜', 'ã”ä¸»äººæ§˜'],
            'ãŠå…„ã¡ã‚ƒã‚“ãŠå…„ã¡ã‚ƒã‚“': ['ãŠå…„ã¡ã‚ƒã‚“', 'ãŠã«ã„ã¡ã‚ƒã‚“'],
            'ãŠå§‰ã¡ã‚ƒã‚“ãŠå§‰ã¡ã‚ƒã‚“': ['ãŠå§‰ã¡ã‚ƒã‚“', 'ãŠã­ãˆã¡ã‚ƒã‚“'],
            
            # é »å‡ºåå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³
            'ã¯ã„ã¯ã„': ['ã¯ã„', 'ãˆãˆ', 'ãã†ã§ã™'],
            'ã„ãˆã„ãˆ': ['ã„ãˆ', 'ã„ã„ãˆ', 'ãã‚“ãª'],
            'ã»ã‚“ã¨ã»ã‚“ã¨': ['ã»ã‚“ã¨', 'æœ¬å½“ã«', 'ãƒã‚¸ã§']
        }

    def suppress_repetitions_with_debug(self, text: str, character_name: str = None) -> Tuple[str, SuppressionMetrics]:
        """
        åå¾©æŠ‘åˆ¶å‡¦ç†ï¼ˆãƒ‡ãƒãƒƒã‚°å¼·åŒ–ç‰ˆï¼‰
        
        Returns:
            (å‡¦ç†æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ, è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹)
        """
        start_time = time.time()
        original_text = text
        
        # åˆ†æãƒ•ã‚§ãƒ¼ã‚º
        analysis = self.analyze_text(text, character_name)
        
        # n-gramãƒ–ãƒ­ãƒƒã‚¯å‰å‡¦ç†
        if self.ngram_block_size > 0:
            text = self._apply_ngram_blocking(text)
        
        # å‹•çš„Repeat-Penaltyé©ç”¨
        if self.enable_drp:
            text = self._apply_dynamic_repeat_penalty(text)
        
        # å¾“æ¥ã®æŠ‘åˆ¶å‡¦ç†
        patterns = []
        for pattern_type, pattern_list in analysis['patterns'].items():
            patterns.extend(pattern_list)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
        patterns.sort(key=lambda p: p.severity, reverse=True)
        
        suppressed_count = 0
        missed_count = 0
        over_compressed_count = 0
        
        for pattern in patterns:
            if pattern.severity > 0.1:  # é–¾å€¤ä»¥ä¸Šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿å‡¦ç†
                old_text = text
                text = self._apply_suppression_with_debug(text, pattern, character_name)
                
                if text != old_text:
                    suppressed_count += 1
                    pattern.suppression_result = "OK"
                    
                    # éå‰°åœ§ç¸®ãƒã‚§ãƒƒã‚¯
                    if self._is_over_compressed(old_text, text):
                        over_compressed_count += 1
                        pattern.suppression_result = "OVER"
                        if self.debug_mode:
                            self.debug_log.append(f"<OVER> {pattern.pattern} -> éå‰°åœ§ç¸®æ¤œå‡º")
                else:
                    missed_count += 1
                    pattern.suppression_result = "MISS"
                    if self.debug_mode:
                        self.debug_log.append(f"<MISS> {pattern.pattern} -> æŠ‘åˆ¶å¤±æ•— (sim={pattern.similarity_score:.2f} < {self.similarity_threshold:.2f})")
        
        # æœ€çµ‚çš„ãªéš£æ¥é‡è¤‡é™¤å»
        text = self._remove_adjacent_duplicates_v2(text)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        processing_time = (time.time() - start_time) * 1000
        total_patterns = len(patterns)
        success_rate = (suppressed_count / total_patterns) if total_patterns > 0 else 1.0
        
        metrics = SuppressionMetrics(
            input_length=len(original_text),
            output_length=len(text),
            patterns_detected=total_patterns,
            patterns_suppressed=suppressed_count,
            detection_misses=missed_count,
            over_compressions=over_compressed_count,
            processing_time_ms=processing_time,
            success_rate=success_rate
        )
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
        self.session_metrics.append(metrics)
        self.total_attempts += 1
        if success_rate >= 0.8:
            self.total_success_count += 1
        
        # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
        if self.debug_mode and (missed_count > 0 or over_compressed_count > 0):
            print(f"ğŸ› ãƒ‡ãƒãƒƒã‚°: æ¤œçŸ¥æ¼ã‚Œ {missed_count}ä»¶, éå‰°åœ§ç¸® {over_compressed_count}ä»¶")
            print(f"    æˆåŠŸç‡: {success_rate:.1%}, å‡¦ç†æ™‚é–“: {processing_time:.1f}ms")
        
        return text, metrics

    def _apply_ngram_blocking(self, text: str) -> str:
        """n-gramãƒ¬ãƒ™ãƒ«ã§ã®åå¾©ãƒ–ãƒ­ãƒƒã‚¯"""
        if self.ngram_block_size <= 0:
            return text
        
        # n-gramã‚’æŠ½å‡ºã—ã¦ãƒ–ãƒ­ãƒƒã‚¯å¯¾è±¡ã‚’ç‰¹å®š
        ngrams = []
        for i in range(len(text) - self.ngram_block_size + 1):
            ngram = text[i:i + self.ngram_block_size]
            if re.search(r'[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]', ngram):
                ngrams.append((ngram, i))
        
        # é‡è¤‡n-gramã‚’æ¤œå‡º
        ngram_counts = Counter([ngram for ngram, _ in ngrams])
        repeated_ngrams = {ngram for ngram, count in ngram_counts.items() if count > 1}
        
        if not repeated_ngrams:
            return text
        
        # é‡è¤‡ç®‡æ‰€ã‚’ç‰¹å®šã—ã¦é™¤å»
        result = text
        offset = 0
        
        for ngram, pos in ngrams:
            if ngram in repeated_ngrams:
                # 2å›ç›®ä»¥é™ã®å‡ºç¾ã‚’å‰Šé™¤
                if result.count(ngram) > 1:
                    # ã‚ˆã‚Šè³¢ã„å‰Šé™¤ï¼ˆæ–‡è„ˆã‚’è€ƒæ…®ï¼‰
                    result = self._smart_ngram_removal(result, ngram)
        
        if result != text and self.debug_mode:
            self.debug_log.append(f"n-gramãƒ–ãƒ­ãƒƒã‚¯é©ç”¨: {len(repeated_ngrams)}å€‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‡¦ç†")
        
        return result

    def _smart_ngram_removal(self, text: str, ngram: str) -> str:
        """æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸn-gramé™¤å»"""
        positions = []
        start = 0
        while True:
            pos = text.find(ngram, start)
            if pos == -1:
                break
            positions.append(pos)
            start = pos + 1
        
        if len(positions) <= 1:
            return text
        
        # æœ€åˆã®å‡ºç¾ã‚’ä¿æŒã—ã€2å›ç›®ä»¥é™ã‚’å‰Šé™¤
        # ãŸã ã—ã€æ–‡è„ˆçš„ã«è‡ªç„¶ãªä½ç½®ã‚’ä¿æŒ
        result = text
        for pos in reversed(positions[1:]):  # å¾Œã‚ã‹ã‚‰å‰Šé™¤
            # å‘¨è¾ºæ–‡è„ˆã‚’ãƒã‚§ãƒƒã‚¯
            before = text[max(0, pos-5):pos] if pos > 0 else ""
            after = text[pos+len(ngram):pos+len(ngram)+5] if pos+len(ngram) < len(text) else ""
            
            # å¥èª­ç‚¹ã‚„åŒºåˆ‡ã‚ŠãŒã‚ã‚‹å ´åˆã¯å‰Šé™¤ã—ã‚„ã™ã„
            if any(char in before + after for char in "ã€‚ã€ï¼ï¼Ÿ\nã€Œã€"):
                result = result[:pos] + result[pos+len(ngram):]
        
        return result

    def _apply_dynamic_repeat_penalty(self, text: str) -> str:
        """å‹•çš„Repeat-Penaltyé©ç”¨"""
        if not self.enable_drp:
            return text
        
        # ç°¡æ˜“çš„ãªDRPå®Ÿè£…ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã§ã¯ãªãæ–‡å­—ãƒ¬ãƒ™ãƒ«ï¼‰
        window_size = self.drp_window
        result = ""
        
        for i, char in enumerate(text):
            if i < window_size:
                result += char
                continue
            
            # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã§ã®æ–‡å­—å‡ºç¾ç‡ã‚’è¨ˆç®—
            window = text[i-window_size:i]
            char_count = window.count(char)
            repeat_rate = char_count / window_size
            
            # ãƒšãƒŠãƒ«ãƒ†ã‚£é©ç”¨ï¼ˆé«˜ã„åå¾©ç‡ã®æ–‡å­—ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            penalty_threshold = 0.3 + self.drp_alpha * repeat_rate
            
            if repeat_rate < penalty_threshold or char in "ã€‚ã€ï¼ï¼Ÿ\n":
                result += char
            # else: æ–‡å­—ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå®Ÿè³ªçš„ãªãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
        
        if result != text and self.debug_mode:
            self.debug_log.append(f"DRPé©ç”¨: {len(text) - len(result)}æ–‡å­—ã‚’æŠ‘åˆ¶")
        
        return result

    def _apply_suppression_with_debug(self, text: str, pattern: RepetitionPattern, character_name: str = None) -> str:
        """ãƒ‡ãƒãƒƒã‚°æƒ…å ±ä»˜ãã®æŠ‘åˆ¶å‡¦ç†"""
        old_text = text
        
        # æ—¢å­˜ã®æŠ‘åˆ¶ãƒ­ã‚¸ãƒƒã‚¯
        if pattern.pattern_type == 'exact':
            text = self._suppress_exact_repetition(text, pattern)
        elif pattern.pattern_type == 'character':
            text = self._suppress_character_repetition(text, pattern)
        elif pattern.pattern_type == 'interjection':
            text = self._suppress_interjection_overuse(text, pattern)
        elif pattern.pattern_type == 'word' and self.enable_aggressive_mode:
            text = self._suppress_word_repetition(text, pattern)
        
        # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        if old_text != text:
            pattern.similarity_score = self._calculate_similarity(old_text, text)
        
        return text

    def _is_over_compressed(self, original: str, compressed: str) -> bool:
        """éå‰°åœ§ç¸®ã®æ¤œå‡º"""
        if len(compressed) < len(original) * 0.5:  # 50%ä»¥ä¸ŠçŸ­ç¸®ã•ã‚ŒãŸå ´åˆ
            return True
        
        # é‡è¦ãªæƒ…å ±ã®æ¶ˆå¤±ãƒã‚§ãƒƒã‚¯
        important_patterns = [
            r'[ã€‚ï¼ï¼Ÿ]',  # å¥èª­ç‚¹
            r'[ã€Œã€]',    # ã‚«ã‚®æ‹¬å¼§
            r'[ã‚-ã‚“]{3,}',  # é•·ã„ã²ã‚‰ãŒãªèª
            r'[ã‚¡-ãƒ³]{3,}',  # é•·ã„ã‚«ã‚¿ã‚«ãƒŠèª
        ]
        
        for pattern in important_patterns:
            original_count = len(re.findall(pattern, original))
            compressed_count = len(re.findall(pattern, compressed))
            
            if original_count > 0 and compressed_count < original_count * 0.7:
                return True
        
        return False

    def _remove_adjacent_duplicates_v2(self, text: str) -> str:
        """éš£æ¥é‡è¤‡é™¤å»ï¼ˆv2æ”¹è‰¯ç‰ˆï¼‰"""
        if not text:
            return text
        
        # ã‚ˆã‚Šç²¾å¯†ãªéš£æ¥é‡è¤‡æ¤œå‡º
        result = []
        i = 0
        
        while i < len(text):
            current_char = text[i]
            
            # é€£ç¶šã™ã‚‹åŒã˜æ–‡å­—ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            consecutive_count = 1
            j = i + 1
            while j < len(text) and text[j] == current_char:
                consecutive_count += 1
                j += 1
            
            # åˆ¶é™ã«å¾“ã£ã¦æ–‡å­—ã‚’è¿½åŠ 
            if re.match(r'[ã‚-ã‚“ã‚¢-ãƒ³ãƒ¼ã£]', current_char):
                # ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠã¯åˆ¶é™ã‚ã‚Š
                limit = self.character_repetition_limit
                add_count = min(consecutive_count, limit)
            elif current_char in "ï¼ï¼Ÿ":
                # æ„Ÿå˜†ç¬¦ã¯æœ€å¤§2å€‹
                add_count = min(consecutive_count, 2)
            elif current_char in "â€¦ã€œ":
                # çœç•¥è¨˜å·ã¯æœ€å¤§3å€‹
                add_count = min(consecutive_count, 3)
            else:
                # ãã®ä»–ã¯åˆ¶é™ãªã—
                add_count = consecutive_count
            
            result.extend([current_char] * add_count)
            i = j
        
        final_result = ''.join(result)
        
        # ãƒ•ãƒ¬ãƒ¼ã‚ºãƒ¬ãƒ™ãƒ«ã®éš£æ¥é‡è¤‡é™¤å»
        final_result = self._remove_phrase_duplicates(final_result)
        
        return final_result

    def _remove_phrase_duplicates(self, text: str) -> str:
        """ãƒ•ãƒ¬ãƒ¼ã‚ºãƒ¬ãƒ™ãƒ«ã®éš£æ¥é‡è¤‡é™¤å»"""
        # 2-5æ–‡å­—ã®ãƒ•ãƒ¬ãƒ¼ã‚ºé‡è¤‡ã‚’æ¤œå‡ºãƒ»é™¤å»
        for length in range(5, 1, -1):  # é•·ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã‹ã‚‰å‡¦ç†
            pattern = r'(.{' + str(length) + r'})\1+'
            
            def replace_duplicate(match):
                original_phrase = match.group(1)
                # ãƒ•ãƒ¬ãƒ¼ã‚ºãŒæ„å‘³ã®ã‚ã‚‹å†…å®¹ã‹ãƒã‚§ãƒƒã‚¯
                if re.search(r'[ã‚-ã‚“ã‚¢-ãƒ³æ¼¢å­—]', original_phrase):
                    return original_phrase  # 1å›ã ã‘æ®‹ã™
                return match.group(0)  # è¨˜å·ç­‰ã¯ãã®ã¾ã¾
            
            text = re.sub(pattern, replace_duplicate, text)
        
        return text

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦è¨ˆç®—ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        if not text1 or not text2:
            return 0.0
        
        # Jaccardé¡ä¼¼åº¦
        if self.use_jaccard_similarity:
            set1 = set(text1)
            set2 = set(text2)
            intersection = len(set1 & set2)
            union = len(set1 | set2)
            return intersection / union if union > 0 else 0.0
        else:
            # å¾“æ¥ã®difflib
            import difflib
            return difflib.SequenceMatcher(None, text1, text2).ratio()

    def get_debug_report(self) -> Dict:
        """ãƒ‡ãƒãƒƒã‚°ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        if not self.session_metrics:
            return {"message": "ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"}
        
        # çµ±è¨ˆè¨ˆç®—
        total_sessions = len(self.session_metrics)
        avg_success_rate = sum(m.success_rate for m in self.session_metrics) / total_sessions
        avg_processing_time = sum(m.processing_time_ms for m in self.session_metrics) / total_sessions
        
        total_patterns = sum(m.patterns_detected for m in self.session_metrics)
        total_misses = sum(m.detection_misses for m in self.session_metrics)
        total_over_compressions = sum(m.over_compressions for m in self.session_metrics)
        
        miss_rate = (total_misses / total_patterns) if total_patterns > 0 else 0
        over_compression_rate = (total_over_compressions / total_patterns) if total_patterns > 0 else 0
        
        # æœ€æ–°ã®ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
        recent_debug_logs = self.debug_log[-20:] if len(self.debug_log) > 20 else self.debug_log
        
        return {
            "session_summary": {
                "total_sessions": total_sessions,
                "overall_success_rate": avg_success_rate,
                "target_achievement": avg_success_rate >= 0.8,
                "avg_processing_time_ms": avg_processing_time
            },
            "failure_analysis": {
                "detection_miss_rate": miss_rate,
                "over_compression_rate": over_compression_rate,
                "primary_issue": "detection_miss" if miss_rate > over_compression_rate else "over_compression"
            },
            "performance_metrics": {
                "total_patterns_detected": total_patterns,
                "total_misses": total_misses,
                "total_over_compressions": total_over_compressions,
                "processing_efficiency": f"{total_patterns / avg_processing_time:.1f} patterns/ms"
            },
            "recent_debug_logs": recent_debug_logs,
            "recommendations": self._generate_tuning_recommendations(miss_rate, over_compression_rate, avg_success_rate)
        }

    def _generate_tuning_recommendations(self, miss_rate: float, over_compression_rate: float, success_rate: float) -> List[str]:
        """ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        
        if success_rate < 0.8:
            if miss_rate > 0.15:
                recommendations.append(f"æ¤œçŸ¥æ¼ã‚Œç‡ {miss_rate:.1%} ãŒé«˜ã„ â†’ similarity_threshold ã‚’ {self.similarity_threshold:.2f} ã‹ã‚‰ {self.similarity_threshold - 0.02:.2f} ã«ä¸‹ã’ã‚‹")
                recommendations.append("min_repeat_threshold ã‚’ 1 ã«è¨­å®šï¼ˆã‚ˆã‚Šå³æ ¼ãªæ¤œå‡ºï¼‰")
            
            if over_compression_rate > 0.05:
                recommendations.append(f"éå‰°åœ§ç¸®ç‡ {over_compression_rate:.1%} ãŒé«˜ã„ â†’ character_repetition_limit ã‚’ {self.character_repetition_limit} ã‹ã‚‰ {self.character_repetition_limit + 1} ã«ä¸Šã’ã‚‹")
                recommendations.append("enable_aggressive_mode ã‚’ False ã«è¨­å®š")
            
            if miss_rate <= 0.1 and over_compression_rate <= 0.05:
                recommendations.append("n-gramãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’ 4 ã‹ã‚‰ 5 ã«æ‹¡å¤§")
                recommendations.append("DRPã‚¢ãƒ«ãƒ•ã‚¡å€¤ã‚’ 0.5 ã‹ã‚‰ 0.6 ã«èª¿æ•´")
        else:
            recommendations.append("âœ… ç›®æ¨™æˆåŠŸç‡ 80% ã‚’é”æˆã—ã¦ã„ã¾ã™ï¼")
        
        return recommendations

    def analyze_text(self, text: str, character_name: str = None) -> Dict:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åŒ…æ‹¬çš„ã«åˆ†æ
        
        Args:
            text: åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            character_name: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åï¼ˆã‚­ãƒ£ãƒ©åˆ¥åˆ†æç”¨ï¼‰
            
        Returns:
            åˆ†æçµæœè¾æ›¸
        """
        print(f"ğŸ” åå¾©åˆ†æé–‹å§‹: {len(text)}æ–‡å­—")
        
        analysis_start = time.time()
        
        patterns = {
            'exact_repetitions': self._detect_exact_repetitions(text),
            'character_repetitions': self._detect_character_repetitions(text),
            'phonetic_repetitions': self._detect_phonetic_repetitions(text),
            'semantic_repetitions': self._detect_semantic_repetitions(text),
            'interjection_overuse': self._detect_interjection_overuse(text)
        }
        
        # ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒ¢ãƒ¼ãƒ‰ã§ã¯èªãƒ¬ãƒ™ãƒ«ã®æ¤œå‡ºã‚‚è¿½åŠ 
        if self.enable_aggressive_mode:
            patterns['word_repetitions'] = self._detect_word_repetitions(text)
        
        # ç·åˆçš„ãªåå¾©ã‚¹ã‚³ã‚¢è¨ˆç®—
        total_severity = sum(
            sum(p.severity for p in pattern_list)
            for pattern_list in patterns.values()
        )
        
        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¨˜éŒ²
        if character_name:
            self.character_patterns[character_name].extend([
                p for pattern_list in patterns.values() 
                for p in pattern_list
            ])
        
        analysis_time = time.time() - analysis_start
        
        result = {
            'patterns': patterns,
            'total_severity': total_severity,
            'analysis_time': analysis_time,
            'character': character_name,
            'text_length': len(text),
            'timestamp': time.time()
        }
        
        self.pattern_history.append(result)
        
        print(f"âœ… åå¾©åˆ†æå®Œäº†: é‡è¦åº¦ {total_severity:.2f} ({analysis_time:.3f}ç§’)")
        
        return result
    
    def _detect_exact_repetitions(self, text: str) -> List[RepetitionPattern]:
        """å®Œå…¨ä¸€è‡´ã™ã‚‹èªå¥ã®åå¾©ã‚’æ¤œå‡º"""
        patterns = []
        
        # 2æ–‡å­—ä»¥ä¸Šã®èªå¥åå¾©ã‚’æ¤œç´¢
        for length in range(2, min(20, len(text) // 2)):
            for i in range(len(text) - length):
                phrase = text[i:i+length]
                
                # ç©ºç™½ã‚„è¨˜å·ã®ã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if not re.search(r'[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]', phrase):
                    continue
                
                positions = []
                for j in range(i + length, len(text) - length + 1):
                    if text[j:j+length] == phrase:
                        positions.append(j)
                
                if len(positions) >= self.min_repeat_threshold - 1:
                    severity = min(1.0, (len(positions) * length) / 10.0)
                    patterns.append(RepetitionPattern(
                        pattern=phrase,
                        count=len(positions) + 1,
                        positions=[i] + positions,
                        pattern_type='exact',
                        severity=severity
                    ))
        
        # é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é™¤å»ï¼ˆã‚ˆã‚Šé•·ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å„ªå…ˆï¼‰
        patterns.sort(key=lambda p: (p.severity, len(p.pattern)), reverse=True)
        filtered_patterns = []
        used_positions = set()
        
        for pattern in patterns:
            if not any(pos in used_positions for pos in pattern.positions):
                filtered_patterns.append(pattern)
                used_positions.update(pattern.positions)
        
        return filtered_patterns
    
    def _detect_character_repetitions(self, text: str) -> List[RepetitionPattern]:
        """æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®åå¾©ã‚’æ¤œå‡º"""
        patterns = []
        
        # åŒã˜æ–‡å­—ã®é€£ç¶š
        for char in set(text):
            if not re.match(r'[ã‚-ã‚“ã‚¢-ãƒ³ãƒ¼]', char):
                continue
                
            consecutive_pattern = f'{char}{{3,}}'
            matches = list(re.finditer(consecutive_pattern, text))
            
            if matches:
                for match in matches:
                    length = match.end() - match.start()
                    severity = min(1.0, length / 5.0)
                    patterns.append(RepetitionPattern(
                        pattern=match.group(),
                        count=1,
                        positions=[match.start()],
                        pattern_type='character',
                        severity=severity
                    ))
        
        return patterns
    
    def _detect_phonetic_repetitions(self, text: str) -> List[RepetitionPattern]:
        """éŸ³éŸ»çš„ã«é¡ä¼¼ã—ãŸåå¾©ã‚’æ¤œå‡º"""
        patterns = []
        
        # ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠã®éŸ³éŸ»çš„é¡ä¼¼æ€§
        hiragana_text = text.translate(self.katakana_map)
        
        # èªå¥ã‚’éŸ³éŸ»ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†é¡
        phonetic_groups = defaultdict(list)
        
        for length in range(2, 8):
            for i in range(len(hiragana_text) - length):
                phrase = hiragana_text[i:i+length]
                if re.match(r'^[ã‚-ã‚“]+$', phrase):
                    # æ¿éŸ³ãƒ»åŠæ¿éŸ³ã‚’æ­£è¦åŒ–
                    normalized = self._normalize_phonetic(phrase)
                    phonetic_groups[normalized].append((phrase, i))
        
        # éŸ³éŸ»çš„ã«é¡ä¼¼ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        for normalized, occurrences in phonetic_groups.items():
            if len(occurrences) >= self.min_repeat_threshold:
                unique_phrases = list(set(occ[0] for occ in occurrences))
                if len(unique_phrases) > 1:  # ç•°ãªã‚‹è¡¨è¨˜ã§åŒã˜éŸ³
                    severity = min(1.0, len(occurrences) / 8.0)
                    patterns.append(RepetitionPattern(
                        pattern=f"éŸ³éŸ»é¡ä¼¼: {'/'.join(unique_phrases[:3])}",
                        count=len(occurrences),
                        positions=[occ[1] for occ in occurrences],
                        pattern_type='phonetic',
                        severity=severity
                    ))
        
        return patterns
    
    def _normalize_phonetic(self, text: str) -> str:
        """éŸ³éŸ»æ­£è¦åŒ–ï¼ˆæ¿éŸ³ãƒ»åŠæ¿éŸ³ãƒ»æ‹—éŸ³ã®çµ±ä¸€ï¼‰"""
        # æ¿éŸ³ãƒ»åŠæ¿éŸ³ã®æ­£è¦åŒ–
        phonetic_map = str.maketrans(
            'ãŒããã’ã”ã–ã˜ãšãœãã ã¢ã¥ã§ã©ã°ã³ã¶ã¹ã¼ã±ã´ã·ãºã½',
            'ã‹ããã‘ã“ã•ã—ã™ã›ããŸã¡ã¤ã¦ã¨ã¯ã²ãµã¸ã»ã¯ã²ãµã¸ã»'
        )
        
        # æ‹—éŸ³ã®æ­£è¦åŒ–
        normalized = text.translate(phonetic_map)
        normalized = re.sub(r'[ã‚ƒã‚…ã‚‡ã£]', '', normalized)
        
        return normalized
    
    def _detect_semantic_repetitions(self, text: str) -> List[RepetitionPattern]:
        """æ„å‘³çš„ã«é¡ä¼¼ã—ãŸåå¾©ã‚’æ¤œå‡º"""
        patterns = []
        
        # åŒç¾©èªãƒ»é¡ç¾©èªã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        semantic_groups = {
            'æ„Ÿå˜†': ['ã‚ã‚', 'ã†ã‚ã‚', 'ãã‚ƒã‚', 'ã²ã‚ƒã‚'],
            'ç¢ºèª': ['ã§ã™ã­', 'ã§ã™ã‚ˆã­', 'ã§ã—ã‚‡ã†', 'ã§ã—ã‚‡ã†ã­'],
            'å¼·èª¿': ['ã¨ã¦ã‚‚', 'ã™ã”ã', 'ã‚ã¡ã‚ƒãã¡ã‚ƒ', 'ã‹ãªã‚Š'],
            'åŒæ„': ['ãã†ã§ã™', 'ãã†ã§ã™ã­', 'ã¯ã„', 'ãˆãˆ']
        }
        
        for group_name, words in semantic_groups.items():
            found_words = []
            positions = []
            
            for word in words:
                for match in re.finditer(re.escape(word), text):
                    found_words.append(word)
                    positions.append(match.start())
            
            if len(found_words) >= self.min_repeat_threshold:
                unique_words = list(set(found_words))
                if len(unique_words) > 1:
                    severity = min(1.0, len(found_words) / 6.0)
                    patterns.append(RepetitionPattern(
                        pattern=f"æ„å‘³é¡ä¼¼({group_name}): {'/'.join(unique_words[:3])}",
                        count=len(found_words),
                        positions=positions,
                        pattern_type='semantic',
                        severity=severity
                    ))
        
        return patterns
    
    def _detect_interjection_overuse(self, text: str) -> List[RepetitionPattern]:
        """æ„Ÿå˜†è©ãƒ»é–“æŠ•è©ã®éå‰°ä½¿ç”¨ã‚’æ¤œå‡º"""
        patterns = []
        
        for pattern_str in self.interjection_patterns:
            matches = list(re.finditer(pattern_str, text))
            
            if len(matches) >= 3:  # 3å›ä»¥ä¸Šã®ä½¿ç”¨ã§éå‰°ã¨åˆ¤å®š
                severity = min(1.0, len(matches) / 5.0)
                patterns.append(RepetitionPattern(
                    pattern=f"æ„Ÿå˜†è©éå¤š: {pattern_str}",
                    count=len(matches),
                    positions=[m.start() for m in matches],
                    pattern_type='interjection',
                    severity=severity
                ))
        
        return patterns
    
    def suppress_repetitions(self, text: str, character_name: str = None) -> str:
        """
        åå¾©ã‚’æŠ‘åˆ¶ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            text: å‡¦ç†å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            character_name: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å
            
        Returns:
            åå¾©æŠ‘åˆ¶æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
        """
        print(f"ğŸ”§ åå¾©æŠ‘åˆ¶å‡¦ç†é–‹å§‹: {len(text)}æ–‡å­—")
        
        # åˆ†æå®Ÿè¡Œ
        analysis = self.analyze_text(text, character_name)
        
        # ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒ¢ãƒ¼ãƒ‰ã§ã¯ä½ã„é‡è¦åº¦ã§ã‚‚å‡¦ç†
        severity_threshold = 0.1 if self.enable_aggressive_mode else 0.3
        
        if analysis['total_severity'] < 0.05:
            print("âœ… åå¾©å•é¡Œãªã—")
            return text
        
        suppressed_text = text
        original_length = len(text)
        
        # ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒ¢ãƒ¼ãƒ‰ã§ã¯å®Œå…¨ä¸€è‡´ã‚’æœ€å„ªå…ˆã§å‡¦ç†
        if self.enable_aggressive_mode and self.exact_match_priority:
            # å®Œå…¨ä¸€è‡´ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æœ€åˆã«å‡¦ç†
            exact_patterns = analysis['patterns'].get('exact_repetitions', [])
            for pattern in sorted(exact_patterns, key=lambda p: p.severity, reverse=True):
                if pattern.severity > severity_threshold:
                    suppressed_text = self._apply_suppression(
                        suppressed_text, pattern, character_name
                    )
        
        # ãã®ä»–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‡¦ç†
        for pattern_type, pattern_list in analysis['patterns'].items():
            if self.enable_aggressive_mode and pattern_type == 'exact_repetitions':
                continue  # æ—¢ã«å‡¦ç†æ¸ˆã¿
                
            for pattern in sorted(pattern_list, key=lambda p: p.severity, reverse=True):
                if pattern.severity > severity_threshold:
                    suppressed_text = self._apply_suppression(
                        suppressed_text, pattern, character_name
                    )
        
        # ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒ¢ãƒ¼ãƒ‰ã§ã®è¿½åŠ å‡¦ç†ï¼šéš£æ¥ã™ã‚‹åŒä¸€èªå¥ã®é™¤å»
        if self.enable_aggressive_mode:
            suppressed_text = self._remove_adjacent_duplicates(suppressed_text)
        
        compression_ratio = (original_length - len(suppressed_text)) / original_length * 100
        print(f"âœ… åå¾©æŠ‘åˆ¶å®Œäº†: é‡è¦åº¦ {analysis['total_severity']:.2f} â†’ æ”¹å–„æ¸ˆã¿ (åœ§ç¸®ç‡: {compression_ratio:.1f}%)")
        
        return suppressed_text
    
    def _apply_suppression(self, text: str, pattern: RepetitionPattern, character_name: str = None) -> str:
        """
        ç‰¹å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã®åå¾©æŠ‘åˆ¶ã‚’é©ç”¨
        """
        if pattern.pattern_type == 'exact':
            return self._suppress_exact_repetition(text, pattern)
        elif pattern.pattern_type == 'character':
            return self._suppress_character_repetition(text, pattern)
        elif pattern.pattern_type == 'interjection':
            return self._suppress_interjection_overuse(text, pattern)
        elif pattern.pattern_type == 'word':
            return self._suppress_word_repetition(text, pattern)
        else:
            return text
    
    def _suppress_exact_repetition(self, text: str, pattern: RepetitionPattern) -> str:
        """å®Œå…¨ä¸€è‡´åå¾©ã®æŠ‘åˆ¶"""
        original_phrase = pattern.pattern
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ä»£æ›¿è¡¨ç¾ã‚’å–å¾—
        if original_phrase in self.replacement_cache:
            alternatives = self.replacement_cache[original_phrase]
        elif original_phrase in self.replacement_dict:
            alternatives = self.replacement_dict[original_phrase]
        else:
            # å‹•çš„ä»£æ›¿è¡¨ç¾ç”Ÿæˆ
            alternatives = self._generate_alternatives(original_phrase)
            self.replacement_cache[original_phrase] = alternatives
        
        # åå¾©ç®‡æ‰€ã‚’ä»£æ›¿è¡¨ç¾ã§ç½®æ›
        result = text
        positions = sorted(pattern.positions, reverse=True)  # å¾Œã‚ã‹ã‚‰å‡¦ç†
        
        for i, pos in enumerate(positions[1:], 1):  # æœ€åˆã®å‡ºç¾ã¯ä¿æŒ
            if i < len(alternatives):
                replacement = alternatives[i - 1]
            else:
                replacement = alternatives[i % len(alternatives)]
            
            # ç½®æ›å®Ÿè¡Œ
            end_pos = pos + len(original_phrase)
            result = result[:pos] + replacement + result[end_pos:]
        
        return result
    
    def _suppress_character_repetition(self, text: str, pattern: RepetitionPattern) -> str:
        """æ–‡å­—åå¾©ã®æŠ‘åˆ¶"""
        repeated_char = pattern.pattern[0]
        length = len(pattern.pattern)
        
        # é©åˆ‡ãªé•·ã•ã«çŸ­ç¸®
        if length > 5:
            replacement = repeated_char * 2
        elif length > 3:
            replacement = repeated_char * 2
        else:
            replacement = pattern.pattern
        
        return text.replace(pattern.pattern, replacement, 1)
    
    def _suppress_interjection_overuse(self, text: str, pattern: RepetitionPattern) -> str:
        """æ„Ÿå˜†è©éå¤šã®æŠ‘åˆ¶"""
        # æ„Ÿå˜†è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å®Ÿéš›ã®æ–‡å­—åˆ—ã‚’æŠ½å‡ºã—ã¦å‡¦ç†
        pattern_matches = re.findall(pattern.pattern.replace('æ„Ÿå˜†è©éå¤š: ', ''), text)
        
        if pattern_matches:
            # æœ€åˆã®2å›ã‚’æ®‹ã—ã¦ä»–ã‚’å‰Šé™¤ã¾ãŸã¯ç°¡ç•¥åŒ–
            for i, match in enumerate(pattern_matches[2:], 2):
                if i % 2 == 0:  # å¶æ•°ç•ªç›®ã¯å‰Šé™¤
                    text = text.replace(match, '', 1)
                else:  # å¥‡æ•°ç•ªç›®ã¯ç°¡ç•¥åŒ–
                    simplified = self._simplify_interjection(match)
                    text = text.replace(match, simplified, 1)
        
        return text
    
    def _simplify_interjection(self, interjection: str) -> str:
        """æ„Ÿå˜†è©ã®ç°¡ç•¥åŒ–"""
        # åŸºæœ¬å½¢ã«æˆ»ã™
        if len(interjection) > 3:
            base_char = interjection[0]
            return base_char * 2
        return interjection
    
    def _generate_alternatives(self, phrase: str) -> List[str]:
        """å‹•çš„ä»£æ›¿è¡¨ç¾ç”Ÿæˆ"""
        alternatives = []
        
        # åŸºæœ¬çš„ãªå¤‰å½¢ãƒ‘ã‚¿ãƒ¼ãƒ³
        if re.match(r'^[ã‚-ã‚“]+$', phrase):
            # ã²ã‚‰ãŒãªã®å ´åˆ
            if len(phrase) <= 2:
                alternatives = [phrase[0], phrase + 'ã£', 'ã‚“' + phrase]
            else:
                alternatives = [phrase[:2], phrase[0] + 'ã£', phrase[:-1]]
        
        elif re.match(r'^[ã‚¢-ãƒ³]+$', phrase):
            # ã‚«ã‚¿ã‚«ãƒŠã®å ´åˆ
            alternatives = [phrase[:2], phrase + 'ï¼', phrase[:-1]]
        
        else:
            # æ¼¢å­—ãƒ»æ··åˆã®å ´åˆ
            alternatives = [phrase[:len(phrase)//2], phrase + 'ã­', phrase + 'ã‚ˆ']
        
        # æœ€ä½é™ã®ä»£æ›¿è¡¨ç¾ã‚’ä¿è¨¼
        if not alternatives:
            alternatives = [phrase[:-1] if len(phrase) > 1 else phrase]
        
        return alternatives[:3]  # æœ€å¤§3ã¤ã®ä»£æ›¿è¡¨ç¾
    
    def _remove_adjacent_duplicates(self, text: str) -> str:
        """éš£æ¥ã™ã‚‹åŒä¸€èªå¥ã®é™¤å»ï¼ˆã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒ¢ãƒ¼ãƒ‰å°‚ç”¨ï¼‰"""
        if not text:
            return text
        
        # èªå¥å¢ƒç•Œã§ã®åˆ†å‰²ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
        words = re.split(r'([ã€‚ã€ï¼ï¼Ÿï¼š\s]+)', text)
        cleaned_words = []
        
        i = 0
        while i < len(words):
            word = words[i]
            
            # ç©ºæ–‡å­—ã‚„è¨˜å·ã®ã¿ã®å ´åˆã¯ãã®ã¾ã¾è¿½åŠ 
            if not word or not re.search(r'[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]', word):
                cleaned_words.append(word)
                i += 1
                continue
            
            # éš£æ¥ã™ã‚‹åŒä¸€èªå¥ã‚’ãƒã‚§ãƒƒã‚¯
            duplicate_count = 1
            while (i + duplicate_count < len(words) and 
                   words[i + duplicate_count] == word):
                duplicate_count += 1
            
            # é‡è¤‡ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ
            if duplicate_count > 1:
                # 2ã¤ä»¥ä¸Šã®é‡è¤‡ã¯1ã¤ã«å‰Šæ¸›
                cleaned_words.append(word)
                if duplicate_count > 2:  # 3ã¤ä»¥ä¸Šã®å ´åˆã¯å‰Šæ¸›åŠ¹æœã‚’ãƒ­ã‚°
                    print(f"   ğŸ“ éš£æ¥é‡è¤‡èªå¥ã‚’å‰Šæ¸›: '{word}' ({duplicate_count} â†’ 1)")
                i += duplicate_count
            else:
                cleaned_words.append(word)
                i += 1
        
        return ''.join(cleaned_words)
    
    def _detect_word_repetitions(self, text: str) -> List[RepetitionPattern]:
        """èªãƒ¬ãƒ™ãƒ«ã§ã®åå¾©æ¤œå‡ºï¼ˆã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒ¢ãƒ¼ãƒ‰ç”¨ï¼‰"""
        patterns = []
        
        # å½¢æ…‹ç´ è§£æçš„ãªèªå¥åˆ†å‰²ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        words = re.findall(r'[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]+|[A-Za-z]+|\d+', text)
        word_positions = {}
        
        # èªå¥ã®ä½ç½®ã‚’è¨˜éŒ²
        current_pos = 0
        for word in words:
            pos = text.find(word, current_pos)
            if word not in word_positions:
                word_positions[word] = []
            word_positions[word].append(pos)
            current_pos = pos + len(word)
        
        # åå¾©èªå¥ã‚’æ¤œå‡º
        for word, positions in word_positions.items():
            if len(positions) >= self.min_repeat_threshold and len(word) >= 2:
                # è¿‘æ¥ã—ã¦ã„ã‚‹åå¾©ã®ã¿ã‚’å¯¾è±¡
                close_positions = []
                for i, pos in enumerate(positions):
                    if i == 0:
                        close_positions.append(pos)
                    elif pos - positions[i-1] <= self.max_distance:
                        close_positions.append(pos)
                
                if len(close_positions) >= self.min_repeat_threshold:
                    severity = min(1.0, (len(close_positions) * len(word)) / 20.0)
                    patterns.append(RepetitionPattern(
                        pattern=word,
                        count=len(close_positions),
                        positions=close_positions,
                        pattern_type='word',
                        severity=severity
                    ))
        
        return patterns
    
    def _suppress_word_repetition(self, text: str, pattern: RepetitionPattern) -> str:
        """èªãƒ¬ãƒ™ãƒ«åå¾©ã®æŠ‘åˆ¶"""
        word = pattern.pattern
        positions = sorted(pattern.positions, reverse=True)
        
        # ä»£æ›¿è¡¨ç¾ã‚’ç”Ÿæˆ
        alternatives = self._generate_alternatives(word)
        
        # å¾Œã‚ã‹ã‚‰ç½®æ›ï¼ˆä½ç½®ãŒãšã‚Œãªã„ã‚ˆã†ã«ï¼‰
        result = text
        for i, pos in enumerate(positions[1:], 1):  # æœ€åˆã®å‡ºç¾ã¯ä¿æŒ
            if i < len(alternatives):
                replacement = alternatives[i - 1]
            else:
                # ä»£æ›¿è¡¨ç¾ãŒè¶³ã‚Šãªã„å ´åˆã¯å‰Šé™¤
                replacement = ""
            
            # ç½®æ›å®Ÿè¡Œ
            end_pos = pos + len(word)
            if replacement:
                result = result[:pos] + replacement + result[end_pos:]
            else:
                # èªå¥ã‚’å®Œå…¨ã«å‰Šé™¤
                result = result[:pos] + result[end_pos:]
                
        return result
    
    def get_statistics(self) -> Dict:
        """åå¾©æŠ‘åˆ¶çµ±è¨ˆã®å–å¾—"""
        if not self.pattern_history:
            return {"message": "åˆ†æå±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“"}
        
        total_analyses = len(self.pattern_history)
        avg_severity = sum(h['total_severity'] for h in self.pattern_history) / total_analyses
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
        pattern_type_stats = defaultdict(int)
        for analysis in self.pattern_history:
            for pattern_type, patterns in analysis['patterns'].items():
                pattern_type_stats[pattern_type] += len(patterns)
        
        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åˆ¥çµ±è¨ˆ
        char_stats = {}
        for char, patterns in self.character_patterns.items():
            char_stats[char] = {
                'total_patterns': len(patterns),
                'avg_severity': sum(p.severity for p in patterns) / len(patterns) if patterns else 0
            }
        
        return {
            'total_analyses': total_analyses,
            'average_severity': avg_severity,
            'pattern_type_distribution': dict(pattern_type_stats),
            'character_statistics': char_stats,
            'replacement_cache_size': len(self.replacement_cache),
            'recent_severity_trend': [h['total_severity'] for h in self.pattern_history[-10:]]
        }
    
    def save_session_data(self, filepath: str = None):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        if filepath is None:
            os.makedirs("logs/repetition", exist_ok=True)
            filepath = f"logs/repetition/session_{int(time.time())}.json"
        
        session_data = {
            'timestamp': time.time(),
            'statistics': self.get_statistics(),
            'replacement_cache': self.replacement_cache,
            'character_patterns': {
                char: [
                    {
                        'pattern': p.pattern,
                        'count': p.count,
                        'severity': p.severity,
                        'type': p.pattern_type
                    }
                    for p in patterns
                ]
                for char, patterns in self.character_patterns.items()
            }
        }
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“Š åå¾©æŠ‘åˆ¶ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {filepath}")
        except Exception as e:
            print(f"âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")


def demo_repetition_suppression():
    """åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    suppressor = AdvancedRepetitionSuppressor()
    
    test_texts = [
        "ã‚ã‚ã‚ã‚ã‚â€¦ï¼ ã‚ªã‚¹æ§˜â€¦ï¼ ã‚ã‚ã‚ã‚ã‚â€¦ï¼ ã†ã‚ã‚ã‚ã‚ã‚ã‚ã‚ã‚ã‚ã‚ã‚ã‚ã‚ãã£â€¦ï¼",
        "å¬‰ã—ã„ã§ã™å¬‰ã—ã„ã§ã™ã€‚ã¨ã¦ã‚‚å¬‰ã—ã„ã§ã™ã€‚å¬‰ã—ã„æ°—æŒã¡ã§ã™ã€‚",
        "ãã†ã§ã™ã­ãã†ã§ã™ã­ã€‚ã§ã‚‚ã§ã‚‚ã§ã‚‚ã€ã‚„ã£ã±ã‚Šã‚„ã£ã±ã‚Šã€‚",
        "ã²ã‚ƒã‚ã‚ã‚ã‚ã‚ï¼ ãã‚ƒã‚ã‚ã‚ã‚ï¼ ã‚ã‚ã‚ã‚ã‚ï¼ ã†ã‚ã‚ã‚ã‚ã‚ï¼"
    ]
    
    print("ğŸ¯ åå¾©æŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒ¢")
    print("=" * 50)
    
    for i, text in enumerate(test_texts, 1):
        print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ {i}:")
        print(f"åŸæ–‡: {text}")
        
        suppressed = suppressor.suppress_repetitions(text, f"ãƒ†ã‚¹ãƒˆã‚­ãƒ£ãƒ©{i}")
        print(f"æ”¹å–„: {suppressed}")
        
        print("-" * 30)
    
    # çµ±è¨ˆè¡¨ç¤º
    stats = suppressor.get_statistics()
    print(f"\nğŸ“Š ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ:")
    print(f"- åˆ†æå›æ•°: {stats['total_analyses']}")
    print(f"- å¹³å‡é‡è¦åº¦: {stats['average_severity']:.2f}")
    print(f"- ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ: {stats['pattern_type_distribution']}")


if __name__ == "__main__":
    demo_repetition_suppression() 