# -*- coding: utf-8 -*-
"""
ä¸¦è¡ŒTTSï¼ˆãƒ†ã‚­ã‚¹ãƒˆéŸ³å£°åŒæ™‚ç”Ÿæˆï¼‰ã‚¨ãƒ³ã‚¸ãƒ³ v3.0
EasyNovelAssistantçµ±åˆç‰ˆ - ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¨éŸ³å£°åˆæˆã®å®Œå…¨åŒæ™‚å®Ÿè¡Œ
"""

import asyncio
import time
import threading
import queue
from typing import Dict, List, Optional, Callable, Any, Tuple
from dataclasses import dataclass
from enum import Enum
import os
import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

try:
    # Style-BERT-VITS2ã®çµ±åˆ
    style_bert_path = os.path.join(current_dir, "Style-Bert-VITS2")
    if os.path.exists(style_bert_path):
        sys.path.insert(0, style_bert_path)
        from style_bert_vits2.tts_model import TTSModel
        from style_bert_vits2.constants import Languages
        TTS_AVAILABLE = True
    else:
        TTS_AVAILABLE = False
        print("âš ï¸ Style-BERT-VITS2 ãƒ‘ã‚¹è¦‹ã¤ã‹ã‚‰ãš")
except ImportError as e:
    TTS_AVAILABLE = False
    print(f"âš ï¸ Style-BERT-VITS2 ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {e}")


class ProcessingStatus(Enum):
    """å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
    PENDING = "pending"
    TEXT_GENERATING = "text_generating"
    VOICE_GENERATING = "voice_generating"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class GenerationRequest:
    """ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    request_id: str
    prompt: str
    settings: Dict[str, Any]
    text_callback: Optional[Callable[[str], None]] = None
    voice_callback: Optional[Callable[[str], None]] = None  # wav_path
    completion_callback: Optional[Callable[["GenerationResult"], None]] = None


@dataclass
class GenerationResult:
    """ç”Ÿæˆçµæœ"""
    request_id: str
    generated_text: str
    voice_path: Optional[str]
    text_generation_time: float
    voice_generation_time: float
    total_time: float
    status: ProcessingStatus
    error_message: Optional[str] = None


class ConcurrentTTSEngine:
    """ä¸¦è¡ŒTTSï¼ˆãƒ†ã‚­ã‚¹ãƒˆéŸ³å£°åŒæ™‚ç”Ÿæˆï¼‰ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or self._get_default_config()
        self.is_running = False
        
        # å‡¦ç†ã‚­ãƒ¥ãƒ¼ã¨çµ±è¨ˆ
        self.request_queue = asyncio.Queue()
        self.active_requests: Dict[str, GenerationRequest] = {}
        self.completed_requests: List[GenerationResult] = []
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'total_requests': 0,
            'completed_requests': 0,
            'failed_requests': 0,
            'average_text_time': 0.0,
            'average_voice_time': 0.0,
            'average_total_time': 0.0,
            'concurrent_efficiency': 0.0  # åŒæ™‚å®Ÿè¡ŒåŠ¹ç‡
        }
        
        # TTSåˆæœŸåŒ–
        self.tts_model = None
        if TTS_AVAILABLE:
            self._initialize_tts()
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¹ã‚¯
        self.text_worker_task = None
        self.voice_worker_task = None
        self.coordinator_task = None
    
    def _get_default_config(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š"""
        return {
            'max_concurrent_requests': 3,
            'text_generation_timeout': 30.0,
            'voice_generation_timeout': 20.0,
            'enable_voice_generation': True,
            'voice_model_path': None,
            'voice_speaker_id': 0,
            'voice_style': 'Neutral',
            'output_dir': './output/concurrent_tts',
            'enable_metrics': True,
            'enable_streaming': False  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ
        }
    
    def _initialize_tts(self):
        """TTSåˆæœŸåŒ–"""
        try:
            if self.config.get('voice_model_path') and os.path.exists(self.config['voice_model_path']):
                self.tts_model = TTSModel(
                    model_path=self.config['voice_model_path'],
                    config_path=self.config['voice_model_path'].replace('.safetensors', '.json'),
                    device='cuda' if torch.cuda.is_available() else 'cpu'
                )
                print("âœ… Style-BERT-VITS2 TTSåˆæœŸåŒ–æˆåŠŸ")
            else:
                print("âš ï¸ TTSãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹æœªè¨­å®š - éŸ³å£°ç”Ÿæˆã¯åˆ©ç”¨ä¸å¯")
        except Exception as e:
            print(f"âŒ TTSåˆæœŸåŒ–å¤±æ•—: {e}")
            self.tts_model = None
    
    async def start(self):
        """ã‚¨ãƒ³ã‚¸ãƒ³é–‹å§‹"""
        if self.is_running:
            print("âš ï¸ ã‚¨ãƒ³ã‚¸ãƒ³ã¯æ—¢ã«å®Ÿè¡Œä¸­ã§ã™")
            return
        
        self.is_running = True
        os.makedirs(self.config['output_dir'], exist_ok=True)
        
        print("ğŸš€ ä¸¦è¡ŒTTSã‚¨ãƒ³ã‚¸ãƒ³é–‹å§‹")
        print(f"   æœ€å¤§åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {self.config['max_concurrent_requests']}")
        print(f"   éŸ³å£°ç”Ÿæˆ: {'æœ‰åŠ¹' if self.config['enable_voice_generation'] and self.tts_model else 'ç„¡åŠ¹'}")
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¹ã‚¯é–‹å§‹
        self.text_worker_task = asyncio.create_task(self._text_generation_worker())
        if self.config['enable_voice_generation'] and self.tts_model:
            self.voice_worker_task = asyncio.create_task(self._voice_generation_worker())
        self.coordinator_task = asyncio.create_task(self._request_coordinator())
    
    async def stop(self):
        """ã‚¨ãƒ³ã‚¸ãƒ³åœæ­¢"""
        if not self.is_running:
            return
        
        self.is_running = False
        print("ğŸ›‘ ä¸¦è¡ŒTTSã‚¨ãƒ³ã‚¸ãƒ³åœæ­¢ä¸­...")
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¹ã‚¯ã®åœæ­¢
        if self.text_worker_task:
            self.text_worker_task.cancel()
        if self.voice_worker_task:
            self.voice_worker_task.cancel()
        if self.coordinator_task:
            self.coordinator_task.cancel()
        
        print("âœ… ä¸¦è¡ŒTTSã‚¨ãƒ³ã‚¸ãƒ³åœæ­¢å®Œäº†")
    
    async def generate_async(self, 
                           prompt: str, 
                           settings: Optional[Dict[str, Any]] = None,
                           text_callback: Optional[Callable[[str], None]] = None,
                           voice_callback: Optional[Callable[[str], None]] = None,
                           completion_callback: Optional[Callable[[GenerationResult], None]] = None) -> str:
        """éåŒæœŸç”Ÿæˆï¼ˆãƒªã‚¯ã‚¨ã‚¹ãƒˆIDã‚’è¿”ã™ï¼‰"""
        
        request_id = f"req_{int(time.time() * 1000)}_{len(self.active_requests)}"
        request = GenerationRequest(
            request_id=request_id,
            prompt=prompt,
            settings=settings or {},
            text_callback=text_callback,
            voice_callback=voice_callback,
            completion_callback=completion_callback
        )
        
        await self.request_queue.put(request)
        self.active_requests[request_id] = request
        self.stats['total_requests'] += 1
        
        print(f"ğŸ“¥ ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä¿¡: {request_id}")
        print(f"   ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt)} æ–‡å­—")
        
        return request_id
    
    async def _request_coordinator(self):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆèª¿æ•´ï¼ˆã‚­ãƒ¥ãƒ¼ã®ç®¡ç†ï¼‰"""
        while self.is_running:
            try:
                # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã®åˆ¶é™
                if len(self.active_requests) >= self.config['max_concurrent_requests']:
                    await asyncio.sleep(0.1)
                    continue
                
                # æ–°ã—ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†é–‹å§‹
                try:
                    request = await asyncio.wait_for(self.request_queue.get(), timeout=1.0)
                    print(f"ğŸ¯ ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†é–‹å§‹: {request.request_id}")
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¨éŸ³å£°ç”Ÿæˆã‚’ä¸¦è¡Œå®Ÿè¡Œ
                    await self._process_request_concurrent(request)
                    
                except asyncio.TimeoutError:
                    continue
                
            except Exception as e:
                print(f"âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆèª¿æ•´ã‚¨ãƒ©ãƒ¼: {e}")
                await asyncio.sleep(1.0)
    
    async def _process_request_concurrent(self, request: GenerationRequest):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä¸¦è¡Œå‡¦ç†"""
        start_time = time.time()
        
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¨éŸ³å£°ç”Ÿæˆã‚’åŒæ™‚é–‹å§‹
            text_task = asyncio.create_task(self._generate_text(request))
            
            if self.config['enable_voice_generation'] and self.tts_model:
                voice_task = asyncio.create_task(self._generate_voice_when_ready(request, text_task))
                # ä¸¡æ–¹ã®å®Œäº†ã‚’å¾…æ©Ÿ
                text_result, voice_result = await asyncio.gather(text_task, voice_task, return_exceptions=True)
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿
                text_result = await text_task
                voice_result = (None, 0.0)
            
            # çµæœã®å‡¦ç†
            if isinstance(text_result, Exception):
                raise text_result
            if isinstance(voice_result, Exception):
                print(f"âš ï¸ éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯æˆåŠŸï¼‰: {voice_result}")
                voice_result = (None, 0.0)
            
            generated_text, text_time = text_result
            voice_path, voice_time = voice_result
            
            total_time = time.time() - start_time
            
            # åŒæ™‚å®Ÿè¡ŒåŠ¹ç‡ã®è¨ˆç®—
            sequential_time = text_time + voice_time
            concurrent_efficiency = (sequential_time / total_time) if total_time > 0 else 1.0
            
            # çµæœã®ä½œæˆ
            result = GenerationResult(
                request_id=request.request_id,
                generated_text=generated_text,
                voice_path=voice_path,
                text_generation_time=text_time,
                voice_generation_time=voice_time,
                total_time=total_time,
                status=ProcessingStatus.COMPLETED
            )
            
            # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
            if request.completion_callback:
                request.completion_callback(result)
            
            # çµ±è¨ˆæ›´æ–°
            self._update_statistics(result, concurrent_efficiency)
            
            print(f"âœ… ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Œäº†: {request.request_id}")
            print(f"   ãƒ†ã‚­ã‚¹ãƒˆ: {text_time:.2f}ç§’, éŸ³å£°: {voice_time:.2f}ç§’, ç·æ™‚é–“: {total_time:.2f}ç§’")
            print(f"   åŒæ™‚å®Ÿè¡ŒåŠ¹ç‡: {concurrent_efficiency:.1%}")
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼çµæœ
            result = GenerationResult(
                request_id=request.request_id,
                generated_text="",
                voice_path=None,
                text_generation_time=0.0,
                voice_generation_time=0.0,
                total_time=time.time() - start_time,
                status=ProcessingStatus.FAILED,
                error_message=str(e)
            )
            
            if request.completion_callback:
                request.completion_callback(result)
            
            self.stats['failed_requests'] += 1
            print(f"âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—: {request.request_id} - {e}")
        
        finally:
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if request.request_id in self.active_requests:
                del self.active_requests[request.request_id]
            self.completed_requests.append(result)
    
    async def _generate_text(self, request: GenerationRequest) -> Tuple[str, float]:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆãƒ€ãƒŸãƒ¼å®Ÿè£… - å®Ÿéš›ã®LLMã‚¨ãƒ³ã‚¸ãƒ³ã¨ç½®ãæ›ãˆï¼‰"""
        start_time = time.time()
        
        try:
            # ãƒ€ãƒŸãƒ¼ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆå®Ÿéš›ã¯KoboldCppã€çµ±åˆã‚·ã‚¹ãƒ†ãƒ v3ç­‰ã‚’ä½¿ç”¨ï¼‰
            prompt = request.prompt
            settings = request.settings
            
            # æ¨¡æ“¬ç”Ÿæˆæ™‚é–“ï¼ˆå®Ÿéš›ã®LLMå‡¦ç†æ™‚é–“ï¼‰
            generation_time = min(5.0, len(prompt) * 0.01)  # æ–‡å­—æ•°ã«æ¯”ä¾‹
            await asyncio.sleep(generation_time)
            
            # ãƒ€ãƒŸãƒ¼ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            generated_text = f"ã“ã‚Œã¯ã€{prompt}ã€ã«å¯¾ã™ã‚‹ç”Ÿæˆã•ã‚ŒãŸå°èª¬ã®å†…å®¹ã§ã™ã€‚ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãŒæ„Ÿæƒ…è±Šã‹ã«ä¼šè©±ã‚’ç¹°ã‚Šåºƒã’ã€ç‰©èªãŒå±•é–‹ã—ã¦ã„ãã¾ã™ã€‚"
            
            # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
            if request.text_callback:
                request.text_callback(generated_text)
            
            generation_time = time.time() - start_time
            print(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†: {request.request_id} ({generation_time:.2f}ç§’)")
            
            return generated_text, generation_time
            
        except Exception as e:
            print(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {request.request_id} - {e}")
            raise
    
    async def _generate_voice_when_ready(self, request: GenerationRequest, text_task: asyncio.Task) -> Tuple[Optional[str], float]:
        """ãƒ†ã‚­ã‚¹ãƒˆå®Œäº†æ™‚ã«éŸ³å£°ç”Ÿæˆé–‹å§‹"""
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®å®Œäº†ã‚’å¾…æ©Ÿ
            generated_text, _ = await text_task
            
            # éŸ³å£°ç”Ÿæˆå®Ÿè¡Œ
            return await self._generate_voice(request, generated_text)
            
        except Exception as e:
            print(f"âŒ éŸ³å£°ç”Ÿæˆå¾…æ©Ÿã‚¨ãƒ©ãƒ¼: {request.request_id} - {e}")
            return None, 0.0
    
    async def _generate_voice(self, request: GenerationRequest, text: str) -> Tuple[Optional[str], float]:
        """éŸ³å£°ç”Ÿæˆ"""
        if not self.tts_model:
            return None, 0.0
        
        start_time = time.time()
        
        try:
            # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
            timestamp = int(time.time() * 1000)
            output_filename = f"{request.request_id}_{timestamp}.wav"
            output_path = os.path.join(self.config['output_dir'], output_filename)
            
            # Style-BERT-VITS2ã§ã®éŸ³å£°ç”Ÿæˆï¼ˆéåŒæœŸï¼‰
            def _sync_voice_generation():
                try:
                    sample_rate, audio_data = self.tts_model.infer(
                        text=text[:200],  # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
                        language=Languages.JP,
                        speaker_id=self.config.get('voice_speaker_id', 0),
                        style=self.config.get('voice_style', 'Neutral'),
                        style_weight=1.0
                    )
                    
                    # WAVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                    import soundfile as sf
                    sf.write(output_path, audio_data, sample_rate)
                    
                    return output_path
                except Exception as e:
                    print(f"âŒ éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {e}")
                    return None
            
            # åŒæœŸå‡¦ç†ã‚’éåŒæœŸã§å®Ÿè¡Œ
            loop = asyncio.get_event_loop()
            voice_path = await loop.run_in_executor(None, _sync_voice_generation)
            
            generation_time = time.time() - start_time
            
            if voice_path:
                # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
                if request.voice_callback:
                    request.voice_callback(voice_path)
                
                print(f"ğŸµ éŸ³å£°ç”Ÿæˆå®Œäº†: {request.request_id} ({generation_time:.2f}ç§’)")
                return voice_path, generation_time
            else:
                print(f"âŒ éŸ³å£°ç”Ÿæˆå¤±æ•—: {request.request_id}")
                return None, generation_time
                
        except Exception as e:
            print(f"âŒ éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {request.request_id} - {e}")
            return None, time.time() - start_time
    
    async def _text_generation_worker(self):
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆäºˆç´„ï¼‰"""
        while self.is_running:
            await asyncio.sleep(0.1)
    
    async def _voice_generation_worker(self):
        """éŸ³å£°ç”Ÿæˆãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆäºˆç´„ï¼‰"""
        while self.is_running:
            await asyncio.sleep(0.1)
    
    def _update_statistics(self, result: GenerationResult, concurrent_efficiency: float):
        """çµ±è¨ˆæ›´æ–°"""
        self.stats['completed_requests'] += 1
        
        # å¹³å‡æ™‚é–“ã®æ›´æ–°
        n = self.stats['completed_requests']
        self.stats['average_text_time'] = (
            self.stats['average_text_time'] * (n - 1) + result.text_generation_time
        ) / n
        self.stats['average_voice_time'] = (
            self.stats['average_voice_time'] * (n - 1) + result.voice_generation_time
        ) / n
        self.stats['average_total_time'] = (
            self.stats['average_total_time'] * (n - 1) + result.total_time
        ) / n
        self.stats['concurrent_efficiency'] = (
            self.stats['concurrent_efficiency'] * (n - 1) + concurrent_efficiency
        ) / n
    
    def get_statistics(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        return {
            **self.stats,
            'active_requests': len(self.active_requests),
            'queue_size': self.request_queue.qsize(),
            'success_rate': (
                self.stats['completed_requests'] / max(1, self.stats['total_requests'])
            ) * 100,
            'average_concurrent_speedup': self.stats['concurrent_efficiency'] * 100
        }
    
    def get_request_status(self, request_id: str) -> Optional[ProcessingStatus]:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆçŠ¶æ…‹å–å¾—"""
        if request_id in self.active_requests:
            return ProcessingStatus.TEXT_GENERATING  # ç°¡ç•¥åŒ–
        
        for result in self.completed_requests:
            if result.request_id == request_id:
                return result.status
        
        return None


# ãƒ‡ãƒ¢ãƒ»ãƒ†ã‚¹ãƒˆé–¢æ•°
async def demo_concurrent_tts():
    """ä¸¦è¡ŒTTSã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ‡ãƒ¢"""
    print("ğŸ§ª ä¸¦è¡ŒTTSï¼ˆãƒ†ã‚­ã‚¹ãƒˆéŸ³å£°åŒæ™‚ç”Ÿæˆï¼‰ã‚¨ãƒ³ã‚¸ãƒ³ ãƒ‡ãƒ¢")
    print("=" * 60)
    
    # ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š
    config = {
        'max_concurrent_requests': 2,
        'enable_voice_generation': TTS_AVAILABLE,
        'output_dir': './output/concurrent_tts_demo',
        'enable_metrics': True
    }
    
    engine = ConcurrentTTSEngine(config)
    
    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
    def on_text_generated(text: str):
        print(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆå—ä¿¡: {text[:50]}...")
    
    def on_voice_generated(voice_path: str):
        print(f"ğŸµ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ: {voice_path}")
    
    def on_completion(result: GenerationResult):
        print(f"ğŸ‰ å®Œäº†é€šçŸ¥: {result.request_id}")
        print(f"   ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {result.status.value}")
        print(f"   ç·æ™‚é–“: {result.total_time:.2f}ç§’")
    
    try:
        # ã‚¨ãƒ³ã‚¸ãƒ³é–‹å§‹
        await engine.start()
        
        # ãƒ†ã‚¹ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        test_prompts = [
            "é­”æ³•å­¦æ ¡ã®æ–°å…¥ç”ŸãŒåˆã‚ã¦é­”æ³•ã‚’ä½¿ã†ç‰©èª",
            "å®‡å®™èˆ¹ã®ä¸­ã§èµ·ã“ã‚‹è¬ã®äº‹ä»¶",
            "å¤ã„æ›¸åº—ã§è¦‹ã¤ã‘ãŸä¸æ€è­°ãªæœ¬ã®è©±"
        ]
        
        # è¤‡æ•°ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®åŒæ™‚é€ä¿¡
        request_ids = []
        for i, prompt in enumerate(test_prompts):
            request_id = await engine.generate_async(
                prompt=prompt,
                settings={'temperature': 0.8, 'max_tokens': 200},
                text_callback=on_text_generated,
                voice_callback=on_voice_generated,
                completion_callback=on_completion
            )
            request_ids.append(request_id)
            print(f"âœ… ãƒªã‚¯ã‚¨ã‚¹ãƒˆ{i+1}é€ä¿¡: {request_id}")
        
        # å‡¦ç†å®Œäº†ã¾ã§å¾…æ©Ÿ
        print("\nâ³ å‡¦ç†å®Œäº†ã¾ã§å¾…æ©Ÿä¸­...")
        start_time = time.time()
        
        while len(engine.active_requests) > 0:
            await asyncio.sleep(0.5)
            
            # çµ±è¨ˆè¡¨ç¤º
            stats = engine.get_statistics()
            print(f"ğŸ“Š é€²è¡ŒçŠ¶æ³: ã‚¢ã‚¯ãƒ†ã‚£ãƒ– {stats['active_requests']}, "
                  f"å®Œäº† {stats['completed_requests']}/{stats['total_requests']}")
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†
            if time.time() - start_time > 30.0:
                print("â±ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - ãƒ‡ãƒ¢çµ‚äº†")
                break
        
        # æœ€çµ‚çµ±è¨ˆ
        final_stats = engine.get_statistics()
        print("\nğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
        print(f"   æˆåŠŸç‡: {final_stats['success_rate']:.1f}%")
        print(f"   å¹³å‡ãƒ†ã‚­ã‚¹ãƒˆæ™‚é–“: {final_stats['average_text_time']:.2f}ç§’")
        print(f"   å¹³å‡éŸ³å£°æ™‚é–“: {final_stats['average_voice_time']:.2f}ç§’")
        print(f"   å¹³å‡ç·æ™‚é–“: {final_stats['average_total_time']:.2f}ç§’")
        print(f"   åŒæ™‚å®Ÿè¡ŒåŠ¹ç‡: {final_stats['average_concurrent_speedup']:.1f}%")
        
        print("\nğŸ¯ ä¸¦è¡ŒTTSåŠ¹æœ:")
        sequential_time = final_stats['average_text_time'] + final_stats['average_voice_time']
        concurrent_time = final_stats['average_total_time']
        speedup = (sequential_time / concurrent_time) if concurrent_time > 0 else 1.0
        print(f"   é †æ¬¡å®Ÿè¡Œäºˆæƒ³æ™‚é–“: {sequential_time:.2f}ç§’")
        print(f"   ä¸¦è¡Œå®Ÿè¡Œå®Ÿæ™‚é–“: {concurrent_time:.2f}ç§’")
        print(f"   é€Ÿåº¦å‘ä¸Š: {speedup:.1f}x")
        
    finally:
        await engine.stop()


if __name__ == "__main__":
    # ãƒ‡ãƒ¢å®Ÿè¡Œ
    asyncio.run(demo_concurrent_tts()) 